{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"architecture/","text":"Architecture","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"cloud-platform/ibm-cloud-operator/","text":"IBM Cloud Operator This operator is used to provision services in IBM Cloud and to create bindings that applications need to consume these services. IBM Cloud Operator Instructions will be provided here when available. Installing the IBM Cloud Operator The IBM Cloud Operator should be available by default in the OperatorHub console in OpenShift. It can be installed using the OperatorHub Console in OpenShift, or be using the OpenShift CLI. Install using the UI Install using the OpenShift CLI Documentation on installing operators in OpenShift can be found here . In the case of the IBM Cloud Operator, it is a matter of creating a yaml file and using the oc cli. On that page it discusses two objects that are needed - the OperatorGroup object and the Subscription object. If you intend to make the operator available in all projects you do not need to create a new OperatorGroup object; one already exists. You only need to create one if you want to install the operator into a single project. Here is sample yaml for creating a Subscription object: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibmcloud-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: ibmcloud-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: ibmcloud-operator.v0.1.7 Note: If you specify the openshift-operators namespace the operator will be installed in all namespaces in the cluster. This is the default behavior; if you specify any other namespace the operator will only be installed in that namespace. More documentation can be found here . Create a yaml file with the content above. Make sure to change the startingCSV value to the correct version of the IBM Cloud Operator as it appears in the OperatorHub console. Login to the oc cli. You can copy the command from the OpenShift Web Console: 1. Click the \"Display Token\" link. Select the command in the \"Log in with this token\" section: Paste the command in a terminal and run it. ``` oc login --token=5y7lF * * ****BOm2cSE --server=https://c106-e.us-south.containers.cloud.ibm.com:31777 Logged into \"https://c106-e.us-south.containers.cloud.ibm.com:31777\" as \"IAM#dwakeman@us.ibm.com\" using the token provided. You have access to 55 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". 1. Run this command: `oc apply -f <yaml_file_name>` oc apply -f ibmcloud-operator-subscription.yaml subscription.operators.coreos.com/ibmcloud-operator created ``` You should see a message that the subscription was created. It should now show up in the OpenShift console in the \"Installed Operators\" page. Congratulations, the IBM Cloud Operator has been installed!! Configuring the IBM Cloud Operator There are a few things that need to be configured in order for the IBM Cloud Operator to work. First, it needs an API key to use to authenticate with IBM Cloud. That user must have sufficient permissions to create services and bindings. More on that in a bit. That API Key gets loaded into a secret named seed-secret . The operator will first look for a secret by that name in the current project, and use it if found. If not, it will look in the default project, and if it finds one there it will use it. This behavior will allow Kaiser Permanente to provide each team with a separate seed-secret that uses an API key for a service ID that only has permission to create certain services, and only in that team's resource group. More discussion is needed to finalize the design for configuration of the operator for KP. If there is no seed-secret in the default project then there must be one in the current project in order for it to work. Each team can be given their own Service ID with permission to only create specific services, and only in the team's resource group. An API Key for that Service ID can be loaded into a seed-secret in the team's project. Similarly, a seed-defaults configmap can be created in the team's project that points to the team's resource group. Using the IBM Cloud Operator Note: For more details see the User Guide or the Readme . The IBM Cloud Operator has two objects that can be created: Service - The service object represents the actual service instance in IBM Cloud. Binding - The binding object creates the service credentials that an application can use to interact with the service. Creating a Service apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: dw-elasticsearch-operator-2 namespace: dw-services spec: plan: standard serviceClass: databases-for-elasticsearch tags: [\"operator\", \"sandbox-roks-cluster\",\"ibm-cloud-utility\"] parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private Creating a Binding apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-dw-elasticsearch-operator-2 namespace: dw-services spec: serviceName: dw-elasticsearch-operator-2 role: Editor Content from OpenShift Page (needs to be reviewed) There are many sample yaml files that show how to create various services. Here is one that creates an instance of Cloudant Standard Plan and a binding for it: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: cloudant-operator-7 namespace: ibm-cloud-utility spec: plan: standard serviceClass: cloudantnosqldb parameters: - name: legacyCredentials value: false - name: environment_crn value: \"crn:v1:bluemix:public:cloudantnosqldb:us-south:a/bd****************************:5d774b30-****-****-****-d7**********::\" --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant-operator-7 spec: serviceName: cloudant-operator-7 role: Manager serviceNamespace: ibm-cloud-utility In this example the application that will be using the service is in the ibm-cloud-utility project in OpenShift. This service will be configured to use only IAM credentials, and the corresponding service credentials will be generated with the Manager role. Note: one additional parameter that Kaiser Permanente will need to use is the environment_crn parameter, which \"pins\" the standard plan instance onto dedicated hardware as defined by an Enterprise Plan instance of Cloudant. At the time this testing was done there was no such instance available to use in testing. To create this service simply save this content to a file (cloudant.yml in this example) and run these commands: oc project ibm-cloud-utility This command targets the ibm-cloud-utility project in the oc cli. oc apply -f cloudant.yml After you run the command you can check the status of the service using this command: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 inactive 72s mycos-operator-1 Online 12h The command will continue to show inactive status until the service completes the provisioning process in IBM Cloud and shows Active status there. Sometime after that the service will show Online status: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 Online 2m42s mycos-operator-1 Online 12h There is a similar command to view the status of the binding: oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Failed 27s binding-mycos-operator-1 Online 12h Note: It is okay that the status is initially Failed for the binding. It fails because the service has not been created. The operator will keep trying to create the binding, and it will eventually succeed when the service comes online. oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Online 2m49s binding-mycos-operator-1 Online 12h Once provisioned you will see that the service instance in IBM Cloud is using only IAM credentials: The credentials have the Manager role: Example for ElasticSearch Here is another yaml file for creating an instance of ElasticSearch: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: myes4 namespace: ibm-cloud-utility spec: plan: standard serviceClass: databases-for-elasticsearch parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-myes4 namespace: ibm-cloud-utility spec: serviceName: myes4 role: Editor In this example, there are several parameters that we used to pass via a -c '{<some JSON>}' option on the cf cli. In this case it provides alternate values for the disk and memory allocation, and specifies that the instance should only use private endpoints. There are no screen shots to share here, but testing did verify that all of these parameters did get applied to the resulting service instance. Viewing Dashboard Links When the operator provisions a service in IBM Cloud it the data about the service that it stores includes the dashboard URL for that service. When viewing the Service in the OpenShift console the UI includes a link to the dashboard. Services can be viewed by following these steps: Login to the OpenShift Web Console. Make sure to be in the Administrator view. Click on Operators -> Installed Operators in the left navigation menu. Use the dropdown at the top of the page to switch to the project where the service has been created. Click on the IBM Cloud Operator. Make sure it is version 0.1.11; if not it will first need to be updated before proceeding. Go to the Services tab. Click on the name of one of the services in the list. It should have a property named DashboardURL at the bottom right. Click the link to go to the service dashboard.","title":"IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#ibm-cloud-operator","text":"This operator is used to provision services in IBM Cloud and to create bindings that applications need to consume these services. IBM Cloud Operator Instructions will be provided here when available.","title":"IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#installing-the-ibm-cloud-operator","text":"The IBM Cloud Operator should be available by default in the OperatorHub console in OpenShift. It can be installed using the OperatorHub Console in OpenShift, or be using the OpenShift CLI.","title":"Installing the IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#install-using-the-ui","text":"","title":"Install using the UI"},{"location":"cloud-platform/ibm-cloud-operator/#install-using-the-openshift-cli","text":"Documentation on installing operators in OpenShift can be found here . In the case of the IBM Cloud Operator, it is a matter of creating a yaml file and using the oc cli. On that page it discusses two objects that are needed - the OperatorGroup object and the Subscription object. If you intend to make the operator available in all projects you do not need to create a new OperatorGroup object; one already exists. You only need to create one if you want to install the operator into a single project. Here is sample yaml for creating a Subscription object: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibmcloud-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: ibmcloud-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: ibmcloud-operator.v0.1.7 Note: If you specify the openshift-operators namespace the operator will be installed in all namespaces in the cluster. This is the default behavior; if you specify any other namespace the operator will only be installed in that namespace. More documentation can be found here . Create a yaml file with the content above. Make sure to change the startingCSV value to the correct version of the IBM Cloud Operator as it appears in the OperatorHub console. Login to the oc cli. You can copy the command from the OpenShift Web Console: 1. Click the \"Display Token\" link. Select the command in the \"Log in with this token\" section: Paste the command in a terminal and run it. ``` oc login --token=5y7lF * * ****BOm2cSE --server=https://c106-e.us-south.containers.cloud.ibm.com:31777 Logged into \"https://c106-e.us-south.containers.cloud.ibm.com:31777\" as \"IAM#dwakeman@us.ibm.com\" using the token provided. You have access to 55 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". 1. Run this command: `oc apply -f <yaml_file_name>` oc apply -f ibmcloud-operator-subscription.yaml subscription.operators.coreos.com/ibmcloud-operator created ``` You should see a message that the subscription was created. It should now show up in the OpenShift console in the \"Installed Operators\" page. Congratulations, the IBM Cloud Operator has been installed!!","title":"Install using the OpenShift CLI"},{"location":"cloud-platform/ibm-cloud-operator/#configuring-the-ibm-cloud-operator","text":"There are a few things that need to be configured in order for the IBM Cloud Operator to work. First, it needs an API key to use to authenticate with IBM Cloud. That user must have sufficient permissions to create services and bindings. More on that in a bit. That API Key gets loaded into a secret named seed-secret . The operator will first look for a secret by that name in the current project, and use it if found. If not, it will look in the default project, and if it finds one there it will use it. This behavior will allow Kaiser Permanente to provide each team with a separate seed-secret that uses an API key for a service ID that only has permission to create certain services, and only in that team's resource group. More discussion is needed to finalize the design for configuration of the operator for KP. If there is no seed-secret in the default project then there must be one in the current project in order for it to work. Each team can be given their own Service ID with permission to only create specific services, and only in the team's resource group. An API Key for that Service ID can be loaded into a seed-secret in the team's project. Similarly, a seed-defaults configmap can be created in the team's project that points to the team's resource group.","title":"Configuring the IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#using-the-ibm-cloud-operator","text":"Note: For more details see the User Guide or the Readme . The IBM Cloud Operator has two objects that can be created: Service - The service object represents the actual service instance in IBM Cloud. Binding - The binding object creates the service credentials that an application can use to interact with the service.","title":"Using the IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#creating-a-service","text":"apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: dw-elasticsearch-operator-2 namespace: dw-services spec: plan: standard serviceClass: databases-for-elasticsearch tags: [\"operator\", \"sandbox-roks-cluster\",\"ibm-cloud-utility\"] parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private","title":"Creating a Service"},{"location":"cloud-platform/ibm-cloud-operator/#creating-a-binding","text":"apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-dw-elasticsearch-operator-2 namespace: dw-services spec: serviceName: dw-elasticsearch-operator-2 role: Editor","title":"Creating a Binding"},{"location":"cloud-platform/ibm-cloud-operator/#content-from-openshift-page-needs-to-be-reviewed","text":"There are many sample yaml files that show how to create various services. Here is one that creates an instance of Cloudant Standard Plan and a binding for it: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: cloudant-operator-7 namespace: ibm-cloud-utility spec: plan: standard serviceClass: cloudantnosqldb parameters: - name: legacyCredentials value: false - name: environment_crn value: \"crn:v1:bluemix:public:cloudantnosqldb:us-south:a/bd****************************:5d774b30-****-****-****-d7**********::\" --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant-operator-7 spec: serviceName: cloudant-operator-7 role: Manager serviceNamespace: ibm-cloud-utility In this example the application that will be using the service is in the ibm-cloud-utility project in OpenShift. This service will be configured to use only IAM credentials, and the corresponding service credentials will be generated with the Manager role. Note: one additional parameter that Kaiser Permanente will need to use is the environment_crn parameter, which \"pins\" the standard plan instance onto dedicated hardware as defined by an Enterprise Plan instance of Cloudant. At the time this testing was done there was no such instance available to use in testing. To create this service simply save this content to a file (cloudant.yml in this example) and run these commands: oc project ibm-cloud-utility This command targets the ibm-cloud-utility project in the oc cli. oc apply -f cloudant.yml After you run the command you can check the status of the service using this command: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 inactive 72s mycos-operator-1 Online 12h The command will continue to show inactive status until the service completes the provisioning process in IBM Cloud and shows Active status there. Sometime after that the service will show Online status: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 Online 2m42s mycos-operator-1 Online 12h There is a similar command to view the status of the binding: oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Failed 27s binding-mycos-operator-1 Online 12h Note: It is okay that the status is initially Failed for the binding. It fails because the service has not been created. The operator will keep trying to create the binding, and it will eventually succeed when the service comes online. oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Online 2m49s binding-mycos-operator-1 Online 12h Once provisioned you will see that the service instance in IBM Cloud is using only IAM credentials: The credentials have the Manager role:","title":"Content from OpenShift Page (needs to be reviewed)"},{"location":"cloud-platform/ibm-cloud-operator/#example-for-elasticsearch","text":"Here is another yaml file for creating an instance of ElasticSearch: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: myes4 namespace: ibm-cloud-utility spec: plan: standard serviceClass: databases-for-elasticsearch parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-myes4 namespace: ibm-cloud-utility spec: serviceName: myes4 role: Editor In this example, there are several parameters that we used to pass via a -c '{<some JSON>}' option on the cf cli. In this case it provides alternate values for the disk and memory allocation, and specifies that the instance should only use private endpoints. There are no screen shots to share here, but testing did verify that all of these parameters did get applied to the resulting service instance.","title":"Example for ElasticSearch"},{"location":"cloud-platform/ibm-cloud-operator/#viewing-dashboard-links","text":"When the operator provisions a service in IBM Cloud it the data about the service that it stores includes the dashboard URL for that service. When viewing the Service in the OpenShift console the UI includes a link to the dashboard. Services can be viewed by following these steps: Login to the OpenShift Web Console. Make sure to be in the Administrator view. Click on Operators -> Installed Operators in the left navigation menu. Use the dropdown at the top of the page to switch to the project where the service has been created. Click on the IBM Cloud Operator. Make sure it is version 0.1.11; if not it will first need to be updated before proceeding. Go to the Services tab. Click on the name of one of the services in the list. It should have a property named DashboardURL at the bottom right. Click the link to go to the service dashboard.","title":"Viewing Dashboard Links"},{"location":"cloud-platform/key-protect/","text":"Key Protect Restricting access to keys Key Protect now supports the ability to assign access to a single key within a Key Protect instance to a given user or access group via Access Policy. If Kaiser Permanente wishes to maintain a single, shared instance of Key Protect and assign individual keys to any development team, this feature will allow it. When a user needs to specify a key for encryption of a particular service, that user will only see the instance of Key Protect to which the user has access and the specific key to which the user has been granted access. In the UI this will filter any dropdowns appropriately and in the CLI or API, access will also be limited. Documentation for setting this level of access can be found here . Note: Key level access is currently only available via access policy for a user or access group. It cannot be applied when granting Service-to-Service Authorization, which is access between two IBM services, such as COS and Key Protect. Dual Authorization Delete This is a new feature that was added to Key Protect that provides the ability to create an instance level policy requiring that two different users approve the deletion of a key. In order to delete a key, a user with appropriate permissions must \"set the key for deletion\", which updates a flag on the key. After that, another user with permission to do so can delete the key. The person who does the first authorization cannot actually delete the key; it requires two different users to complete the key deletion. Note: At this time the \"first\" action to set the key for deletion can only be done via API. Once done, any available option for deleting a key (UI, CLI, API) can be used to actually delete the key. In order to make it easier to use this new feature IBM has provided (As-Is, not part of the product and not supported) a script that automates some of these actions using the API. The script can be found here . Considerations The Dual Auth Delete policy is applied at the individual key level, although it can be defined at the service instance level. Once the Dual Auth policy is set for a service instance, all keys created after that will automatically inherit that policy. Any keys that existed prior to the creation of the service instance level policy will NOT inherit the policy. If need be the policy can be set on individual keys. Note: This still needs to be validated. Using the script The script leverages the Key Protect API via cURL commands. These commands require an auth token, which can be retrieved via the ibmcloud CLI. It also uses the CLI to look up information needed by some of the APIs. The script will automatically get the auth token and use the CLI to look things up, but it does require the user of the script to first login to the CLI in the same terminal/shell where the script will be run. Command Options NAME: keyprotect.sh - Manage features of Key Protect USAGE: keyprotect.sh <key-protect-instance-name> command [options] COMMANDS: ------------------------------------------------------------------------------------------- view-policies List the current policies for the Key Protect Instance enable-dual-auth Enable the Dual Authorization policy for key deletes for all keys disable-dual-auth Disable the Dual Authorization policy for key deletes for all keys view-keys List the keys in the Key Protect Instance in JSON format view-keys-list List the keys in the Key Protect Instance in list format view-deleted-keys List the deleted keys in the Key Protect Instance in JSON format view-deleted-keys-list List the deleted keys in the Key Protect Instance in list format view-key-material View the material for a standard key view-key-policies View the current polices for the specified key import-key Import a standard or root key restore-key Restore an imported key that has been deleted set-key-deletion Set the specified key for deletion (first auth) unset-key-deletion Unset the specified key for deletion, which removes the first auth help, h View help for this script Note: For your convenience this command executes the ibmcloud cli to look up certain information needed to perform these tasks. It requires you to be logged into the ibmcloud cli before you run this command. Viewing policies for a Key Protect Instance Command ./keyprotect.sh key-protect-dallas-dw view-policies Output Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 }, \"resources\": [] } Enabling the Dual Authorization policy for a Key Protect Instance Command ./keyprotect.sh key-protect-dallas-dw enable-dual-auth Output Enabling Dual Authorization for service key-protect-dallas-dw... Done. Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true }, \"creation_date\": \"2020-01-16T20:47:30Z\", \"created_by\": \"IBMid-110000J6VA\", \"updated_by\": \"IBMid-110000J6VA\", \"last_updated\": \"2020-01-16T20:47:30Z\" } ] } Request complete. View Keys in JSON format Command ./keyprotect.sh key-protect-dallas-dw view-keys Output Listing keys for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 2 }, \"resources\": [ { \"id\": \"a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-delete-crk\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 }, { \"id\": \"f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-cos-crk-1\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-17T21:44:39Z\", \"lastUpdateDate\": \"2020-01-17T21:44:39Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 } ] } Request complete. View Keys in list format Command ./keyprotect.sh key-protect-dallas-dw view-keys-list Output Listing keys for service key-protect-dallas-dw... name id crn dw-test-delete-crk a15604c8-e5c6-4fc9-ac92-5be79bdb1424 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424 dw-test-cos-crk-1 f41d77aa-f357-4234-ba46-6aec1b4a7f92 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92 Request complete. View Deleted Keys in JSON Format Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys Output Listing keys for service key-protect-dallas-dw... Values for State field: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 4 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"name\": \"dw-ui-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"imported\": false, \"creationDate\": \"2020-01-16T22:40:15Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T22:40:15Z\", \"keyVersion\": { \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-01-23T22:41:02Z\" } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\", \"name\": \"dw-test-import-delete-restore-01\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7\", \"imported\": true, \"creationDate\": \"2020-05-01T21:31:47Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T21:31:47Z\", \"keyVersion\": { \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"name\": \"dw-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"imported\": false, \"creationDate\": \"2020-01-16T20:53:55Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T20:53:55Z\", \"keyVersion\": { \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"name\": \"${keyName}\", \"state\": 5, \"extractable\": true, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"imported\": false, \"creationDate\": \"2020-05-01T14:37:44Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T14:37:44Z\", \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-05-08T16:16:10Z\" } } ] } View Deleted Keys in List Format Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys-list Output Listing deleted keys for service key-protect-dallas-dw... Values for State column: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed name id state crn dw-ui-test-crk-1 38d3c46d-0f94-4843-ba36-9ac6c08c4c3c 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c dw-test-import-delete-restore-01 4b559191-e10e-4bf6-9891-9af1948b55f7 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7 dw-test-crk-1 75588eb5-81d4-4cdb-997c-b6deb6be06cf 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf ${keyName} 814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 Request complete. View Key Policies Command ./keyprotect.sh key-protect-dallas-dw view-key-policies a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Note: The key-specific commands require an extra parameter, the key id, which can be found in the UI, CLI or API. If not provided these commands will produce this error: for view-key-policies a key id is required USAGE: keyprotect.sh [service instance name] view-key-policies [key-id] Output viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\" } ] } Request complete. Set Key for Deletion Command ./keyprotect.sh key-protect-dallas-dw set-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Setting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:05:09Z\" } ] } Request complete. Note: If the key has not been enabled for Dual Auth Delete, the command will return the error below. Setting key 75588eb5-81d4-4cdb-997c-b6deb6be06cf in service key-protect-dallas-dw for deletion... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.error+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"errorMsg\": \"Action could not be performed on key. Please see `reasons` for more details.\", \"reasons\": [ { \"code\": \"NOT_DUAL_AUTH_ERR\", \"message\": \"The key is not dual auth enabled and cannot be set for deletion\", \"status\": 409, \"more_info\": \"https://cloud.ibm.com/apidocs/key-protect\" } ] } ] } Done. viewing policies for key 75588eb5-81d4-4cdb-997c-b6deb6be06cf... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 } } Request complete. Unset key for deletion In some cases it may turn out that the first authorization was done in error. This command can be used to unset the key for deletion, which removes the first authorization. Command ./keyprotect.sh key-protect-dallas-dw unset-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Unsetting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:15:44Z\" } ] } Request complete. Note: Setting or unsetting the key for deletion does NOT affect the policy itself; as seen in the output above the policy is still set to true. At this time the \"set deletion\" status and its lastUpdated date do not show up in the API response. IBM is going to update the API in the next iteration to include these two fields. Deleting a Key from the UI IMPORTANT WARNING: New functionality for Key Protect has been delivered that will prevent a key from being deleted when using the Key Protect API if that key is being used by any services. This feature has NOT YET been updated in the UI!!! If you delete the key in the UI and the key (or the Key Protect instance) does NOT have Dual Authorization Delete policy enabled, that key WILL get deleted, regardless of whether or not is being used. To protect against this possibility make sure that all Key Protect instances have the Dual Authorization Delete policy enabled and use the Registration API before deleting any key to see if it is being used by any services or resources. Note: The steps below for deleting a kay via the UI assume that the Dual Authorization Policy has been enabled. If you try to delete a key from the UI and it has not yet been set for deletion you will see an error like this: If you try to delete the key after it has been set for deletion and you're the one who did that you will see this error: Note: If you try to delete the key and somebody else set the key for deletion you will be able to delete the key. Viewing usage of keys It is possible in IBM Cloud to track where Key Protect keys are being used. When a service is provisioned or configure to use a Key Protect key, it \"registers\" that usage with Key Protect. You can see the list of resources using a given key with a new operation in the Key Protect API . GET https://us-south.kms.cloud.ibm.com/api/v2/keys/4442aa89-9749-4cad-9a6e-73a77508a616/registrations Headers: Authorization: 'Bearer ' bluemix-instance: ' ' The URL endpoint will vary by region, and it contains the GUID of the key. { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 3 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8:bucket:dw-kp-bucket-test-registration-api-1578085-01\", \"createdBy\": \"crn-crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8::\", \"creationDate\": \"2020-03-30T15:44:47Z\", \"lastUpdated\": \"2020-03-30T15:44:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } In the JSON example above, the key is used with two service instances - an instance Databases for Redis and a Cloud Object Storage bucket. Notice that the Redis instance has two entries; that is because Key Protect registers usage at the resource level, not necessarily at the service instance level. In the case of Redis, the key is used in two separate PVCs, so there are two registrations, both that have the same value for the createdBy field, which contains the CRN of the Redis instance. Later in the key's lifecycle, operations on that key (rotation, deletion, crypto erasure) can be tracked by individual resource instance. Crypto Erasure It is sometimes necessary to rotate Key Protect Keys, whether it be proactive security measures to rotate periodically, or reactive, when it is possible that the key has become compromised. Key rotation actually involves two steps: Key Rotation : - action initiated in Key Protect to rotate the key. Key Protect replaces the old material with newly generated material. Only works for IBM-generated keys today. Key Replacement - action initiated by the service using the key when notified by Key Protect that the key has been rotated. The service does what it needs to to to replace the old material with the new material. Validation Steps To validate the behavior of this feature the following general steps can be followed. Prerequisites Key Protect Instance with an IBM-generated root key a service (i.e. Databases for Redis, Databases for PostGreS, Cloudant, Event Streams, Cloud Object Storage) that was provisioned using the key Validation steps Use the Key Protect API to verify that the key is being used. In this example, the key is being used for two service instances, one Redis and one ElasticSearch. The API will return JSON like this: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 5 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-4061efea-2c14-48b7-b016-86da46238b2b\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:48Z\", \"lastUpdated\": \"2020-03-31T13:15:48Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-781b92eb-c93a-49b8-96cc-d4b484d3bdeb\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-85b7b18d-e186-4a0e-be9c-09343571914e\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } Use the Key Protect API to determine the current version of the key: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"name\": \"dw-kp-reg-test-crk-1\", \"state\": 1, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4442aa89-9749-4cad-9a6e-73a77508a616\", \"imported\": false, \"creationDate\": \"2020-03-30T13:43:29Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-03-31T14:18:33Z\", \"lastRotateDate\": \"2020-03-31T14:18:33Z\", \"keyVersion\": { \"id\": \"b959b827-9453-4707-a92b-6e9d632c5b98\", \"creationDate\": \"2020-03-31T14:18:33Z\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": false }, \"deleted\": false } ] } Notice the keyVersion object; it shows the id and creation date of the current version of the key. In this example, the JSON above was obtained after the key rotation, so it shows a different version than what appears in the registrations. Go into the Key Protect instance and rotate the key. Once the key has been rotated you will see a popup saying that it has been done. This means that Key Protect replaced the material and notified the services about it. Review the Activity Tracker instance for the region in which the services are provisioned. Filter by sources, selecting kms and ibm-cloud-databases-prod . You should see an event from Key Protect (kms) that the key was rotated, and events from ICD that they completed the key replacement. Here is the detail for each event: Key Rotation: ElasticSearch: Redis: Note: if Key Protect doesn't get a report back from a service that the rotation has been completed it will send failure events to Activity Tracker: In this case, it turns out that there is a defect in the acknowledgement that ICD sends back to Key Protect to indicate that the rotation is complete. The error message shows that the Ack failed, but the rotation did complete. The ICD engineering team has already submitted a pull request with a fix for this issue, and as soon as it gets deployed these \"ack failures\" should no longer occur.","title":"Key Protect"},{"location":"cloud-platform/key-protect/#key-protect","text":"","title":"Key Protect"},{"location":"cloud-platform/key-protect/#restricting-access-to-keys","text":"Key Protect now supports the ability to assign access to a single key within a Key Protect instance to a given user or access group via Access Policy. If Kaiser Permanente wishes to maintain a single, shared instance of Key Protect and assign individual keys to any development team, this feature will allow it. When a user needs to specify a key for encryption of a particular service, that user will only see the instance of Key Protect to which the user has access and the specific key to which the user has been granted access. In the UI this will filter any dropdowns appropriately and in the CLI or API, access will also be limited. Documentation for setting this level of access can be found here . Note: Key level access is currently only available via access policy for a user or access group. It cannot be applied when granting Service-to-Service Authorization, which is access between two IBM services, such as COS and Key Protect.","title":"Restricting access to keys"},{"location":"cloud-platform/key-protect/#dual-authorization-delete","text":"This is a new feature that was added to Key Protect that provides the ability to create an instance level policy requiring that two different users approve the deletion of a key. In order to delete a key, a user with appropriate permissions must \"set the key for deletion\", which updates a flag on the key. After that, another user with permission to do so can delete the key. The person who does the first authorization cannot actually delete the key; it requires two different users to complete the key deletion. Note: At this time the \"first\" action to set the key for deletion can only be done via API. Once done, any available option for deleting a key (UI, CLI, API) can be used to actually delete the key. In order to make it easier to use this new feature IBM has provided (As-Is, not part of the product and not supported) a script that automates some of these actions using the API. The script can be found here . Considerations The Dual Auth Delete policy is applied at the individual key level, although it can be defined at the service instance level. Once the Dual Auth policy is set for a service instance, all keys created after that will automatically inherit that policy. Any keys that existed prior to the creation of the service instance level policy will NOT inherit the policy. If need be the policy can be set on individual keys. Note: This still needs to be validated.","title":"Dual Authorization Delete"},{"location":"cloud-platform/key-protect/#using-the-script","text":"The script leverages the Key Protect API via cURL commands. These commands require an auth token, which can be retrieved via the ibmcloud CLI. It also uses the CLI to look up information needed by some of the APIs. The script will automatically get the auth token and use the CLI to look things up, but it does require the user of the script to first login to the CLI in the same terminal/shell where the script will be run. Command Options NAME: keyprotect.sh - Manage features of Key Protect USAGE: keyprotect.sh <key-protect-instance-name> command [options] COMMANDS: ------------------------------------------------------------------------------------------- view-policies List the current policies for the Key Protect Instance enable-dual-auth Enable the Dual Authorization policy for key deletes for all keys disable-dual-auth Disable the Dual Authorization policy for key deletes for all keys view-keys List the keys in the Key Protect Instance in JSON format view-keys-list List the keys in the Key Protect Instance in list format view-deleted-keys List the deleted keys in the Key Protect Instance in JSON format view-deleted-keys-list List the deleted keys in the Key Protect Instance in list format view-key-material View the material for a standard key view-key-policies View the current polices for the specified key import-key Import a standard or root key restore-key Restore an imported key that has been deleted set-key-deletion Set the specified key for deletion (first auth) unset-key-deletion Unset the specified key for deletion, which removes the first auth help, h View help for this script Note: For your convenience this command executes the ibmcloud cli to look up certain information needed to perform these tasks. It requires you to be logged into the ibmcloud cli before you run this command.","title":"Using the script"},{"location":"cloud-platform/key-protect/#viewing-policies-for-a-key-protect-instance","text":"Command ./keyprotect.sh key-protect-dallas-dw view-policies Output Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 }, \"resources\": [] }","title":"Viewing policies for a Key Protect Instance"},{"location":"cloud-platform/key-protect/#enabling-the-dual-authorization-policy-for-a-key-protect-instance","text":"Command ./keyprotect.sh key-protect-dallas-dw enable-dual-auth Output Enabling Dual Authorization for service key-protect-dallas-dw... Done. Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true }, \"creation_date\": \"2020-01-16T20:47:30Z\", \"created_by\": \"IBMid-110000J6VA\", \"updated_by\": \"IBMid-110000J6VA\", \"last_updated\": \"2020-01-16T20:47:30Z\" } ] } Request complete.","title":"Enabling the Dual Authorization policy for a Key Protect Instance"},{"location":"cloud-platform/key-protect/#view-keys-in-json-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-keys Output Listing keys for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 2 }, \"resources\": [ { \"id\": \"a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-delete-crk\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 }, { \"id\": \"f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-cos-crk-1\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-17T21:44:39Z\", \"lastUpdateDate\": \"2020-01-17T21:44:39Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 } ] } Request complete.","title":"View Keys in JSON format"},{"location":"cloud-platform/key-protect/#view-keys-in-list-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-keys-list Output Listing keys for service key-protect-dallas-dw... name id crn dw-test-delete-crk a15604c8-e5c6-4fc9-ac92-5be79bdb1424 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424 dw-test-cos-crk-1 f41d77aa-f357-4234-ba46-6aec1b4a7f92 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92 Request complete.","title":"View Keys in list format"},{"location":"cloud-platform/key-protect/#view-deleted-keys-in-json-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys Output Listing keys for service key-protect-dallas-dw... Values for State field: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 4 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"name\": \"dw-ui-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"imported\": false, \"creationDate\": \"2020-01-16T22:40:15Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T22:40:15Z\", \"keyVersion\": { \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-01-23T22:41:02Z\" } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\", \"name\": \"dw-test-import-delete-restore-01\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7\", \"imported\": true, \"creationDate\": \"2020-05-01T21:31:47Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T21:31:47Z\", \"keyVersion\": { \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"name\": \"dw-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"imported\": false, \"creationDate\": \"2020-01-16T20:53:55Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T20:53:55Z\", \"keyVersion\": { \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"name\": \"${keyName}\", \"state\": 5, \"extractable\": true, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"imported\": false, \"creationDate\": \"2020-05-01T14:37:44Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T14:37:44Z\", \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-05-08T16:16:10Z\" } } ] }","title":"View Deleted Keys in JSON Format"},{"location":"cloud-platform/key-protect/#view-deleted-keys-in-list-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys-list Output Listing deleted keys for service key-protect-dallas-dw... Values for State column: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed name id state crn dw-ui-test-crk-1 38d3c46d-0f94-4843-ba36-9ac6c08c4c3c 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c dw-test-import-delete-restore-01 4b559191-e10e-4bf6-9891-9af1948b55f7 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7 dw-test-crk-1 75588eb5-81d4-4cdb-997c-b6deb6be06cf 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf ${keyName} 814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 Request complete.","title":"View Deleted Keys in List Format"},{"location":"cloud-platform/key-protect/#view-key-policies","text":"Command ./keyprotect.sh key-protect-dallas-dw view-key-policies a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Note: The key-specific commands require an extra parameter, the key id, which can be found in the UI, CLI or API. If not provided these commands will produce this error: for view-key-policies a key id is required USAGE: keyprotect.sh [service instance name] view-key-policies [key-id] Output viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\" } ] } Request complete.","title":"View Key Policies"},{"location":"cloud-platform/key-protect/#set-key-for-deletion","text":"Command ./keyprotect.sh key-protect-dallas-dw set-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Setting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:05:09Z\" } ] } Request complete. Note: If the key has not been enabled for Dual Auth Delete, the command will return the error below. Setting key 75588eb5-81d4-4cdb-997c-b6deb6be06cf in service key-protect-dallas-dw for deletion... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.error+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"errorMsg\": \"Action could not be performed on key. Please see `reasons` for more details.\", \"reasons\": [ { \"code\": \"NOT_DUAL_AUTH_ERR\", \"message\": \"The key is not dual auth enabled and cannot be set for deletion\", \"status\": 409, \"more_info\": \"https://cloud.ibm.com/apidocs/key-protect\" } ] } ] } Done. viewing policies for key 75588eb5-81d4-4cdb-997c-b6deb6be06cf... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 } } Request complete.","title":"Set Key for Deletion"},{"location":"cloud-platform/key-protect/#unset-key-for-deletion","text":"In some cases it may turn out that the first authorization was done in error. This command can be used to unset the key for deletion, which removes the first authorization. Command ./keyprotect.sh key-protect-dallas-dw unset-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Unsetting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:15:44Z\" } ] } Request complete. Note: Setting or unsetting the key for deletion does NOT affect the policy itself; as seen in the output above the policy is still set to true. At this time the \"set deletion\" status and its lastUpdated date do not show up in the API response. IBM is going to update the API in the next iteration to include these two fields.","title":"Unset key for deletion"},{"location":"cloud-platform/key-protect/#deleting-a-key-from-the-ui","text":"IMPORTANT WARNING: New functionality for Key Protect has been delivered that will prevent a key from being deleted when using the Key Protect API if that key is being used by any services. This feature has NOT YET been updated in the UI!!! If you delete the key in the UI and the key (or the Key Protect instance) does NOT have Dual Authorization Delete policy enabled, that key WILL get deleted, regardless of whether or not is being used. To protect against this possibility make sure that all Key Protect instances have the Dual Authorization Delete policy enabled and use the Registration API before deleting any key to see if it is being used by any services or resources. Note: The steps below for deleting a kay via the UI assume that the Dual Authorization Policy has been enabled. If you try to delete a key from the UI and it has not yet been set for deletion you will see an error like this: If you try to delete the key after it has been set for deletion and you're the one who did that you will see this error: Note: If you try to delete the key and somebody else set the key for deletion you will be able to delete the key.","title":"Deleting a Key from the UI"},{"location":"cloud-platform/key-protect/#viewing-usage-of-keys","text":"It is possible in IBM Cloud to track where Key Protect keys are being used. When a service is provisioned or configure to use a Key Protect key, it \"registers\" that usage with Key Protect. You can see the list of resources using a given key with a new operation in the Key Protect API . GET https://us-south.kms.cloud.ibm.com/api/v2/keys/4442aa89-9749-4cad-9a6e-73a77508a616/registrations Headers: Authorization: 'Bearer ' bluemix-instance: ' ' The URL endpoint will vary by region, and it contains the GUID of the key. { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 3 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8:bucket:dw-kp-bucket-test-registration-api-1578085-01\", \"createdBy\": \"crn-crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8::\", \"creationDate\": \"2020-03-30T15:44:47Z\", \"lastUpdated\": \"2020-03-30T15:44:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } In the JSON example above, the key is used with two service instances - an instance Databases for Redis and a Cloud Object Storage bucket. Notice that the Redis instance has two entries; that is because Key Protect registers usage at the resource level, not necessarily at the service instance level. In the case of Redis, the key is used in two separate PVCs, so there are two registrations, both that have the same value for the createdBy field, which contains the CRN of the Redis instance. Later in the key's lifecycle, operations on that key (rotation, deletion, crypto erasure) can be tracked by individual resource instance.","title":"Viewing usage of keys"},{"location":"cloud-platform/key-protect/#crypto-erasure","text":"It is sometimes necessary to rotate Key Protect Keys, whether it be proactive security measures to rotate periodically, or reactive, when it is possible that the key has become compromised. Key rotation actually involves two steps: Key Rotation : - action initiated in Key Protect to rotate the key. Key Protect replaces the old material with newly generated material. Only works for IBM-generated keys today. Key Replacement - action initiated by the service using the key when notified by Key Protect that the key has been rotated. The service does what it needs to to to replace the old material with the new material.","title":"Crypto Erasure"},{"location":"cloud-platform/key-protect/#validation-steps","text":"To validate the behavior of this feature the following general steps can be followed. Prerequisites Key Protect Instance with an IBM-generated root key a service (i.e. Databases for Redis, Databases for PostGreS, Cloudant, Event Streams, Cloud Object Storage) that was provisioned using the key Validation steps Use the Key Protect API to verify that the key is being used. In this example, the key is being used for two service instances, one Redis and one ElasticSearch. The API will return JSON like this: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 5 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-4061efea-2c14-48b7-b016-86da46238b2b\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:48Z\", \"lastUpdated\": \"2020-03-31T13:15:48Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-781b92eb-c93a-49b8-96cc-d4b484d3bdeb\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-85b7b18d-e186-4a0e-be9c-09343571914e\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } Use the Key Protect API to determine the current version of the key: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"name\": \"dw-kp-reg-test-crk-1\", \"state\": 1, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4442aa89-9749-4cad-9a6e-73a77508a616\", \"imported\": false, \"creationDate\": \"2020-03-30T13:43:29Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-03-31T14:18:33Z\", \"lastRotateDate\": \"2020-03-31T14:18:33Z\", \"keyVersion\": { \"id\": \"b959b827-9453-4707-a92b-6e9d632c5b98\", \"creationDate\": \"2020-03-31T14:18:33Z\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": false }, \"deleted\": false } ] } Notice the keyVersion object; it shows the id and creation date of the current version of the key. In this example, the JSON above was obtained after the key rotation, so it shows a different version than what appears in the registrations. Go into the Key Protect instance and rotate the key. Once the key has been rotated you will see a popup saying that it has been done. This means that Key Protect replaced the material and notified the services about it. Review the Activity Tracker instance for the region in which the services are provisioned. Filter by sources, selecting kms and ibm-cloud-databases-prod . You should see an event from Key Protect (kms) that the key was rotated, and events from ICD that they completed the key replacement. Here is the detail for each event: Key Rotation: ElasticSearch: Redis: Note: if Key Protect doesn't get a report back from a service that the rotation has been completed it will send failure events to Activity Tracker: In this case, it turns out that there is a defect in the acknowledgement that ICD sends back to Key Protect to indicate that the rotation is complete. The error message shows that the Ack failed, but the rotation did complete. The ICD engineering team has already submitted a pull request with a fix for this issue, and as soon as it gets deployed these \"ack failures\" should no longer occur.","title":"Validation Steps"},{"location":"cloud-platform/openshift-ibm-cloud/","text":"OpenShift on IBM Cloud This page will contain details and documentation about OpenShift 4.x on IBM Cloud as work progresses. Base Images Many IBM Cloud Dedicated customers will be primarily using two images that correspond to two commonly used buildpacks in Cloud Foundry today: WebSphere Liberty - Both the standard liberty and open liberty images are available now. The current version is 19.0.0.2, with many older versions available in the repo. The current version of the buildpack also uses 19.0.0.2. Node.js - IBM is no longer delivering builds of Node.js for buildpacks and is not delivering any images; instead IBM is providing support for the community buid. An IBM Team (the Cloud Engagement Hub) has built a guide for customizing images prior to loading them into an enterprise registry: https://medium.com/cloud-engagement-hub/so-you-want-customized-websphere-container-images-heres-how-42f0e598733f Operators Using the IBM Cloud Operator The IBM Cloud Operator is used to create services in IBM Cloud (i.e. Cloudant, PostGreSQL, Redis, etc.) and bindings in an OpenShift project that contain the credentials needed to programmatically access the service. The IBM Cloud Operator can be found on operatorhub.io . It is currently in the alpha channel. It may not appear in the OperatorHub Console in OpenShift right away after a new cluster is provisioned. There is post-provisioning configuration that will update the OperatorHub Console. If you don't see it right away and this is a new cluster, please wait awhile and look again. For more information see the Operators page in this wiki. Provision OpenShift on IBM Cloud As of June 16, 2020 OpenShift on IBM Cloud is now generally available on VPC Gen 2! It is now possible to provision a cluster on VPC Gen 2 with only private endpoints and a publicly resolvable but only privately accessible ingress subdomain. When provisioning with the CLI or Terraform use the disable_public_service_endpoint to provision the cluster with private only networking. Worker pools are required to have a minimum count of 2 nodes per availability zone. This change was made during the Early Access period, but really applies to all OpenShift clusters. The UI, CLI and Terraform will all enforce a minimum worker_count of 2. One other change that occurred during the Early Access period is that the storage configuration for the OpenShift internal registry was updated to use Cloud Object Storage. Prior to that the configuration was set to emptyDir: {} , which used ephemeral storage on the worker nodes. Images would be stored on the nodes, and if the node was rebooted or replaced the images would be lost. With the GA release cluster provisioning requires you to provide a valid CRN for a Cloud Object Storage instance. The docs have been updated to reflect new user access permissions required for Cloud Object Storage. When the cluster is provisioned it will perform the following additional actions: Create a new bucket in the COS instance (US-Geo, Standard tier) Create Service Credentials (including HMAC) Configure the Image Registry to use the bucket for storage of images. If for any reason these steps fail and the cluster is not able to access the instance or the bucket it will \"fail gracefully\" and revert back to the emptyDir: {} configuration. If that happens there are instructions in the troubleshooting docs for how to manually configure the cluster to use a bucket once the problems have been resolved. Provisioning with Terraform The IBM Terraform Provider now supports OpenShift on IBM Cloud in VPC Gen 2 as of v1.8.0 . It includes support for the new cos_instance_crn parameter that is required for OpenShift clusters. Examples for provisioning a cluster can be found here . Here is example terraform that was used to perform validation testing: Provider variable \"ibmcloud_api_key\" {} provider \"ibm\" { generation = var.generation region = var.region version = \"~> 1.8\" ibmcloud_api_key = var.ibmcloud_api_key } Variables variable \"environment\" { default = \"sandbox\" } variable \"vpc_name\" { default = \"sandbox-dallas\" } variable \"vpc_resource_group\" { default = \"vpc-sandbox\" } variable \"adm_resource_group\" { default = \"account-admin-services\" } variable \"env_resource_group\" { default = \"sandbox-tf-env\" } variable \"region\" { default = \"us-south\" } variable \"generation\" { default = 2 } Main data \"ibm_resource_group\" \"vpc_resource_group\" { name = \"${var.vpc_resource_group}\" } data \"ibm_resource_group\" \"adm_resource_group\" { name = \"${var.adm_resource_group}\" } data \"ibm_resource_group\" \"env_resource_group\" { name = \"${var.env_resource_group}\" } data \"ibm_resource_group\" \"cos_group\" { name = \"roks-cos-test-deleteme\" } data \"ibm_resource_instance\" \"cos_instance\" { name = \"cos-roks-internal-registry-test\" resource_group_id = data.ibm_resource_group.cos_group.id service = \"cloud-object-storage\" } ############################################################################## # Create OCP Cluster ############################################################################## resource \"ibm_container_vpc_cluster\" \"app_cluster2\" { name = \"${var.environment}-tf-02\" vpc_id = \"r006-a1705bea-d7ab-429c-8e17-00d21c6ffe83\" flavor = \"bx2.4x16\" kube_version = \"4.3_openshift\" worker_count = \"2\" entitlement = \"cloud_pak\" wait_till = \"MasterNodeReady\" disable_public_service_endpoint = false cos_instance_crn = data.ibm_resource_instance.cos_instance.id resource_group_id = data.ibm_resource_group.env_resource_group.id tags = [\"env:${var.environment}\",\"vpc:${var.vpc_name}\"] zones { subnet_id = \"0717-c63180ac-2765-4f91-b184-9373fb1514f2\" name = \"${var.region}-1\" } } Remove items from OpenShift Catalog This came up in a discussion with KP where we landed on the \"Add\" page in the Developer view of the OpenShift Console. If you click on \"From Catalog\" or \"Database\" you get taken to a Developer Catalog. Kaiser Permanente would like to completely remove the Developer Catalog, or at least remove all of the items in it. The items that show up in the \"From Catalog\" image above are controlled by the Samples Operator . Control over what shows up can be managed using Samples Operator configuration parameters . Specifically, if the managementState parameter is changed from Managed to Removed the set of managed imagestreams and templates in the openshift namespace are removed from the catalog. Using the OpenShift Console You can access the Samples Operator configuration using these instructions . It can also be viewed and changed from the UI using the Administrator -> Administration -> Custom Resource Definitions panel. Click on the Config CRD for samples.operator.openshift.io . Go to the Instances tab and click on cluster . Go to the YAML tab and change the managementState parameter from Managed to Removed : Click Save. Then you can go back to the Developer view, click Add and click on the From Catalog tile. The panel should be empty. A variation of this option would be to set the skippedTemplates parameter. This parameter tells the operator to ignore the templates in the list provided as the parameter's value. This is an easy way to drop a few charts from the catalog. The configuration above (setting managedState to Removed ) only affects the templates managed by the Samples operator. It will ignore any templates that a cluster administrator has added. For this reason, if there is need to remove most of the ones in the catalog but leave a few in, you can export the charts you want to keep, then set the managedState to Removed , then import back in the templates that you want to add back in. To export templates go to the openshift project in the Administrator view and search for Template Custom Resources. Click on the template you want to export, go to the YAML tab and copy/paste the yaml to a file. To do it with oc target the openshift project and run this command (using 3scale-gateway as an example): oc get template 3scale-gateway -o yaml > 3scale-gateway.yaml Using the CLI or Ansible Here is the patch command to use. I created a file called disablecatalog.yml with this content: spec: managementState: Removed Then I made sure I was logged into oc in a terminal and ran this command: oc patch configs.samples.operator.openshift.io/cluster --type merge --patch \"$(cat disablecatalog.yml)\" The command will return this: config.samples.operator.openshift.io/cluster patched After a minute or two the operator will finish removing the content and it will be gone from the From Catalog page. Note: Even with the tiles removed from the Samples catalog there may still be tiles for any operators that are installed in the cluster. See my comments above for more information about the object that gets patched. This Ansible playbook will also apply the patch. Forwarding logs to external LogStash (on-premise) These instructions can be run from a command line using oc authenticated to the target cluster. They were derived from the OpenShift documentation for installing the Cluster Logging Operator using the CLI . Here is an ansible playbook that was also used during testing. Create the namespace for logging Create a file named logging_namespace.yml with the following content: apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" Run this command to create the namespace: oc create -f logging_namespace.yml Create the Operator Group for the Cluster Logging Operator Create a file named og-clo.yml with the following content: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging Run this command: oc create -f og-clo.yml Install the Cluster Logging Operator Create a file named clo_subscription.yml with this content: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"4.3\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace Run this command: oc create -f clo_subscription.yml At this point the Cluster Logging Operator should be installed. Verify that the installation succeeded by viewing the Installed Operators page in the openshift-logging project in the web console. Create the ClusterLogging Custom Resource This custom resource tells the operator how to configure cluster logging. A \"full stack\" configuration includes a highly-available ElasticSearch cluster, fluentd and Kibana for viewing logs. Since Kaiser Permanente is going to be shipping logs on-premise to Splunk (via LogStash) the full stack is not required. Create a file named clusterlogging.yml with the following content: apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging annotations: clusterlogging.openshift.io/logforwardingtechpreview: enabled spec: managementState: Managed logStore: type: elasticsearch elasticsearch: # no elasticsearch pods nodeCount: 0 visualization: type: kibana kibana: # no kibana pods replicas: 0 curation: type: curator curator: # schedule the cron job to never run curator schedule: \"* * 31 2 *\" collection: logs: type: fluentd fluentd: {} Note: This configuration was originally tested with the expectation that at least one ElasticSearch pod would be required. The settings above in the ElasticSearch section would create a pod using the minimum CPU, memory and storage that would still run. During testing it was discovered that this configuration would work without ElasticSearch at all, so the nodeCount is set to 0, but the minimal configuration is still included for reference. This configuration will not install or configure ElasticSearch, and it will create a Kibana deployment with 0 replicas. Run this command: oc create -f clusterlogging.yml Configure Log Forwarding At this point the fluentd daemonset has been created and fluentd pods are running but they have nowhere to send the logs. The LogForwarding custom resource provides the additional configuration to set up log forwarding to LogStash. When you create the LogForwarding custom resource in this step, the fluentd pods will automatically be deleted and recreated by the operator to pick up the changes. The same thing happens if you delete the LogForwarding custom resource. Create a file named logforwarding.yml with the following content: NOTE: Be sure to update the endpoint parameter with the url for the on-premise logstash server. apiVersion: logging.openshift.io/v1alpha1 kind: LogForwarding metadata: name: instance namespace: openshift-logging spec: disableDefaultForwarding: true outputs: - endpoint: 'host:port' name: insecureforward type: forward pipelines: - inputSource: logs.app name: container-logs outputRefs: - insecureforward Disclaimer: This is a test configuration that uses insecure communication between the fluentd pods and the logstash server. Be sure to update the configuration to do secure forwarding. Details can be found in the OpenShift documentation . Run this command: oc create -f logforwarding.yml Once the fluentd pods restart the logs will start flowing to the external server.","title":"OpenShift on IBM Cloud"},{"location":"cloud-platform/openshift-ibm-cloud/#openshift-on-ibm-cloud","text":"This page will contain details and documentation about OpenShift 4.x on IBM Cloud as work progresses.","title":"OpenShift on IBM Cloud"},{"location":"cloud-platform/openshift-ibm-cloud/#base-images","text":"Many IBM Cloud Dedicated customers will be primarily using two images that correspond to two commonly used buildpacks in Cloud Foundry today: WebSphere Liberty - Both the standard liberty and open liberty images are available now. The current version is 19.0.0.2, with many older versions available in the repo. The current version of the buildpack also uses 19.0.0.2. Node.js - IBM is no longer delivering builds of Node.js for buildpacks and is not delivering any images; instead IBM is providing support for the community buid. An IBM Team (the Cloud Engagement Hub) has built a guide for customizing images prior to loading them into an enterprise registry: https://medium.com/cloud-engagement-hub/so-you-want-customized-websphere-container-images-heres-how-42f0e598733f","title":"Base Images"},{"location":"cloud-platform/openshift-ibm-cloud/#operators","text":"","title":"Operators"},{"location":"cloud-platform/openshift-ibm-cloud/#using-the-ibm-cloud-operator","text":"The IBM Cloud Operator is used to create services in IBM Cloud (i.e. Cloudant, PostGreSQL, Redis, etc.) and bindings in an OpenShift project that contain the credentials needed to programmatically access the service. The IBM Cloud Operator can be found on operatorhub.io . It is currently in the alpha channel. It may not appear in the OperatorHub Console in OpenShift right away after a new cluster is provisioned. There is post-provisioning configuration that will update the OperatorHub Console. If you don't see it right away and this is a new cluster, please wait awhile and look again. For more information see the Operators page in this wiki.","title":"Using the IBM Cloud Operator"},{"location":"cloud-platform/openshift-ibm-cloud/#provision-openshift-on-ibm-cloud","text":"As of June 16, 2020 OpenShift on IBM Cloud is now generally available on VPC Gen 2! It is now possible to provision a cluster on VPC Gen 2 with only private endpoints and a publicly resolvable but only privately accessible ingress subdomain. When provisioning with the CLI or Terraform use the disable_public_service_endpoint to provision the cluster with private only networking. Worker pools are required to have a minimum count of 2 nodes per availability zone. This change was made during the Early Access period, but really applies to all OpenShift clusters. The UI, CLI and Terraform will all enforce a minimum worker_count of 2. One other change that occurred during the Early Access period is that the storage configuration for the OpenShift internal registry was updated to use Cloud Object Storage. Prior to that the configuration was set to emptyDir: {} , which used ephemeral storage on the worker nodes. Images would be stored on the nodes, and if the node was rebooted or replaced the images would be lost. With the GA release cluster provisioning requires you to provide a valid CRN for a Cloud Object Storage instance. The docs have been updated to reflect new user access permissions required for Cloud Object Storage. When the cluster is provisioned it will perform the following additional actions: Create a new bucket in the COS instance (US-Geo, Standard tier) Create Service Credentials (including HMAC) Configure the Image Registry to use the bucket for storage of images. If for any reason these steps fail and the cluster is not able to access the instance or the bucket it will \"fail gracefully\" and revert back to the emptyDir: {} configuration. If that happens there are instructions in the troubleshooting docs for how to manually configure the cluster to use a bucket once the problems have been resolved.","title":"Provision OpenShift on IBM Cloud"},{"location":"cloud-platform/openshift-ibm-cloud/#provisioning-with-terraform","text":"The IBM Terraform Provider now supports OpenShift on IBM Cloud in VPC Gen 2 as of v1.8.0 . It includes support for the new cos_instance_crn parameter that is required for OpenShift clusters. Examples for provisioning a cluster can be found here . Here is example terraform that was used to perform validation testing: Provider variable \"ibmcloud_api_key\" {} provider \"ibm\" { generation = var.generation region = var.region version = \"~> 1.8\" ibmcloud_api_key = var.ibmcloud_api_key } Variables variable \"environment\" { default = \"sandbox\" } variable \"vpc_name\" { default = \"sandbox-dallas\" } variable \"vpc_resource_group\" { default = \"vpc-sandbox\" } variable \"adm_resource_group\" { default = \"account-admin-services\" } variable \"env_resource_group\" { default = \"sandbox-tf-env\" } variable \"region\" { default = \"us-south\" } variable \"generation\" { default = 2 } Main data \"ibm_resource_group\" \"vpc_resource_group\" { name = \"${var.vpc_resource_group}\" } data \"ibm_resource_group\" \"adm_resource_group\" { name = \"${var.adm_resource_group}\" } data \"ibm_resource_group\" \"env_resource_group\" { name = \"${var.env_resource_group}\" } data \"ibm_resource_group\" \"cos_group\" { name = \"roks-cos-test-deleteme\" } data \"ibm_resource_instance\" \"cos_instance\" { name = \"cos-roks-internal-registry-test\" resource_group_id = data.ibm_resource_group.cos_group.id service = \"cloud-object-storage\" } ############################################################################## # Create OCP Cluster ############################################################################## resource \"ibm_container_vpc_cluster\" \"app_cluster2\" { name = \"${var.environment}-tf-02\" vpc_id = \"r006-a1705bea-d7ab-429c-8e17-00d21c6ffe83\" flavor = \"bx2.4x16\" kube_version = \"4.3_openshift\" worker_count = \"2\" entitlement = \"cloud_pak\" wait_till = \"MasterNodeReady\" disable_public_service_endpoint = false cos_instance_crn = data.ibm_resource_instance.cos_instance.id resource_group_id = data.ibm_resource_group.env_resource_group.id tags = [\"env:${var.environment}\",\"vpc:${var.vpc_name}\"] zones { subnet_id = \"0717-c63180ac-2765-4f91-b184-9373fb1514f2\" name = \"${var.region}-1\" } }","title":"Provisioning with Terraform"},{"location":"cloud-platform/openshift-ibm-cloud/#remove-items-from-openshift-catalog","text":"This came up in a discussion with KP where we landed on the \"Add\" page in the Developer view of the OpenShift Console. If you click on \"From Catalog\" or \"Database\" you get taken to a Developer Catalog. Kaiser Permanente would like to completely remove the Developer Catalog, or at least remove all of the items in it. The items that show up in the \"From Catalog\" image above are controlled by the Samples Operator . Control over what shows up can be managed using Samples Operator configuration parameters . Specifically, if the managementState parameter is changed from Managed to Removed the set of managed imagestreams and templates in the openshift namespace are removed from the catalog.","title":"Remove items from OpenShift Catalog"},{"location":"cloud-platform/openshift-ibm-cloud/#using-the-openshift-console","text":"You can access the Samples Operator configuration using these instructions . It can also be viewed and changed from the UI using the Administrator -> Administration -> Custom Resource Definitions panel. Click on the Config CRD for samples.operator.openshift.io . Go to the Instances tab and click on cluster . Go to the YAML tab and change the managementState parameter from Managed to Removed : Click Save. Then you can go back to the Developer view, click Add and click on the From Catalog tile. The panel should be empty. A variation of this option would be to set the skippedTemplates parameter. This parameter tells the operator to ignore the templates in the list provided as the parameter's value. This is an easy way to drop a few charts from the catalog. The configuration above (setting managedState to Removed ) only affects the templates managed by the Samples operator. It will ignore any templates that a cluster administrator has added. For this reason, if there is need to remove most of the ones in the catalog but leave a few in, you can export the charts you want to keep, then set the managedState to Removed , then import back in the templates that you want to add back in. To export templates go to the openshift project in the Administrator view and search for Template Custom Resources. Click on the template you want to export, go to the YAML tab and copy/paste the yaml to a file. To do it with oc target the openshift project and run this command (using 3scale-gateway as an example): oc get template 3scale-gateway -o yaml > 3scale-gateway.yaml","title":"Using the OpenShift Console"},{"location":"cloud-platform/openshift-ibm-cloud/#using-the-cli-or-ansible","text":"Here is the patch command to use. I created a file called disablecatalog.yml with this content: spec: managementState: Removed Then I made sure I was logged into oc in a terminal and ran this command: oc patch configs.samples.operator.openshift.io/cluster --type merge --patch \"$(cat disablecatalog.yml)\" The command will return this: config.samples.operator.openshift.io/cluster patched After a minute or two the operator will finish removing the content and it will be gone from the From Catalog page. Note: Even with the tiles removed from the Samples catalog there may still be tiles for any operators that are installed in the cluster. See my comments above for more information about the object that gets patched. This Ansible playbook will also apply the patch.","title":"Using the CLI or Ansible"},{"location":"cloud-platform/openshift-ibm-cloud/#forwarding-logs-to-external-logstash-on-premise","text":"These instructions can be run from a command line using oc authenticated to the target cluster. They were derived from the OpenShift documentation for installing the Cluster Logging Operator using the CLI . Here is an ansible playbook that was also used during testing.","title":"Forwarding logs to external LogStash (on-premise)"},{"location":"cloud-platform/openshift-ibm-cloud/#create-the-namespace-for-logging","text":"Create a file named logging_namespace.yml with the following content: apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" Run this command to create the namespace: oc create -f logging_namespace.yml","title":"Create the namespace for logging"},{"location":"cloud-platform/openshift-ibm-cloud/#create-the-operator-group-for-the-cluster-logging-operator","text":"Create a file named og-clo.yml with the following content: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging Run this command: oc create -f og-clo.yml","title":"Create the Operator Group for the Cluster Logging Operator"},{"location":"cloud-platform/openshift-ibm-cloud/#install-the-cluster-logging-operator","text":"Create a file named clo_subscription.yml with this content: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"4.3\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace Run this command: oc create -f clo_subscription.yml At this point the Cluster Logging Operator should be installed. Verify that the installation succeeded by viewing the Installed Operators page in the openshift-logging project in the web console.","title":"Install the Cluster Logging Operator"},{"location":"cloud-platform/openshift-ibm-cloud/#create-the-clusterlogging-custom-resource","text":"This custom resource tells the operator how to configure cluster logging. A \"full stack\" configuration includes a highly-available ElasticSearch cluster, fluentd and Kibana for viewing logs. Since Kaiser Permanente is going to be shipping logs on-premise to Splunk (via LogStash) the full stack is not required. Create a file named clusterlogging.yml with the following content: apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging annotations: clusterlogging.openshift.io/logforwardingtechpreview: enabled spec: managementState: Managed logStore: type: elasticsearch elasticsearch: # no elasticsearch pods nodeCount: 0 visualization: type: kibana kibana: # no kibana pods replicas: 0 curation: type: curator curator: # schedule the cron job to never run curator schedule: \"* * 31 2 *\" collection: logs: type: fluentd fluentd: {} Note: This configuration was originally tested with the expectation that at least one ElasticSearch pod would be required. The settings above in the ElasticSearch section would create a pod using the minimum CPU, memory and storage that would still run. During testing it was discovered that this configuration would work without ElasticSearch at all, so the nodeCount is set to 0, but the minimal configuration is still included for reference. This configuration will not install or configure ElasticSearch, and it will create a Kibana deployment with 0 replicas. Run this command: oc create -f clusterlogging.yml","title":"Create the ClusterLogging Custom Resource"},{"location":"cloud-platform/openshift-ibm-cloud/#configure-log-forwarding","text":"At this point the fluentd daemonset has been created and fluentd pods are running but they have nowhere to send the logs. The LogForwarding custom resource provides the additional configuration to set up log forwarding to LogStash. When you create the LogForwarding custom resource in this step, the fluentd pods will automatically be deleted and recreated by the operator to pick up the changes. The same thing happens if you delete the LogForwarding custom resource. Create a file named logforwarding.yml with the following content: NOTE: Be sure to update the endpoint parameter with the url for the on-premise logstash server. apiVersion: logging.openshift.io/v1alpha1 kind: LogForwarding metadata: name: instance namespace: openshift-logging spec: disableDefaultForwarding: true outputs: - endpoint: 'host:port' name: insecureforward type: forward pipelines: - inputSource: logs.app name: container-logs outputRefs: - insecureforward Disclaimer: This is a test configuration that uses insecure communication between the fluentd pods and the logstash server. Be sure to update the configuration to do secure forwarding. Details can be found in the OpenShift documentation . Run this command: oc create -f logforwarding.yml Once the fluentd pods restart the logs will start flowing to the external server.","title":"Configure Log Forwarding"},{"location":"custom-domain/create-domain/","text":"Custom Domains Creating your own domain A domain is created by using one of the many services called Registrars that will do all of the things that are required for your domain. Specifically they will be the custodian of your domain with all of the appropriate information about you. You can use any registrar you like; there are many of them out there. IBM provides this capability with IBM Cloud. This document will use IBM Cloud as the registrar. To create your domain: Login to IBM Cloud and navigate to the catalog. Search for \"domain\". Click on the Domain Name Service tile as seen in the image above. On this page click the blue Create button. Expand the Register menu, enter your domain and click the Check Availability button, as shown in the image above. If your domain is available, click Continue . Provide the required information as shown in the screen above and click Order Now . You should see a confirmation window: Click OK . Then click on the refresh icon in the list menu. You can see your new domain! Notice that it's not \"verified\" yet. You will get an email from Softlayer that confirms that you've ordered your domain. The email will look something like this: After some time (could be minutes) Softlayer does things to verify your domain. Wait a few minutes and refresh the domain list. When your domain is verified you will see the Verified column change, as shown below: That's it! Your domain is now ready for use.","title":"Create a domain"},{"location":"custom-domain/create-domain/#custom-domains","text":"","title":"Custom Domains"},{"location":"custom-domain/create-domain/#creating-your-own-domain","text":"A domain is created by using one of the many services called Registrars that will do all of the things that are required for your domain. Specifically they will be the custodian of your domain with all of the appropriate information about you. You can use any registrar you like; there are many of them out there. IBM provides this capability with IBM Cloud. This document will use IBM Cloud as the registrar. To create your domain: Login to IBM Cloud and navigate to the catalog. Search for \"domain\". Click on the Domain Name Service tile as seen in the image above. On this page click the blue Create button. Expand the Register menu, enter your domain and click the Check Availability button, as shown in the image above. If your domain is available, click Continue . Provide the required information as shown in the screen above and click Order Now . You should see a confirmation window: Click OK . Then click on the refresh icon in the list menu. You can see your new domain! Notice that it's not \"verified\" yet. You will get an email from Softlayer that confirms that you've ordered your domain. The email will look something like this: After some time (could be minutes) Softlayer does things to verify your domain. Wait a few minutes and refresh the domain list. When your domain is verified you will see the Verified column change, as shown below: That's it! Your domain is now ready for use.","title":"Creating your own domain"},{"location":"custom-domain/domain-with-openshift/","text":"Using your custom domain with OpenShift Now that you have your spiffy new custom domain, let's put it to work! The steps documented here assume that you have done the following: Registered your custom domain Provisioned an instance of IBM Cloud Internet Services and configured it to manage your domain Ordered origin certificates in IBM Cloud Internet Services Ordered edge certificates in IBM Cloud Internet Services I used this page in the OpenShift docs to create the secured route for my application. Creating an edge route with a custom certificate","title":"Configure domain for OpenShift"},{"location":"custom-domain/domain-with-openshift/#using-your-custom-domain-with-openshift","text":"Now that you have your spiffy new custom domain, let's put it to work! The steps documented here assume that you have done the following: Registered your custom domain Provisioned an instance of IBM Cloud Internet Services and configured it to manage your domain Ordered origin certificates in IBM Cloud Internet Services Ordered edge certificates in IBM Cloud Internet Services I used this page in the OpenShift docs to create the secured route for my application. Creating an edge route with a custom certificate","title":"Using your custom domain with OpenShift"},{"location":"custom-domain/manage-domain/","text":"Manage your domain with IBM Cloud Internet Services If you have your own domain you can manage it using IBM Cloud Internet Services. This service provides a number of management, reliability, performance and security features for your domain. You can learn more about IBM Cloud Internet Services here . Provision Cloud Internet Services The first steps to using Cloud Internet Services is to provision an instance of it in your IBM Cloud account. Login to IBM Cloud, navigate to the catalog and search for \"internet\". Click on the tile for Internet Services. Select the Standard plan, scroll down and provide a name for your instance and a resource group, then click Create . You will see the screen below, where you will add your domain to your CIS instance. Click on the Let's Get Started button to continue. Add your domain to Cloud Internet Services You should seen the screen below. Enter your domain in the screen above and click Connect and Continue . If you have DNS records in your existing DNS provider you can export them and import them here. In this case we don't have any existing records, so click Next Step . The next step is VERY IMPORTANT! It tells your registrar (where you registered your domain) to delegate to CIS for name servers. You need to update the regisrar with the new name servers in the New NS records table above. In the case where you used Softlayer to register your domain, here are the steps to follow. Go to Classic Infrastructure in IBM Cloud. Expand the Services menu on the left and click on \"Domain Registration\". On the page below, click the arrow to the left of your domain name to expand it. Make sure the \"Lock Domain\" dropdown is set to \"Unlocked\" and click the \"Add / Edit DNS\" link for your domain. Replace the two existing name servers with the two from your CIS domain. When you first click the link the screen will look like this: When you replace these name servers with new ones it will look something like this: Note: Your name servers will be different, but they should match what you have on the CIS screen below. If they do, click \"Associate\" on the screen above. Go back to the CIS screen. Click \"Check Name Servers\" to validate that your name servers were updated. You might get a warning saying that the name servers were not updated. This is okay; it takes a few minutes for the updates to propagate. If that happens, refresh the page and try again in a few minutes. You may also get an error saying that \"you can only perform this action once per hour\". This is also okay. If you refresh the page again you will eventually see that your domain has become active. When that happens it means that now Cloud Internet Services is managing your domain! There is one more step that you should do at this point. By default, the Web Application Firewall is not enabled for your domain. To enable it, click on the Web Application Firewall link on the overview page and move the slider to the right. Adding DNS records To add a DNS record for your application you will create either an A or CNAME record. An A record maps your application to a specific IP address, whereas a CNAME record creates a mapping from your domain to another hostname. When you use a CNAME record you don't need to know the specific address, nor do you need to know if it ever changes. When you create an kubernetes cluster in the IBM Cloud Kubernetes Service (IKS), or an IBM Cloud Foundry Enterprise Environment (which is built on IKS), an ingress subdomain is created automatically for you and is managed by IKS. When you create your own host name, say myapp.eyebeemdemos.com , and that application is running in Cloud Foundry Enterprise, you will need to create a CNAME record for it in Cloud Internet Services. To add a CNAME record for you application, go to the Overview page for your Cloud Internet Services instance. Click on the link for DNS Records . Scroll down to the DNS Records section. Change the record type to CNAME , enter your app name in the Name field and the ingress subdomain for your Cloud Foundry Enterprise cluster in the Alias Domain Name field. Click the Add Record button. You will see your new record show up in the list. To enable the security features (ie. TLS) you need to enable the proxy by clicking on the slider. Now your CNAME record has been added, but it may still take a few minutes for it to be available, as the DNS record needs to be propagated out to all the other DNS servers. When it is ready you will be able to ping it. Note The address you see when you ping your hostname is actually an IP address for your Cloud Internet Services instance. All traffic gets routed there first so that security features can be applied. This also includes the TLS-enabled connection between Cloud Internet Services and your Cloud Foundry Enterprise cluster. Ordering Origin Certificates Ordering Edge Certificates To properly encrypt traffic between your domain in CIS and the browser you need to have edge certificates. Without them the browser will not trust your application because it did not present a certificate signed by a trusted authority. Edge certificates are signed by a trusted authority. To order edge certificates for your domain in IBM Cloud Internet Services, go to the Security -> TLS pane in the left nav, then click the button on the right to order a certificate. Note You can add multiple subdomains to the certificate when you order it. This will allow you to use a single certificate for all of the subdomains you add to it. You can always order another certificate if you add subdomains later, but you cannot add them to an existing certificate. Once you have completed the order you will see them listed: You can view the details of the certificate by clicking the three dots to the right. The details include a list of all of the subdomains covered by the certificate. Here is what the TLS page should look like: Configuring your domain in Cloud Foundry Enterprise Setting up your custom domain for use in Cloud Foundry Enterprise is pretty simple. You will want to order an origin certificate for your domain in Cloud Internet Services, as shown in the previous section. This will allow you to use TLS for communication from Internet Services to your Cloud Foundry Enterprise cluster. You will also need a DNS CNAME record for your application so that Internet Services will route traffic to your Cloud Foundry Enterprise cluster. The diagram below shows the basic configuration for using your custom domain. The Edge Services box on the left is your Cloud Internet Services instance. It contains the origin certificates you ordered above, and the DNS CNAME record that maps your hostname, myapp.<your-domain>.com to the ingress subdomain that was created by your Cloud Foundry Enterprise cluster. The ingress subdomain is how all traffic gets routed to your cluster.","title":"Manage your domain"},{"location":"custom-domain/manage-domain/#manage-your-domain-with-ibm-cloud-internet-services","text":"If you have your own domain you can manage it using IBM Cloud Internet Services. This service provides a number of management, reliability, performance and security features for your domain. You can learn more about IBM Cloud Internet Services here .","title":"Manage your domain with IBM Cloud Internet Services"},{"location":"custom-domain/manage-domain/#provision-cloud-internet-services","text":"The first steps to using Cloud Internet Services is to provision an instance of it in your IBM Cloud account. Login to IBM Cloud, navigate to the catalog and search for \"internet\". Click on the tile for Internet Services. Select the Standard plan, scroll down and provide a name for your instance and a resource group, then click Create . You will see the screen below, where you will add your domain to your CIS instance. Click on the Let's Get Started button to continue.","title":"Provision Cloud Internet Services"},{"location":"custom-domain/manage-domain/#add-your-domain-to-cloud-internet-services","text":"You should seen the screen below. Enter your domain in the screen above and click Connect and Continue . If you have DNS records in your existing DNS provider you can export them and import them here. In this case we don't have any existing records, so click Next Step . The next step is VERY IMPORTANT! It tells your registrar (where you registered your domain) to delegate to CIS for name servers. You need to update the regisrar with the new name servers in the New NS records table above. In the case where you used Softlayer to register your domain, here are the steps to follow. Go to Classic Infrastructure in IBM Cloud. Expand the Services menu on the left and click on \"Domain Registration\". On the page below, click the arrow to the left of your domain name to expand it. Make sure the \"Lock Domain\" dropdown is set to \"Unlocked\" and click the \"Add / Edit DNS\" link for your domain. Replace the two existing name servers with the two from your CIS domain. When you first click the link the screen will look like this: When you replace these name servers with new ones it will look something like this: Note: Your name servers will be different, but they should match what you have on the CIS screen below. If they do, click \"Associate\" on the screen above. Go back to the CIS screen. Click \"Check Name Servers\" to validate that your name servers were updated. You might get a warning saying that the name servers were not updated. This is okay; it takes a few minutes for the updates to propagate. If that happens, refresh the page and try again in a few minutes. You may also get an error saying that \"you can only perform this action once per hour\". This is also okay. If you refresh the page again you will eventually see that your domain has become active. When that happens it means that now Cloud Internet Services is managing your domain! There is one more step that you should do at this point. By default, the Web Application Firewall is not enabled for your domain. To enable it, click on the Web Application Firewall link on the overview page and move the slider to the right.","title":"Add your domain to Cloud Internet Services"},{"location":"custom-domain/manage-domain/#adding-dns-records","text":"To add a DNS record for your application you will create either an A or CNAME record. An A record maps your application to a specific IP address, whereas a CNAME record creates a mapping from your domain to another hostname. When you use a CNAME record you don't need to know the specific address, nor do you need to know if it ever changes. When you create an kubernetes cluster in the IBM Cloud Kubernetes Service (IKS), or an IBM Cloud Foundry Enterprise Environment (which is built on IKS), an ingress subdomain is created automatically for you and is managed by IKS. When you create your own host name, say myapp.eyebeemdemos.com , and that application is running in Cloud Foundry Enterprise, you will need to create a CNAME record for it in Cloud Internet Services. To add a CNAME record for you application, go to the Overview page for your Cloud Internet Services instance. Click on the link for DNS Records . Scroll down to the DNS Records section. Change the record type to CNAME , enter your app name in the Name field and the ingress subdomain for your Cloud Foundry Enterprise cluster in the Alias Domain Name field. Click the Add Record button. You will see your new record show up in the list. To enable the security features (ie. TLS) you need to enable the proxy by clicking on the slider. Now your CNAME record has been added, but it may still take a few minutes for it to be available, as the DNS record needs to be propagated out to all the other DNS servers. When it is ready you will be able to ping it. Note The address you see when you ping your hostname is actually an IP address for your Cloud Internet Services instance. All traffic gets routed there first so that security features can be applied. This also includes the TLS-enabled connection between Cloud Internet Services and your Cloud Foundry Enterprise cluster.","title":"Adding DNS records"},{"location":"custom-domain/manage-domain/#ordering-origin-certificates","text":"","title":"Ordering Origin Certificates"},{"location":"custom-domain/manage-domain/#ordering-edge-certificates","text":"To properly encrypt traffic between your domain in CIS and the browser you need to have edge certificates. Without them the browser will not trust your application because it did not present a certificate signed by a trusted authority. Edge certificates are signed by a trusted authority. To order edge certificates for your domain in IBM Cloud Internet Services, go to the Security -> TLS pane in the left nav, then click the button on the right to order a certificate. Note You can add multiple subdomains to the certificate when you order it. This will allow you to use a single certificate for all of the subdomains you add to it. You can always order another certificate if you add subdomains later, but you cannot add them to an existing certificate. Once you have completed the order you will see them listed: You can view the details of the certificate by clicking the three dots to the right. The details include a list of all of the subdomains covered by the certificate. Here is what the TLS page should look like:","title":"Ordering Edge Certificates"},{"location":"custom-domain/manage-domain/#configuring-your-domain-in-cloud-foundry-enterprise","text":"Setting up your custom domain for use in Cloud Foundry Enterprise is pretty simple. You will want to order an origin certificate for your domain in Cloud Internet Services, as shown in the previous section. This will allow you to use TLS for communication from Internet Services to your Cloud Foundry Enterprise cluster. You will also need a DNS CNAME record for your application so that Internet Services will route traffic to your Cloud Foundry Enterprise cluster. The diagram below shows the basic configuration for using your custom domain. The Edge Services box on the left is your Cloud Internet Services instance. It contains the origin certificates you ordered above, and the DNS CNAME record that maps your hostname, myapp.<your-domain>.com to the ingress subdomain that was created by your Cloud Foundry Enterprise cluster. The ingress subdomain is how all traffic gets routed to your cluster.","title":"Configuring your domain in Cloud Foundry Enterprise"},{"location":"dev-tools/crw/","text":"CodeReady Workspaces Overview One key tool that simplifies the life of a developer is CodeReady Workspaces . This section describes the steps to install and configure CodeReady Workspaces in OpenShift in IBM Cloud . It assumes that you already have a cluster, and that it is provisioned in VPC. Steps: Access the OpenShift console, access the OAuth CRD and add an identity provider. Use Basic Authentication and use a fake URL, like https://doesnotexist.io. Install CRW using the operator. Let it use openshift-workspaces as the project. Configure the Che Cluster. Change the PVC claim size from 1Gi to 10Gi. This is because the minimum volume size in VPC is 10GB. VPC does not support Read Write Many (RWX) so the same","title":"CodeReady Workspaces"},{"location":"dev-tools/crw/#codeready-workspaces","text":"","title":"CodeReady Workspaces"},{"location":"dev-tools/crw/#overview","text":"One key tool that simplifies the life of a developer is CodeReady Workspaces . This section describes the steps to install and configure CodeReady Workspaces in OpenShift in IBM Cloud . It assumes that you already have a cluster, and that it is provisioned in VPC. Steps: Access the OpenShift console, access the OAuth CRD and add an identity provider. Use Basic Authentication and use a fake URL, like https://doesnotexist.io. Install CRW using the operator. Let it use openshift-workspaces as the project. Configure the Che Cluster. Change the PVC claim size from 1Gi to 10Gi. This is because the minimum volume size in VPC is 10GB. VPC does not support Read Write Many (RWX) so the same","title":"Overview"},{"location":"dev-tools/dev-experience/","text":"The Developer Experience Cloud native development ushers in a new set of tools, technologies and practices for developing applications more quickly and higher levels of quality and security.","title":"Overview"},{"location":"dev-tools/dev-experience/#the-developer-experience","text":"Cloud native development ushers in a new set of tools, technologies and practices for developing applications more quickly and higher levels of quality and security.","title":"The Developer Experience"},{"location":"disaster-recovery/disaster-recovery/","text":"Disaster Recovery Overview The approach to defining the disaster recover strategy needs to be systematic, and start with the \"application\", which is defined as a set of compute resources (IKS or OpenShift apps, VSIs, services, etc.) that make up a \"business application\". While a holistic approach may be desired, the reality is that each business application is independent, with its own RTO/RPO requirements, which for many customers is expressed in the form of a set of service classes or tiers. Resiliency Tier Recovery Description Recovery Time Objective Recovery Point Objective Tier 1 Continuous Availabilty <= 1 Hour <= 1 Hour Tier 2 Advanced Recovery > 1 Hrs - <= 24 Hrs <2 Hrs - <24 Hrs Tier 3 Standard Recovery > 24 Hrs - <= 72 Hrs Last Backup Tier 4 No Recovery N/A N/A Note: The IBM Cloud platform itself must support the LOWEST number in the range. Therefore, for the majority of their applications (Tier 1 and Tier 2), the IBM Cloud services (OpenShift, IKS, ICD, Cloudant, ICOS, Key Protect, Push Notifications, etc.) must have options/capabilities for support application recovery in 1 hour or less. For databases that require backup/restore, in order to qualify for Tier 2 the backups need to be every two hours? Sounds like the app owners can choose their RPO, so they could schedule their backups accordingly to fit both RTO and RPO they decide for themselves. Since each business application will have a unique set of compute, service and other resources, each one will need to be reviewed to make sure that the strategy and requirements for DR for that app are understood, documented and implemented (automation?) before going to production. To build the framework that will drive that analysis and work we will profile three business applications with comment sets of resources to create the scaffolding that application teams can use for their own applications. High Availability and Disaster Recovery When dealing with improved resilience it important to make some distinctions between High Availability (HA) and Disaster Recovery (DR). HA is mainly about keeping the service available to the end users when \"ordinary\" activities are performed on the system like deploying updates, rebooting the hosting Virtual Machines, applying security patches to the hosting OS, etc. For our purposes, High Availability within a single site can be achieved by eliminating single points of failure. The Blue Compute sample application in its current form implements high availability. HA usually doesn't deal with major unplanned (or planned) issues such as complete site loss due to major power outages, earthquakes, severe hardware failures, full-site connectivity loss, etc. In such cases, if the service must meet strict Service Level Objectives (SLO), you should make the whole application stack (infrastructure, services and application components) redundant by deploying it in at least two different Bluemix regions. This is typically defined as a DR Architecture. There are many options to implement DR solutions. For the sake of simplicity, we can group the different options in three major categories: Active/Passive - Active/Passive options are based on keeping the full application stack active in one location, while another application stack is deployed in a different location, but kept idle (or shut down). In the case of prolonged unavailability of the primary site, the application stack is activated in the backup site. Often that requires the restoring of backups taken in the primary site. This approach is not recommended when loosing data can be a problem (e.g. when the Recovery Point Objective (RPO) is less than a few hours ) or when the availability of the service is critical (e.g. when the Return to Operations (RTO) objective is less than a few hours). Active/Standby - In the Active/Standby case the full application stack is active in both primary and backup location, however users transactions are served only by the primary site. The backup site takes care of keeping a replica of the status of the main location though data replication (such as DB replication or disk replication). In case of prolonged unavailability of the primary site, all client transactions are routed to the backup site. This approach provides quite good RPO and RTO (generally measured in minutes), however it is significantly more expensive than the Active/Passive options because of the double deployment (e.g., resources are wasted because the Stand by assets can't be used to improve scalability and throughput). Active/Active - In the Active/Active case both locations are active and client transactions are distributed according to predefined policies (such as round-robin, geographical load balancing, etc. ) to both regions. In the case of failure of one site the other site must be able to serve all clients. It's possible to achieve both an RPO and RTO close to zero with this configuration. The drawback is that both regions must be sized to handle the full load, even if they are used at the half of their capabilities when both locations are available. In such cases the Bluemix Autoscaling service can help in keeping always resources allocated according to the needs (as happens with the BlueCompute sample application). *Source: Making Microservices Resilient Cloud Architecture Network Architecture Sample Application Architecture Application Profiles One approach is to build a set of architecture profiles that represent the majority of apps. These profiles would include options for various classes of service, compute requirements, and all the other stuff listed below. Tier 1 Tier 1 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured CFEE provisioned in the VPC Org, spaces, custom domains and other custom configurations needed All applications deployed with the same version as deployed in primary MZR Application: All applications deployed with the same version as deployed in primary MZR Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, etc.) Note: Databases that use backup/restore as their only cross region replication option will not support Tier 1 availability unless the backups occur hourly and a restore can be completed in less than an hour. Service credentials and service bindings Tier 1 Compute In order to meet Tier 1 RTO/RPO these components will need to be preconfigured and at their production workload capacity in US East. The size of the CF Enterprise or IKS clusters in US East may only need to be large enough to support Tier 1 and 2 workloads. All applications can always be deployed there (as they are today in Dedicated) but for Tier 3 and 4 applications they could be stopped. The idea would be to allow for Tier 1 and 2 apps to immediately become available with enough capacity to run them should a failure occur, but to save money the cluster would only be scaled up to match the primary cluster (and support the Tier 3/4 workloads) when actually needed, as Tier 3/4 apps have longer RTO in which the scaling operation would complete. The same technique could be applied to updates to the CF Enterprise or IKS clusters. The first step would be to scale up the backup cluster to full capacity (matching the primary cluster) before starting the upgrade. Then do some sort of blue/green update where workload could be switched to the backup while the primary cluster is upgraded. Once the primary cluster is upgraded and verified, workload would be switched back, the backup cluster could be scaled back down and then upgraded. Gaps/Unknowns Where will traffic routing be managed/changed in order to redirect from primary to backup site? Where is it done today? For databases, do we have any issues or constraints with scaling in terms of timing? For example, for Cloudant Enterprise, will there always be enough capacity in the backup MZR such that all databases in the instance in the primary MZR are able to be replicated or restored? If not this could be an impact on RTO for apps that use Cloudant. Tier 1 Application - Cloudant In this scenario the data will be replicated automatically by Cloudant. In the event of a disaster the only change needed will be to reconfigure DNS to point to the cluster in US East. Benefits - Fastest recovery time, dependent only on the time it takes to verify that the data was replicated and make the DNS switch Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful. Tier 1 Application - PostGreSQL In this scenario the data will be replicated automatically by IBM Cloud Databases to a read-only replica in the backup MZR. In the event of a disaster the application team will need to manually trigger a promotion of the read-only replica to become the leader. This action will take some time; as a read-only replica the service instance is NOT configured using an HA topology . When the promotion occurs several steps happen to elevate the instance to an HA configuration. The only other change needed will be to reconfigure DNS to point to the cluster in US East, once the database promotion is complete. Benefits - Faster recovery time, as the data is already replicated to the backup MZR. There is still some latency related to the time it takes to reconfigure the database to an HA configuration. Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful. - Manual intervention is required to trigger and monitor the promotion of the read-only replica to leader status. Configuring read-only replicas Basic Steps: create instance in primary MZR (i.e. Dallas) create read-only replica in DR MZR (i.e. Washington, DC) Read-only replica is a single zone instance If primary MZR is unavailable, promote read-only replica to leader. Now DR MZR becomes the leader updates the config in the DR MZR to be MZR resilient, meaning additional nodes are added takes a full backup of the database in the DR MZR DR MZR becomes the leader and ties to original leader in primary MZR are broken original instance in Primary MZR can be deleted. This deletes all backups in the primary MZR. if original instance is not deleted (i.e. backups are still available) a new instance can be created by restoring from a backup, should that become necessary. the backups taken in the original MZR are still accessible even if that MZR is completely unavailable; they are stored in cross-regional ICOS buckets. If a read-only replica is promoted to leader, the original leader is no longer viable. To move the data/workload back to the original MZR, create a read-replica in the original MZR and promote it to be the leader. This does create a new instance of the database, not a restore of the original instance. Tier 1 Application - Cloud Object Storage In this scenario the data is always available in both MZRs via cross-regional buckets. Therefore, the only change needed is to update the DNS routing to point to the backup MZR. Tier 2 Tier 2 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: - VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured - CFEE provisioned in the VPC - Org, spaces, custom domains and other custom configurations needed - All applications deployed with the same version as deployed in primary MZR Application: - All applications deployed with the same version as deployed in primary MZR - Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, backup/restore, etc.) - Service credentials and service bindings Tier 2 Application - MongoDB In this scenario, the data is not replicated to the backup MZR. When a disaster is declared the application team will need to create a new database instance in the backup MZR by restoring the data from backup. Note: The backup from the database instance in the Primary MZR is available in the backup MZR even if the primary MZR is completely unavailable. Once the data is restored, the only other change needed is to change the DNS routing to point to the backup MZR. Backup and restore procedures ElasticSearch MongoDB Redis References IBM Kubernetes Service Multi-Region Architecture Strategies for Resilient Applications Secure web applications across multiple regions Resilient and secure multi-region Kubernetes clusters with IBM Cloud Internet Services ibm-cloud-services-resilience This repository provides links to the HA / DR / Backup information for the IBM Cloud Services in Scope. Services DR Capabilities DR Ownership/Control Documentation RTO Console Core service available in multiple regions IBM N/A HA IAM Core Service available in multiple regions IBM N/A HA CFEE Service available in multiple regions Client Tgt. end of November based on Client DR Implementation Cloudant Service available in multiple regions Client Disaster recovery and backup based on Client DR Implementation IBM Cloud DB Elastic Search Service available in multiple regions Client Managing Backups based on on Client DR Implementation IBM Cloud DB Mongo DB Service available in multiple regions Client Managing Backups IBM Cloud DB Postgres Service available in multiple regions Client Managing Backups based on Client DR Implementation IBM Cloud Object Storage Service available in multiple regions Client About IBM Cloud Object Storage depends on Client DR Implementation IKS Service available in multiple regions Client High availability for IBM Cloud Kubernetes Service Planning your cluster for high availability based on Client DR Implementation Key Protect Regional: IBM is responsible for bringing the service back in a different Region IBM N/A the current documented RTO is <1 day Message Hub/Event streams Service available in multiple regions Client FAQs based on Client DR Implementation Push Notification Regional IBM TBD the current documented RTO is 1-3 days Virtual Private Cloud Service available in multiple regions Client Introduction based on Client DR Implementation Watson Natural Language Understanding (NLU) Client is responsible for restoring the service back in a different Region Client High availability and disaster recovery based on Client DR Implementation Other documentation: Build resilient applications on the cloud Hybrid integration for solutions that span environments Hybrid integration for solutions that span environments Strategies for resilient applications","title":"Overview"},{"location":"disaster-recovery/disaster-recovery/#disaster-recovery","text":"","title":"Disaster Recovery"},{"location":"disaster-recovery/disaster-recovery/#overview","text":"The approach to defining the disaster recover strategy needs to be systematic, and start with the \"application\", which is defined as a set of compute resources (IKS or OpenShift apps, VSIs, services, etc.) that make up a \"business application\". While a holistic approach may be desired, the reality is that each business application is independent, with its own RTO/RPO requirements, which for many customers is expressed in the form of a set of service classes or tiers. Resiliency Tier Recovery Description Recovery Time Objective Recovery Point Objective Tier 1 Continuous Availabilty <= 1 Hour <= 1 Hour Tier 2 Advanced Recovery > 1 Hrs - <= 24 Hrs <2 Hrs - <24 Hrs Tier 3 Standard Recovery > 24 Hrs - <= 72 Hrs Last Backup Tier 4 No Recovery N/A N/A Note: The IBM Cloud platform itself must support the LOWEST number in the range. Therefore, for the majority of their applications (Tier 1 and Tier 2), the IBM Cloud services (OpenShift, IKS, ICD, Cloudant, ICOS, Key Protect, Push Notifications, etc.) must have options/capabilities for support application recovery in 1 hour or less. For databases that require backup/restore, in order to qualify for Tier 2 the backups need to be every two hours? Sounds like the app owners can choose their RPO, so they could schedule their backups accordingly to fit both RTO and RPO they decide for themselves. Since each business application will have a unique set of compute, service and other resources, each one will need to be reviewed to make sure that the strategy and requirements for DR for that app are understood, documented and implemented (automation?) before going to production. To build the framework that will drive that analysis and work we will profile three business applications with comment sets of resources to create the scaffolding that application teams can use for their own applications. High Availability and Disaster Recovery When dealing with improved resilience it important to make some distinctions between High Availability (HA) and Disaster Recovery (DR). HA is mainly about keeping the service available to the end users when \"ordinary\" activities are performed on the system like deploying updates, rebooting the hosting Virtual Machines, applying security patches to the hosting OS, etc. For our purposes, High Availability within a single site can be achieved by eliminating single points of failure. The Blue Compute sample application in its current form implements high availability. HA usually doesn't deal with major unplanned (or planned) issues such as complete site loss due to major power outages, earthquakes, severe hardware failures, full-site connectivity loss, etc. In such cases, if the service must meet strict Service Level Objectives (SLO), you should make the whole application stack (infrastructure, services and application components) redundant by deploying it in at least two different Bluemix regions. This is typically defined as a DR Architecture. There are many options to implement DR solutions. For the sake of simplicity, we can group the different options in three major categories: Active/Passive - Active/Passive options are based on keeping the full application stack active in one location, while another application stack is deployed in a different location, but kept idle (or shut down). In the case of prolonged unavailability of the primary site, the application stack is activated in the backup site. Often that requires the restoring of backups taken in the primary site. This approach is not recommended when loosing data can be a problem (e.g. when the Recovery Point Objective (RPO) is less than a few hours ) or when the availability of the service is critical (e.g. when the Return to Operations (RTO) objective is less than a few hours). Active/Standby - In the Active/Standby case the full application stack is active in both primary and backup location, however users transactions are served only by the primary site. The backup site takes care of keeping a replica of the status of the main location though data replication (such as DB replication or disk replication). In case of prolonged unavailability of the primary site, all client transactions are routed to the backup site. This approach provides quite good RPO and RTO (generally measured in minutes), however it is significantly more expensive than the Active/Passive options because of the double deployment (e.g., resources are wasted because the Stand by assets can't be used to improve scalability and throughput). Active/Active - In the Active/Active case both locations are active and client transactions are distributed according to predefined policies (such as round-robin, geographical load balancing, etc. ) to both regions. In the case of failure of one site the other site must be able to serve all clients. It's possible to achieve both an RPO and RTO close to zero with this configuration. The drawback is that both regions must be sized to handle the full load, even if they are used at the half of their capabilities when both locations are available. In such cases the Bluemix Autoscaling service can help in keeping always resources allocated according to the needs (as happens with the BlueCompute sample application). *Source: Making Microservices Resilient","title":"Overview"},{"location":"disaster-recovery/disaster-recovery/#cloud-architecture","text":"","title":"Cloud Architecture"},{"location":"disaster-recovery/disaster-recovery/#network-architecture","text":"","title":"Network Architecture"},{"location":"disaster-recovery/disaster-recovery/#sample-application-architecture","text":"","title":"Sample Application Architecture"},{"location":"disaster-recovery/disaster-recovery/#application-profiles","text":"One approach is to build a set of architecture profiles that represent the majority of apps. These profiles would include options for various classes of service, compute requirements, and all the other stuff listed below.","title":"Application Profiles"},{"location":"disaster-recovery/disaster-recovery/#tier-1","text":"Tier 1 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured CFEE provisioned in the VPC Org, spaces, custom domains and other custom configurations needed All applications deployed with the same version as deployed in primary MZR Application: All applications deployed with the same version as deployed in primary MZR Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, etc.) Note: Databases that use backup/restore as their only cross region replication option will not support Tier 1 availability unless the backups occur hourly and a restore can be completed in less than an hour. Service credentials and service bindings","title":"Tier 1"},{"location":"disaster-recovery/disaster-recovery/#tier-1-compute","text":"In order to meet Tier 1 RTO/RPO these components will need to be preconfigured and at their production workload capacity in US East. The size of the CF Enterprise or IKS clusters in US East may only need to be large enough to support Tier 1 and 2 workloads. All applications can always be deployed there (as they are today in Dedicated) but for Tier 3 and 4 applications they could be stopped. The idea would be to allow for Tier 1 and 2 apps to immediately become available with enough capacity to run them should a failure occur, but to save money the cluster would only be scaled up to match the primary cluster (and support the Tier 3/4 workloads) when actually needed, as Tier 3/4 apps have longer RTO in which the scaling operation would complete. The same technique could be applied to updates to the CF Enterprise or IKS clusters. The first step would be to scale up the backup cluster to full capacity (matching the primary cluster) before starting the upgrade. Then do some sort of blue/green update where workload could be switched to the backup while the primary cluster is upgraded. Once the primary cluster is upgraded and verified, workload would be switched back, the backup cluster could be scaled back down and then upgraded. Gaps/Unknowns Where will traffic routing be managed/changed in order to redirect from primary to backup site? Where is it done today? For databases, do we have any issues or constraints with scaling in terms of timing? For example, for Cloudant Enterprise, will there always be enough capacity in the backup MZR such that all databases in the instance in the primary MZR are able to be replicated or restored? If not this could be an impact on RTO for apps that use Cloudant.","title":"Tier 1 Compute"},{"location":"disaster-recovery/disaster-recovery/#tier-1-application-cloudant","text":"In this scenario the data will be replicated automatically by Cloudant. In the event of a disaster the only change needed will be to reconfigure DNS to point to the cluster in US East. Benefits - Fastest recovery time, dependent only on the time it takes to verify that the data was replicated and make the DNS switch Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful.","title":"Tier 1 Application - Cloudant"},{"location":"disaster-recovery/disaster-recovery/#tier-1-application-postgresql","text":"In this scenario the data will be replicated automatically by IBM Cloud Databases to a read-only replica in the backup MZR. In the event of a disaster the application team will need to manually trigger a promotion of the read-only replica to become the leader. This action will take some time; as a read-only replica the service instance is NOT configured using an HA topology . When the promotion occurs several steps happen to elevate the instance to an HA configuration. The only other change needed will be to reconfigure DNS to point to the cluster in US East, once the database promotion is complete. Benefits - Faster recovery time, as the data is already replicated to the backup MZR. There is still some latency related to the time it takes to reconfigure the database to an HA configuration. Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful. - Manual intervention is required to trigger and monitor the promotion of the read-only replica to leader status. Configuring read-only replicas Basic Steps: create instance in primary MZR (i.e. Dallas) create read-only replica in DR MZR (i.e. Washington, DC) Read-only replica is a single zone instance If primary MZR is unavailable, promote read-only replica to leader. Now DR MZR becomes the leader updates the config in the DR MZR to be MZR resilient, meaning additional nodes are added takes a full backup of the database in the DR MZR DR MZR becomes the leader and ties to original leader in primary MZR are broken original instance in Primary MZR can be deleted. This deletes all backups in the primary MZR. if original instance is not deleted (i.e. backups are still available) a new instance can be created by restoring from a backup, should that become necessary. the backups taken in the original MZR are still accessible even if that MZR is completely unavailable; they are stored in cross-regional ICOS buckets. If a read-only replica is promoted to leader, the original leader is no longer viable. To move the data/workload back to the original MZR, create a read-replica in the original MZR and promote it to be the leader. This does create a new instance of the database, not a restore of the original instance.","title":"Tier 1 Application - PostGreSQL"},{"location":"disaster-recovery/disaster-recovery/#tier-1-application-cloud-object-storage","text":"In this scenario the data is always available in both MZRs via cross-regional buckets. Therefore, the only change needed is to update the DNS routing to point to the backup MZR.","title":"Tier 1 Application - Cloud Object Storage"},{"location":"disaster-recovery/disaster-recovery/#tier-2","text":"Tier 2 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: - VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured - CFEE provisioned in the VPC - Org, spaces, custom domains and other custom configurations needed - All applications deployed with the same version as deployed in primary MZR Application: - All applications deployed with the same version as deployed in primary MZR - Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, backup/restore, etc.) - Service credentials and service bindings","title":"Tier 2"},{"location":"disaster-recovery/disaster-recovery/#tier-2-application-mongodb","text":"In this scenario, the data is not replicated to the backup MZR. When a disaster is declared the application team will need to create a new database instance in the backup MZR by restoring the data from backup. Note: The backup from the database instance in the Primary MZR is available in the backup MZR even if the primary MZR is completely unavailable. Once the data is restored, the only other change needed is to change the DNS routing to point to the backup MZR. Backup and restore procedures ElasticSearch MongoDB Redis","title":"Tier 2 Application - MongoDB"},{"location":"disaster-recovery/disaster-recovery/#references","text":"IBM Kubernetes Service Multi-Region Architecture Strategies for Resilient Applications Secure web applications across multiple regions Resilient and secure multi-region Kubernetes clusters with IBM Cloud Internet Services","title":"References"},{"location":"disaster-recovery/disaster-recovery/#ibm-cloud-services-resilience","text":"This repository provides links to the HA / DR / Backup information for the IBM Cloud Services in Scope. Services DR Capabilities DR Ownership/Control Documentation RTO Console Core service available in multiple regions IBM N/A HA IAM Core Service available in multiple regions IBM N/A HA CFEE Service available in multiple regions Client Tgt. end of November based on Client DR Implementation Cloudant Service available in multiple regions Client Disaster recovery and backup based on Client DR Implementation IBM Cloud DB Elastic Search Service available in multiple regions Client Managing Backups based on on Client DR Implementation IBM Cloud DB Mongo DB Service available in multiple regions Client Managing Backups IBM Cloud DB Postgres Service available in multiple regions Client Managing Backups based on Client DR Implementation IBM Cloud Object Storage Service available in multiple regions Client About IBM Cloud Object Storage depends on Client DR Implementation IKS Service available in multiple regions Client High availability for IBM Cloud Kubernetes Service Planning your cluster for high availability based on Client DR Implementation Key Protect Regional: IBM is responsible for bringing the service back in a different Region IBM N/A the current documented RTO is <1 day Message Hub/Event streams Service available in multiple regions Client FAQs based on Client DR Implementation Push Notification Regional IBM TBD the current documented RTO is 1-3 days Virtual Private Cloud Service available in multiple regions Client Introduction based on Client DR Implementation Watson Natural Language Understanding (NLU) Client is responsible for restoring the service back in a different Region Client High availability and disaster recovery based on Client DR Implementation Other documentation: Build resilient applications on the cloud Hybrid integration for solutions that span environments Hybrid integration for solutions that span environments Strategies for resilient applications","title":"ibm-cloud-services-resilience"},{"location":"disaster-recovery/tier1/","text":"Tier 1 Application Overview This is what makes an application a tier 1 application Architecture Recovery Requirements","title":"Tier 1 Application"},{"location":"disaster-recovery/tier1/#tier-1-application","text":"","title":"Tier 1 Application"},{"location":"disaster-recovery/tier1/#overview","text":"This is what makes an application a tier 1 application","title":"Overview"},{"location":"disaster-recovery/tier1/#architecture","text":"","title":"Architecture"},{"location":"disaster-recovery/tier1/#recovery-requirements","text":"","title":"Recovery Requirements"},{"location":"disaster-recovery/tier2/","text":"Tier 2 Application Overview Architecture","title":"Tier 2 Application"},{"location":"disaster-recovery/tier2/#tier-2-application","text":"","title":"Tier 2 Application"},{"location":"disaster-recovery/tier2/#overview","text":"","title":"Overview"},{"location":"disaster-recovery/tier2/#architecture","text":"","title":"Architecture"},{"location":"ha-app/create-env/","text":"Overview The architecture that we want to create looks like this: Regardless of whether you choose to run your applications in IBM Cloud Kubernetes Service (IKS) or IBM Cloud Foundry Enterprise Environment (CFEE) the provisioning process will be substantially the same, at least as far as the architecture goes. For the purposes of this document we wil use CFEE as the compute platform. In addition to the clusters themselves, most enterprises will use additional tools for managing and monitoring the environment. We will add additional services from IBM Cloud into the mix by provisioning them and configuring your clusters to use them. Logging - Log Analysis with LogDNA Auditing - Activity Tracker with LogDNA Monitoring - SysDig Encryption - Key Protect Setting up the VPC You can use Schematics on IBM Cloud to set up your VPCs. Schematics uses terraform to create and manage the infrastructure. Directions for using Schematics can be found here , and you can use this repo for your schematics workspaces for your VPCs. https://github.com/dwakeman/private-vpc-terraform You will need to create a separate workspace for each VPC, one in US South and one in US East. The terraform project will create your VPC in the region you specify, and it will create three subnets for you, in different availability zones. Provision the clusters Provision cluster in US South","title":"Creating the Environment"},{"location":"ha-app/create-env/#overview","text":"The architecture that we want to create looks like this: Regardless of whether you choose to run your applications in IBM Cloud Kubernetes Service (IKS) or IBM Cloud Foundry Enterprise Environment (CFEE) the provisioning process will be substantially the same, at least as far as the architecture goes. For the purposes of this document we wil use CFEE as the compute platform. In addition to the clusters themselves, most enterprises will use additional tools for managing and monitoring the environment. We will add additional services from IBM Cloud into the mix by provisioning them and configuring your clusters to use them. Logging - Log Analysis with LogDNA Auditing - Activity Tracker with LogDNA Monitoring - SysDig Encryption - Key Protect","title":"Overview"},{"location":"ha-app/create-env/#setting-up-the-vpc","text":"You can use Schematics on IBM Cloud to set up your VPCs. Schematics uses terraform to create and manage the infrastructure. Directions for using Schematics can be found here , and you can use this repo for your schematics workspaces for your VPCs. https://github.com/dwakeman/private-vpc-terraform You will need to create a separate workspace for each VPC, one in US South and one in US East. The terraform project will create your VPC in the region you specify, and it will create three subnets for you, in different availability zones.","title":"Setting up the VPC"},{"location":"ha-app/create-env/#provision-the-clusters","text":"","title":"Provision the clusters"},{"location":"ha-app/create-env/#provision-cluster-in-us-south","text":"","title":"Provision cluster in US South"},{"location":"ha-app/deploy-ha-app/","text":"Prerequisites This document is based on a topology that includes IKS on VPC in US South and US East. VPC in US South VPC in US East IKS cluster on VPC in US South IKS cluster on VPC in US East Custom domain Cloud Internet Services instance provisioned and configured to manage custom domain Setup Need to follow these steps to deploy the app, setup TLS termination and expose the app to the public.","title":"HA app deployment"},{"location":"ha-app/deploy-ha-app/#prerequisites","text":"This document is based on a topology that includes IKS on VPC in US South and US East. VPC in US South VPC in US East IKS cluster on VPC in US South IKS cluster on VPC in US East Custom domain Cloud Internet Services instance provisioned and configured to manage custom domain","title":"Prerequisites"},{"location":"ha-app/deploy-ha-app/#setup","text":"Need to follow these steps to deploy the app, setup TLS termination and expose the app to the public.","title":"Setup"},{"location":"ha-app/ha-app/","text":"Overview One of the great things about hosting your application in the cloud is the variety of options to consider when planning for a high availability application deployment pattern. Most cloud providers offer multiple regions around the world with some sort of multi-zone architecture to facilitate such deployments. What to consider Compute - Your app will need a place to run. There are many choices for compute, including bare metal and virtual servers, platforms like Kubernetes or Cloud Foundry, and serverless platforms. Each option will have its own set of characterstics, tools and features to consider; this document will focus on two of the more popular ones, cloud foundry and kubernetes. Data - If your apps will include data, as most do, you need to consider which database technology to use, and how to configure it for use in a multi-region deployment. Most cloud provider-managed databases are \"regional\" in nature, meaning they provide high availability within a region, but not across regions. It is up to you to configure them such that the data is properly replicated, backed up/restored, etc., as appropriate for your application. DevOps - How are you going to deploy your application? If your application is to highly available it has to stay up during deployments. There are many patterns for doing this, some built into the compute platforms themselves. Kubernetes, for example, does rolling deployments automatically so that your application is always available, asusming your application is configured correctly. Blue/green deployments are another common pattern that are used with many compute platforms. Routing - How are you going to route the traffic to your application? You want your application to be served up by a single URL, like myapp.eyebeemdemos.com , but you need the capability to route traffic to multiple instances of your application. If you're using kubernetes or cloud foundry they will automatically provide application load balancer capabilities to route traffic to multiple instances running in the same region (i.e kubernetes cluster). But what about a cross-region scenario? There are multiple ways to accomplish this; this document will leverage IBM Cloud Internet Services by creating a global load balancer that will perform health checking and routing. Architecture At a high level the architecture looks like this: In the diagram above you can see the that application is deployed into two regions, US South and US East . In each region the compute platform - in thise case IBM Cloud Foundry Enterprise, which runs on IBM Kubernetes Service. Within each region, the application is part of a subdomain provided by Cloud Foundry Enterprise: <cluster-name>.<region>.containers.appdomain.cloud . These subdomains can be used as sources in a global load balancer called myapp.eyebeemdemos.com in IBM Cloud Internet Services, which will route traffic across both regions. There is additional setup work that needs to be done for you to be able to use your own domain. More information can be found here . DevOps It is common for enterprises to use multiple non-production environments as they develop and test their applications. The number of these environment varies, but in general there are at least three: Dev - this is where developers do their initial testing in a server-based environment Test - Consolidated testing of multiple components is done here by the development team before handing the app over to users for testing UAT - This is where user acceptance testing is performed In addition to these non-production environments the application will have at least one production environment where the application will be used. It may also be important to have multiple production environments in different regions around the world. Typically there will also be a backup or disaster recovery environment. For this document, the non-production environments above will be implemented as different namespaces within the same IKS cluster, and the production environment will be a namespace in the production IKS clusters in two different regions. Another common practice is to use different subdomains for each environment. As the application moves through the environments on the way to production, its URL will change: Dev - myapp.dev.eyebeemdemos.com Test - myapp.test.eyebeemdemos.com UAT - myapp.uat.eyebeemdemos.com To enable this capability you will need to perform a few steps: create some DNS CNAME record to point these URLs to the ingress subdomain for your non-production IKS cluster make sure you have certificates for these wildcard domains (i.e. *.dev.eyebeemdemos.com ) create a kubernetes secret with the certificate and private key create an ingress controller to configure TLS termination and map a route to your app Documentation for these steps can be found in the IBM Kubernetes Service documentation here .","title":"HA app considerations"},{"location":"ha-app/ha-app/#overview","text":"One of the great things about hosting your application in the cloud is the variety of options to consider when planning for a high availability application deployment pattern. Most cloud providers offer multiple regions around the world with some sort of multi-zone architecture to facilitate such deployments.","title":"Overview"},{"location":"ha-app/ha-app/#what-to-consider","text":"Compute - Your app will need a place to run. There are many choices for compute, including bare metal and virtual servers, platforms like Kubernetes or Cloud Foundry, and serverless platforms. Each option will have its own set of characterstics, tools and features to consider; this document will focus on two of the more popular ones, cloud foundry and kubernetes. Data - If your apps will include data, as most do, you need to consider which database technology to use, and how to configure it for use in a multi-region deployment. Most cloud provider-managed databases are \"regional\" in nature, meaning they provide high availability within a region, but not across regions. It is up to you to configure them such that the data is properly replicated, backed up/restored, etc., as appropriate for your application. DevOps - How are you going to deploy your application? If your application is to highly available it has to stay up during deployments. There are many patterns for doing this, some built into the compute platforms themselves. Kubernetes, for example, does rolling deployments automatically so that your application is always available, asusming your application is configured correctly. Blue/green deployments are another common pattern that are used with many compute platforms. Routing - How are you going to route the traffic to your application? You want your application to be served up by a single URL, like myapp.eyebeemdemos.com , but you need the capability to route traffic to multiple instances of your application. If you're using kubernetes or cloud foundry they will automatically provide application load balancer capabilities to route traffic to multiple instances running in the same region (i.e kubernetes cluster). But what about a cross-region scenario? There are multiple ways to accomplish this; this document will leverage IBM Cloud Internet Services by creating a global load balancer that will perform health checking and routing.","title":"What to consider"},{"location":"ha-app/ha-app/#architecture","text":"At a high level the architecture looks like this: In the diagram above you can see the that application is deployed into two regions, US South and US East . In each region the compute platform - in thise case IBM Cloud Foundry Enterprise, which runs on IBM Kubernetes Service. Within each region, the application is part of a subdomain provided by Cloud Foundry Enterprise: <cluster-name>.<region>.containers.appdomain.cloud . These subdomains can be used as sources in a global load balancer called myapp.eyebeemdemos.com in IBM Cloud Internet Services, which will route traffic across both regions. There is additional setup work that needs to be done for you to be able to use your own domain. More information can be found here .","title":"Architecture"},{"location":"ha-app/ha-app/#devops","text":"It is common for enterprises to use multiple non-production environments as they develop and test their applications. The number of these environment varies, but in general there are at least three: Dev - this is where developers do their initial testing in a server-based environment Test - Consolidated testing of multiple components is done here by the development team before handing the app over to users for testing UAT - This is where user acceptance testing is performed In addition to these non-production environments the application will have at least one production environment where the application will be used. It may also be important to have multiple production environments in different regions around the world. Typically there will also be a backup or disaster recovery environment. For this document, the non-production environments above will be implemented as different namespaces within the same IKS cluster, and the production environment will be a namespace in the production IKS clusters in two different regions. Another common practice is to use different subdomains for each environment. As the application moves through the environments on the way to production, its URL will change: Dev - myapp.dev.eyebeemdemos.com Test - myapp.test.eyebeemdemos.com UAT - myapp.uat.eyebeemdemos.com To enable this capability you will need to perform a few steps: create some DNS CNAME record to point these URLs to the ingress subdomain for your non-production IKS cluster make sure you have certificates for these wildcard domains (i.e. *.dev.eyebeemdemos.com ) create a kubernetes secret with the certificate and private key create an ingress controller to configure TLS termination and map a route to your app Documentation for these steps can be found in the IBM Kubernetes Service documentation here .","title":"DevOps"},{"location":"hybrid-cloud/hybrid/","text":"Overview Application Architecture Authentication","title":"Deploying a Hybrid Cloud Application"},{"location":"hybrid-cloud/hybrid/#overview","text":"","title":"Overview"},{"location":"hybrid-cloud/hybrid/#application-architecture","text":"","title":"Application Architecture"},{"location":"hybrid-cloud/hybrid/#authentication","text":"","title":"Authentication"},{"location":"hybrid-cloud/hybrid/#_1","text":"","title":""},{"location":"infrastructure/iac/","text":"Infrastructure as Code Overview What is IaC?","title":"Infrastructure as Code (IaC)"},{"location":"infrastructure/iac/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"infrastructure/iac/#overview","text":"What is IaC?","title":"Overview"},{"location":"infrastructure/schematics/","text":"Schematics","title":"Schematics"},{"location":"infrastructure/schematics/#schematics","text":"","title":"Schematics"},{"location":"infrastructure/sds/","text":"Using Software Defined Storage There are many types of Software Defined Storage (SDS) implementations in the market, and they may apply with different types of compute technology. The purpose of this section is to document how to install and configure Portworx Enterprise with OpenShift on IBM Cloud . Portworx acts as a storage layer in kubernetes that sits on top of some underlying physical storage attached to the worker nodes in the cluster. It also provides additional storage classes for creating persistent volumes in Portworx. Additional information on using Portworx with OpenShift on IBM Cloud can be found in the documentation . Note: These instructions were tested with OpenShift 4.5.13 running in Virtual Private Cloud and assume that you already have an OpenShift cluster provisioned. Preparing for Portworx Review the documentation for Planning your Portworx setup to verify that your existing cluster meets the requirements. If not, create a new cluster. Note: It would appear that the provisioning page for Portworx in the IBM Cloud Catalog does some sort of requirements checking after you supply an API key. It did not recognize my cluster when it had bx2.8x32 nodes, but id did when I added bx2.16x64 nodes. There are two common topologies for adding SDS to your cluster. The first one is called hyper-converged , where every worker node in the cluster has attached storage for Portworx. This is the topology with the best performance, but it requires you to have storage attached to every worker node. The other topology is called storage-heavy ; in this topology only some of the worker nodes have attached storage. Portworx will still make the storage available to pods running on any worker node in the cluster, but for pods running on non-SDS nodes the storage access requests will be routed on the private network to one of the SDS nodes. Note: As far as I can tell the installation and configuration of Portworx is the same either way, but that may not be true. For purposes of this exercise I have created an OpenShift cluster with 3 worker nodes in US South , with one worker node in each zone. Creating block storage Your worker nodes will need to have block storage volumes attached to them in order for Portworx to work. See the documentation for more details. Since these instuctions are for a cluster in VPC, follow these steps to create and attach the block storage volumes to your worker nodes. Documentation can also be found here . You can create the block storage volumes using the IBM Cloud UI . Make sure to create the block storage volume in the same Availability zone as the worker node to which you want to attach it. For visibility purposes, you should consider a naming convention that will help you determine where the storage volumes are being used. In this example the name of the storage volume includes the worker ID of the worker node to which it will be attached. If you don't want to look up the worker nodes in the UI you can run this command to get a list of your worker nodes: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift Note: As you create each storage volume, take note of it's volume ID . You will need it in the next step. Attaching block storage to worker nodes In order for Portworx to work it needs the storage volumes created above to be attached to the worker nodes. The documentation was used to derive these instructions. Make sure you are logged into the IBM Cloud CLI. Run this command to fetch your oauth token and store it in an environment variable: export iamtoken=$(ibmcloud iam oauth-tokens) <--- This does not work right!! Need to figure out the right command. In this example the cluster was created in US South , with worker nodes in us-south- , us-south-2 and us-south-3 . Block storage volumes of equal size were also created in those three zones. Since worker nodes are not visible in the IBM Cloud portal we cannot attach the storage using normal methods in the UI or CLI. In order to attach the volumes to the worker nodes we have to use the IKS API. The format of the API call looks like this: curl -X POST -H \"Authorization: $iamtoken\" \"https://<region>.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=<cluster_ID>&worker=<worker_ID>&volumeID=<volume_ID>\" Use this command to get your worker IDs: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift The volume IDs should have been collected above when you created the volumes. If not, you can get them from the UI. Note: Make sure to update this section with a CLI command to get the volumes! Run the cURL command above for each worker node and volume. For example: curl -X POST -H \"Authorization: $iamtoken\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386&volumeID=r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\" The output should look like this: {\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"\"},\"name\":\"volume-attachment\",\"status\":\"attaching\",\"type\":\"data\"} To review the volume attachments for a worker node you can run this command: curl -X GET -H \"Authorization: <IAM_token>\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: <resource_group_ID>\" \"https://<region>.containers.cloud.ibm.com/v2/storage/clusters/<cluster_ID>/workers/<worker_ID>/volume_attachments\" For example: curl -X GET -H \"Authorization: $iamtoken\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: 5e6a2492dd9249659469ea51da471197\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/getAttachmentsList?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\" The output should look like this: {\"volume_attachments\":[{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e-9nh8x\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"data\"},{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31\",\"volume\":{\"name\":\"hypnotist-freezing-willow-display\",\"id\":\"r006-a354ecea-4cc3-4445-9431-12cd9ce569a8\"},\"device\":{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31-fzd6s\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"boot\"}]} Provision an instance of Databases for Etcd in the same resource group as the OpenShift cluster. For visibility purposes, consider a naming convention that includes the cluster name, such as px-sbx-ocp-11 , and add a tag with the name of the cluster as well. Review the documentation for more infomration on configuration options. Follow the steps in the documentation. When it comes to encoding the username and password, I found that the instructions did not work, as they added the -n from the command into the output. I used this command: echo \"<the username or password>\" | base64 At this point the Databases for Etcd instance is created, the username and password have been encoded and the secret has been created. Now we need to copy the image pull secrets and add them to the service account . Copy the image pull secrets To list the secrets in the default namespace: oc get secrets -n default | grep icr-io The output should look like this: all-icr-io kubernetes.io/dockerconfigjson 1 115m To copy the all-icr-io secret to the kube-system namespace: oc get secret all-icr-io -n default -o yaml | sed 's/default/kube-system/g' | oc create -n kube-system -f - The output should look like this: secret/all-icr-io created Store the pull secret in the service account Check if the image pull secret already exists for the default service account: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that the Image pull secrets sction only lists default-dockercfg-bkxjw . This means that the image pull secrets are already defined, but do not yet contain all-icr-io . Run this command to add it: oc patch -n kube-system serviceaccount/default --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\":\"all-icr-io\"}}]' The output should look like this: serviceaccount/default patched You can verify that it worked using this command: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw all-icr-io Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that now all-icr-io shows up in the Image pull secrets section.","title":"Software Defined Storage"},{"location":"infrastructure/sds/#using-software-defined-storage","text":"There are many types of Software Defined Storage (SDS) implementations in the market, and they may apply with different types of compute technology. The purpose of this section is to document how to install and configure Portworx Enterprise with OpenShift on IBM Cloud . Portworx acts as a storage layer in kubernetes that sits on top of some underlying physical storage attached to the worker nodes in the cluster. It also provides additional storage classes for creating persistent volumes in Portworx. Additional information on using Portworx with OpenShift on IBM Cloud can be found in the documentation . Note: These instructions were tested with OpenShift 4.5.13 running in Virtual Private Cloud and assume that you already have an OpenShift cluster provisioned.","title":"Using Software Defined Storage"},{"location":"infrastructure/sds/#preparing-for-portworx","text":"Review the documentation for Planning your Portworx setup to verify that your existing cluster meets the requirements. If not, create a new cluster. Note: It would appear that the provisioning page for Portworx in the IBM Cloud Catalog does some sort of requirements checking after you supply an API key. It did not recognize my cluster when it had bx2.8x32 nodes, but id did when I added bx2.16x64 nodes. There are two common topologies for adding SDS to your cluster. The first one is called hyper-converged , where every worker node in the cluster has attached storage for Portworx. This is the topology with the best performance, but it requires you to have storage attached to every worker node. The other topology is called storage-heavy ; in this topology only some of the worker nodes have attached storage. Portworx will still make the storage available to pods running on any worker node in the cluster, but for pods running on non-SDS nodes the storage access requests will be routed on the private network to one of the SDS nodes. Note: As far as I can tell the installation and configuration of Portworx is the same either way, but that may not be true. For purposes of this exercise I have created an OpenShift cluster with 3 worker nodes in US South , with one worker node in each zone.","title":"Preparing for Portworx"},{"location":"infrastructure/sds/#creating-block-storage","text":"Your worker nodes will need to have block storage volumes attached to them in order for Portworx to work. See the documentation for more details. Since these instuctions are for a cluster in VPC, follow these steps to create and attach the block storage volumes to your worker nodes. Documentation can also be found here . You can create the block storage volumes using the IBM Cloud UI . Make sure to create the block storage volume in the same Availability zone as the worker node to which you want to attach it. For visibility purposes, you should consider a naming convention that will help you determine where the storage volumes are being used. In this example the name of the storage volume includes the worker ID of the worker node to which it will be attached. If you don't want to look up the worker nodes in the UI you can run this command to get a list of your worker nodes: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift Note: As you create each storage volume, take note of it's volume ID . You will need it in the next step.","title":"Creating block storage"},{"location":"infrastructure/sds/#attaching-block-storage-to-worker-nodes","text":"In order for Portworx to work it needs the storage volumes created above to be attached to the worker nodes. The documentation was used to derive these instructions. Make sure you are logged into the IBM Cloud CLI. Run this command to fetch your oauth token and store it in an environment variable: export iamtoken=$(ibmcloud iam oauth-tokens) <--- This does not work right!! Need to figure out the right command. In this example the cluster was created in US South , with worker nodes in us-south- , us-south-2 and us-south-3 . Block storage volumes of equal size were also created in those three zones. Since worker nodes are not visible in the IBM Cloud portal we cannot attach the storage using normal methods in the UI or CLI. In order to attach the volumes to the worker nodes we have to use the IKS API. The format of the API call looks like this: curl -X POST -H \"Authorization: $iamtoken\" \"https://<region>.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=<cluster_ID>&worker=<worker_ID>&volumeID=<volume_ID>\" Use this command to get your worker IDs: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift The volume IDs should have been collected above when you created the volumes. If not, you can get them from the UI. Note: Make sure to update this section with a CLI command to get the volumes! Run the cURL command above for each worker node and volume. For example: curl -X POST -H \"Authorization: $iamtoken\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386&volumeID=r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\" The output should look like this: {\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"\"},\"name\":\"volume-attachment\",\"status\":\"attaching\",\"type\":\"data\"} To review the volume attachments for a worker node you can run this command: curl -X GET -H \"Authorization: <IAM_token>\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: <resource_group_ID>\" \"https://<region>.containers.cloud.ibm.com/v2/storage/clusters/<cluster_ID>/workers/<worker_ID>/volume_attachments\" For example: curl -X GET -H \"Authorization: $iamtoken\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: 5e6a2492dd9249659469ea51da471197\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/getAttachmentsList?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\" The output should look like this: {\"volume_attachments\":[{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e-9nh8x\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"data\"},{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31\",\"volume\":{\"name\":\"hypnotist-freezing-willow-display\",\"id\":\"r006-a354ecea-4cc3-4445-9431-12cd9ce569a8\"},\"device\":{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31-fzd6s\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"boot\"}]} Provision an instance of Databases for Etcd in the same resource group as the OpenShift cluster. For visibility purposes, consider a naming convention that includes the cluster name, such as px-sbx-ocp-11 , and add a tag with the name of the cluster as well. Review the documentation for more infomration on configuration options. Follow the steps in the documentation. When it comes to encoding the username and password, I found that the instructions did not work, as they added the -n from the command into the output. I used this command: echo \"<the username or password>\" | base64 At this point the Databases for Etcd instance is created, the username and password have been encoded and the secret has been created. Now we need to copy the image pull secrets and add them to the service account .","title":"Attaching block storage to worker nodes"},{"location":"infrastructure/sds/#copy-the-image-pull-secrets","text":"To list the secrets in the default namespace: oc get secrets -n default | grep icr-io The output should look like this: all-icr-io kubernetes.io/dockerconfigjson 1 115m To copy the all-icr-io secret to the kube-system namespace: oc get secret all-icr-io -n default -o yaml | sed 's/default/kube-system/g' | oc create -n kube-system -f - The output should look like this: secret/all-icr-io created","title":"Copy the image pull secrets"},{"location":"infrastructure/sds/#store-the-pull-secret-in-the-service-account","text":"Check if the image pull secret already exists for the default service account: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that the Image pull secrets sction only lists default-dockercfg-bkxjw . This means that the image pull secrets are already defined, but do not yet contain all-icr-io . Run this command to add it: oc patch -n kube-system serviceaccount/default --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\":\"all-icr-io\"}}]' The output should look like this: serviceaccount/default patched You can verify that it worked using this command: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw all-icr-io Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that now all-icr-io shows up in the Image pull secrets section.","title":"Store the pull secret in the service account"},{"location":"infrastructure/vpc/","text":"Creating a VPC","title":"Virtual Private Cloud"},{"location":"infrastructure/vpc/#creating-a-vpc","text":"","title":"Creating a VPC"},{"location":"satellite/","text":"Overview This site provides documentation and samples for working with IBM Cloud Satellite. It includes step-by-step instructions for creating locations and sample terraform for creating the infrastructure needed for your hosts.","title":"Overview"},{"location":"satellite/#overview","text":"This site provides documentation and samples for working with IBM Cloud Satellite. It includes step-by-step instructions for creating locations and sample terraform for creating the infrastructure needed for your hosts.","title":"Overview"},{"location":"satellite/aws/assign-hosts/","text":"Assign Hosts At this point you have created an IBM Satellite location and created some hosts in AWS that are attached to your location. In order to finish setting up your location we need to assign some hosts to the control plane. You will need at least three hosts, managed from three different zones in IBM Cloud. When you created the hosts, you provisioned them in different availability zones in AWS; when you assign each host to the control plane you will select a different IBM Cloud availiability zone to manage each of these hosts. Let's get started! To Assign hosts to your control plane: Access your IBM Satellite location in IBM Cloud by using the icon and choose Satellite . On the Satellite landing page, click the Locations item on the left navigation menu. Click on your location in the list. It should automatically take you to the Hosts page where you will see your EC2 instances. If you don't end up on the Hosts page click on the Hosts link in the left navigation menu. Note When you assign a host to the control plane you will want to have each host managed by a different availability zone in IBM Cloud. However, the IBM Cloud console does not know anything about your EC2 instances. You will have to manually decide which of your hosts should be managed by which zone in IBM Cloud. The name of your hosts in the IBM console will probably be something like ip-<an-ip-address> . That IP address is the private IP address of the host. In AWS the guest OS in your host does not know anything about public IP addresses; that's just how AWS networking works. If you want you can match up the host with the right EC2 instance by comparing the name of the host in IBM Cloud with the private IP address of your EC2 instance. Click on the three dots at the far right of the first line in the table of hosts. In the menu that pops up select Assign host . In the dialog that pops up there are two dropdown menus. The first one is for the cluster to which the host will be assigned. Right now only the Control plane is avaiable; later, when you provision other IBM Services will show up on this list. In the Zone field, there is a list of all of the availability zones in the IBM Cloud MZR. Choose any of the three zones and click Assign host . You wil see in the table of hosts that the status changes to Assigned and the Health changes to Provisioning . Repeat the steps above for the other two hosts. Be sure to pick different zones for each one, and different than the first host. Now we wait for the provisioning to finish. Click on the Overview link in the left navigation menu to go back to the overview page for your location. The Deployment message should now be R0023: Wait while Satellite sets up the location control plane. That message will change when the provisioning is complete. Tip IBM Satellite, like all services in IBM Cloud, does a certain amount of logging at the platform level. Now would be a create time to create an instance of IBM Log Analysis with LogDNA in the IBM Cloud region where your location is defined and enable it for Platform Logging in that region. You will be able to see logs from IBM Satellite as the provisioning progresses. Note At this time the message when provisioning is done will probably be an error having to do with DNS or routing or something. This is expected, according to the documentation ; when the hosts were attached to the location the value for the public IP address was actually set to the private IP address. This is due to the way AWS does networking. We will need to run a command using the IBM Cloud CLI and the satellite plugin to register the public IP addresses for your control plane hosts with the subdomain that IBM created for your control plane. When the provisioning is complete the deployment message changes to: R0036: The location subdomains are not correctly routing traffic to your control plane hosts. Verify that the location subdomains are registered with the correct IP addresses for your control plane hosts with the 'ibmcloud sat location dns' commands. For more information, see 'http://ibm.biz/satloc-ts-subdomain'. Now it is time to update the DNS entry for your location with the public IP addresses of your control plane hosts. Open a terminal on your computer and login to the IBM Cloud CLI. If you don't have the CLI installed you can find instructions here . Run this command to list your satellite locations: ibmcloud sat location ls The output will look like this: Retrieving locations... OK Name ID Status Ready Created Hosts (used/total) Managed From Fairfax buukhi9w02ja0nlgvfg0 action required no 1 day ago 3 / 3 Washington D.C. Toronto butv8rdw0lre015nkteg normal yes 2 days ago 3 / 6 Washington D.C. To see the existing IP addresses registered for your location run this command: ibmcloud sat location dns ls --location <location id or name> For example: ibmcloud sat location dns ls --location Fairfax Retrieving location subdomains... OK Hostname Records Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c000.us-east.satellite.appdomain.cloud 10.5.0.5,10.5.0.22,10.5.0.40 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c000 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c001.us-east.satellite.appdomain.cloud 10.5.0.5 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c001 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c002.us-east.satellite.appdomain.cloud 10.5.0.40 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c002 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c003.us-east.satellite.appdomain.cloud 10.5.0.22 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c003 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-ce00.us-east.satellite.appdomain.cloud f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c000.us-east.satellite.appdomain.cloud None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-ce00 default You can see that the IP addresses are all 10.5.x.x , which are the private IP addresses in my VPC. The command to register new IP addresses in DNS will look like this: ibmcloud sat location dns register --location <location ID or Name> --ip <aws_host_public_ip> --ip <aws_host_public_ip> --ip <aws_host_public_ip> Each of the <aws_host_public_ip> placeholders will contain the public IP address of one of your control plane hosts. You can get these IP addresses from the Instances console of the EC2 Dashboard. Once you have updated the location DNS with the public IP addresses it's time to wait for the DNS updates to be propogated and for the location to recover from the error. If all goes well, in a few minutes the deployment message will change to: R0001: the Satellite location is ready for operations. Congratulations! Your location is up and running normally. Now it is time to add some more hosts to your location and provision other services, like OpenShift on IBM Cloud.","title":"Assign Hosts to Location"},{"location":"satellite/aws/assign-hosts/#assign-hosts","text":"At this point you have created an IBM Satellite location and created some hosts in AWS that are attached to your location. In order to finish setting up your location we need to assign some hosts to the control plane. You will need at least three hosts, managed from three different zones in IBM Cloud. When you created the hosts, you provisioned them in different availability zones in AWS; when you assign each host to the control plane you will select a different IBM Cloud availiability zone to manage each of these hosts. Let's get started! To Assign hosts to your control plane: Access your IBM Satellite location in IBM Cloud by using the icon and choose Satellite . On the Satellite landing page, click the Locations item on the left navigation menu. Click on your location in the list. It should automatically take you to the Hosts page where you will see your EC2 instances. If you don't end up on the Hosts page click on the Hosts link in the left navigation menu. Note When you assign a host to the control plane you will want to have each host managed by a different availability zone in IBM Cloud. However, the IBM Cloud console does not know anything about your EC2 instances. You will have to manually decide which of your hosts should be managed by which zone in IBM Cloud. The name of your hosts in the IBM console will probably be something like ip-<an-ip-address> . That IP address is the private IP address of the host. In AWS the guest OS in your host does not know anything about public IP addresses; that's just how AWS networking works. If you want you can match up the host with the right EC2 instance by comparing the name of the host in IBM Cloud with the private IP address of your EC2 instance. Click on the three dots at the far right of the first line in the table of hosts. In the menu that pops up select Assign host . In the dialog that pops up there are two dropdown menus. The first one is for the cluster to which the host will be assigned. Right now only the Control plane is avaiable; later, when you provision other IBM Services will show up on this list. In the Zone field, there is a list of all of the availability zones in the IBM Cloud MZR. Choose any of the three zones and click Assign host . You wil see in the table of hosts that the status changes to Assigned and the Health changes to Provisioning . Repeat the steps above for the other two hosts. Be sure to pick different zones for each one, and different than the first host. Now we wait for the provisioning to finish. Click on the Overview link in the left navigation menu to go back to the overview page for your location. The Deployment message should now be R0023: Wait while Satellite sets up the location control plane. That message will change when the provisioning is complete. Tip IBM Satellite, like all services in IBM Cloud, does a certain amount of logging at the platform level. Now would be a create time to create an instance of IBM Log Analysis with LogDNA in the IBM Cloud region where your location is defined and enable it for Platform Logging in that region. You will be able to see logs from IBM Satellite as the provisioning progresses. Note At this time the message when provisioning is done will probably be an error having to do with DNS or routing or something. This is expected, according to the documentation ; when the hosts were attached to the location the value for the public IP address was actually set to the private IP address. This is due to the way AWS does networking. We will need to run a command using the IBM Cloud CLI and the satellite plugin to register the public IP addresses for your control plane hosts with the subdomain that IBM created for your control plane. When the provisioning is complete the deployment message changes to: R0036: The location subdomains are not correctly routing traffic to your control plane hosts. Verify that the location subdomains are registered with the correct IP addresses for your control plane hosts with the 'ibmcloud sat location dns' commands. For more information, see 'http://ibm.biz/satloc-ts-subdomain'. Now it is time to update the DNS entry for your location with the public IP addresses of your control plane hosts. Open a terminal on your computer and login to the IBM Cloud CLI. If you don't have the CLI installed you can find instructions here . Run this command to list your satellite locations: ibmcloud sat location ls The output will look like this: Retrieving locations... OK Name ID Status Ready Created Hosts (used/total) Managed From Fairfax buukhi9w02ja0nlgvfg0 action required no 1 day ago 3 / 3 Washington D.C. Toronto butv8rdw0lre015nkteg normal yes 2 days ago 3 / 6 Washington D.C. To see the existing IP addresses registered for your location run this command: ibmcloud sat location dns ls --location <location id or name> For example: ibmcloud sat location dns ls --location Fairfax Retrieving location subdomains... OK Hostname Records Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c000.us-east.satellite.appdomain.cloud 10.5.0.5,10.5.0.22,10.5.0.40 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c000 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c001.us-east.satellite.appdomain.cloud 10.5.0.5 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c001 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c002.us-east.satellite.appdomain.cloud 10.5.0.40 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c002 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c003.us-east.satellite.appdomain.cloud 10.5.0.22 None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c003 default f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-ce00.us-east.satellite.appdomain.cloud f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-c000.us-east.satellite.appdomain.cloud None creating f3dc29213e9fd2bee18b6-6b64a6ccc9c596bf59a86625d8fa2202-ce00 default You can see that the IP addresses are all 10.5.x.x , which are the private IP addresses in my VPC. The command to register new IP addresses in DNS will look like this: ibmcloud sat location dns register --location <location ID or Name> --ip <aws_host_public_ip> --ip <aws_host_public_ip> --ip <aws_host_public_ip> Each of the <aws_host_public_ip> placeholders will contain the public IP address of one of your control plane hosts. You can get these IP addresses from the Instances console of the EC2 Dashboard. Once you have updated the location DNS with the public IP addresses it's time to wait for the DNS updates to be propogated and for the location to recover from the error. If all goes well, in a few minutes the deployment message will change to: R0001: the Satellite location is ready for operations. Congratulations! Your location is up and running normally. Now it is time to add some more hosts to your location and provision other services, like OpenShift on IBM Cloud.","title":"Assign Hosts"},{"location":"satellite/aws/create-hosts/","text":"Create Hosts We will use the script generated you created the location as the User Data to be passed into each EC2 instance created. Don't forget to update it with the right update commands after the API_URL variable, as documented. Navigate to the EC2 console and let's get started! Create SSH Key To create an SSH Key: Click on the Key pairs link in the EC2 console. You will probably see that you have no key pairs in the list. Note AWS will allow you to generate a new key pair, in which it will generate and download a key file (a file with a .pem extension) that you can later specify as the private key when you use the ssh command. It will save the public key in the key pair object in AWS and add that public key to your EC2 instance when you create it (assuming you choose this key). The other option is to import a key. In this case you will import the contents of your public key and AWS stores it in the key pair. You can use this option if you already have an SSH key and don't want to create a new one. If you choose to import an existing key, click the Actions dropdown at the top of the list and select import key pair . Give your key a name and copy/paste the contents of your key into the text box in the Key pair file section. You could also use the Browse button to choose the file. Click Import key pair . If you choose to create a new key pair click the Create key pair button above the list of keys on the Key pairs console. Give your key a name, leave the file format set to pem and click Create key pair . Warning Be aware that when you clicked the Create key pair button, the browser automatically downloaded a file. This is very important!! This is your private key, and you will need it later if you use the ssh command to access your EC2 instance. DO NOT LOSE THIS FILE! You will never get a chance to download it again and without it you will not be able to access your EC2 instance. Create EC2 instances Now we are ready to create some EC2 instances to use for the control plane for you IBM Cloud Satellite location. These hosts have some minimum requirements, which are documented here . For the most part, the key requirements are: at least 4 vCPUs at least 16GB Memory at least 100GB attached storage To create an EC2 instance: Click on the Instances link in the left navigation menu of the EC2 Dashboard. Click on the Launch instances dropdown and select Launch instances . The first step is to select an Amazon Machine Image (AMI) to use for the EC2 instance. Search for RHEL-7.9 in the search window. We want to use a community image, so click the link for results in Community AMIs. There should be an image on the list with a name like RHEL-7.9_HVM_GA-20200917-x86_64-0-Hourly2-GP2 . Click the Select button on the line containing that image. Next you need to choose the instance type. This is the hardware configuration for the instance. Click the checkbox next to t3.xlarge . This type has 4 vCPUs and 16GB of RAM. Click Next: Configure Instance Details . In the Configure Instance Details page there are several things we need to change. Click the Network dropdown and select your VPC. The subnet should automatically fill in with one of the subnets in your VPC. Click on the Auto-assign Public IP dropdown and choose Enable . Attention Take note of which subnet is selected! We want each of our EC2 instances for the control plane to be on different subnets; this will give us instances in different availability zones for resilience. Scroll down to the bottom of the form and find the user data field. This field can contain a script that will be automatically executed when the instance is created. Click the as file radio button, click the Choose file button and select the script that you downloaded when you created your satellite location. Note Remmeber that you updated the script with some additional commands after you downloaded it. Before selecting the file, check to be sure those updates are in the file. Click the Next: Add Storage button. All we need to do on the storage page is change the size of the volume to make sure it meets the minimum requirements. Click in the Size field and change the value to 100 . Click Next: Add Tags . Add some tags if you wish. At a minumum you should provide a Name tag, which gives your EC2 instance a name. It might be a good idea to add tags for sat-location-name and sat-location-id so that you have traceability back to your IBM Cloud Satellite location. When done adding tags click Next: Configure Security Group . Next we need to specify the security group to attach to this EC2 instance. We already created one earlier, so we will use that one. Click the radio button for Select an **existing** security group , then click the checkbox next to the security group you created. Click Review and Launch . Review the details on the Review Instance Launch page to verify that they are correct. Caution You might see a warning that your security group is open to the world. That is because we used 0.0.0.0/0 as the source for the inbound security rules. That is okay for demo and testing purposes but is not best practices for production. IBM documents the minumum required inbound and outbound connectivity requirements that are needed for your location to function properly. More access may be required for services you provision into your IBM Satellite location. When you are satisfied that everything is correct, click the Launch button. A new dialog will pop up asking you to specify a key pair. Make sure the first dropdown is set to Choose an eisting key pair . If not, click the dropdown and select it. Then click the second dropdown to choose the key pair you created earlier. Click the checkbox acknowledging that you can't access the intance without the file. Click launch Instances . You will be shown a Launch Status page. Click the View Instances button at the bottom to go back to the Instances dashboard. You may also want to go back to your IBM Satellite location and view the Hosts tab. After a few minutes your new EC2 instance should show up. At this point you should see your EC2 instance being provisioned. Once created it will run the registration script you generated when you created your location and will attach itself to your location. Usually this process takes just a few minutes. When it is ready you will see your host on the Hosts page for your location. Now repeat the steps above to create two more EC2 instances. Remember to: Use a different subnet in your VPC for each instance. You want each of your three control plane hosts to be in a different availability zone. Use a different value for the Name tag so that each instance has a unique name. Once all three instances are up and running your Instances view in the EC2 dashboard will look like this: Attention The control plane for an IBM Cloud Satellite location requires at least 3 hosts that are managed from 3 different availability zones in IBM Cloud. The hosts themselves do not have to be separated from each other, but it is a good practice to separate them if possible. When they have all been attached to your satellite location you will see them in the Hosts page. Note It may take several minutes for the hosts to appear in the satellite location hosts page. Congratulations! You have successfully created EC2 instances and attached them to your IBM Satellite location. Next we will assign them to the control plane for your location.","title":"Create Hosts"},{"location":"satellite/aws/create-hosts/#create-hosts","text":"We will use the script generated you created the location as the User Data to be passed into each EC2 instance created. Don't forget to update it with the right update commands after the API_URL variable, as documented. Navigate to the EC2 console and let's get started!","title":"Create Hosts"},{"location":"satellite/aws/create-hosts/#create-ssh-key","text":"To create an SSH Key: Click on the Key pairs link in the EC2 console. You will probably see that you have no key pairs in the list. Note AWS will allow you to generate a new key pair, in which it will generate and download a key file (a file with a .pem extension) that you can later specify as the private key when you use the ssh command. It will save the public key in the key pair object in AWS and add that public key to your EC2 instance when you create it (assuming you choose this key). The other option is to import a key. In this case you will import the contents of your public key and AWS stores it in the key pair. You can use this option if you already have an SSH key and don't want to create a new one. If you choose to import an existing key, click the Actions dropdown at the top of the list and select import key pair . Give your key a name and copy/paste the contents of your key into the text box in the Key pair file section. You could also use the Browse button to choose the file. Click Import key pair . If you choose to create a new key pair click the Create key pair button above the list of keys on the Key pairs console. Give your key a name, leave the file format set to pem and click Create key pair . Warning Be aware that when you clicked the Create key pair button, the browser automatically downloaded a file. This is very important!! This is your private key, and you will need it later if you use the ssh command to access your EC2 instance. DO NOT LOSE THIS FILE! You will never get a chance to download it again and without it you will not be able to access your EC2 instance.","title":"Create SSH Key"},{"location":"satellite/aws/create-hosts/#create-ec2-instances","text":"Now we are ready to create some EC2 instances to use for the control plane for you IBM Cloud Satellite location. These hosts have some minimum requirements, which are documented here . For the most part, the key requirements are: at least 4 vCPUs at least 16GB Memory at least 100GB attached storage To create an EC2 instance: Click on the Instances link in the left navigation menu of the EC2 Dashboard. Click on the Launch instances dropdown and select Launch instances . The first step is to select an Amazon Machine Image (AMI) to use for the EC2 instance. Search for RHEL-7.9 in the search window. We want to use a community image, so click the link for results in Community AMIs. There should be an image on the list with a name like RHEL-7.9_HVM_GA-20200917-x86_64-0-Hourly2-GP2 . Click the Select button on the line containing that image. Next you need to choose the instance type. This is the hardware configuration for the instance. Click the checkbox next to t3.xlarge . This type has 4 vCPUs and 16GB of RAM. Click Next: Configure Instance Details . In the Configure Instance Details page there are several things we need to change. Click the Network dropdown and select your VPC. The subnet should automatically fill in with one of the subnets in your VPC. Click on the Auto-assign Public IP dropdown and choose Enable . Attention Take note of which subnet is selected! We want each of our EC2 instances for the control plane to be on different subnets; this will give us instances in different availability zones for resilience. Scroll down to the bottom of the form and find the user data field. This field can contain a script that will be automatically executed when the instance is created. Click the as file radio button, click the Choose file button and select the script that you downloaded when you created your satellite location. Note Remmeber that you updated the script with some additional commands after you downloaded it. Before selecting the file, check to be sure those updates are in the file. Click the Next: Add Storage button. All we need to do on the storage page is change the size of the volume to make sure it meets the minimum requirements. Click in the Size field and change the value to 100 . Click Next: Add Tags . Add some tags if you wish. At a minumum you should provide a Name tag, which gives your EC2 instance a name. It might be a good idea to add tags for sat-location-name and sat-location-id so that you have traceability back to your IBM Cloud Satellite location. When done adding tags click Next: Configure Security Group . Next we need to specify the security group to attach to this EC2 instance. We already created one earlier, so we will use that one. Click the radio button for Select an **existing** security group , then click the checkbox next to the security group you created. Click Review and Launch . Review the details on the Review Instance Launch page to verify that they are correct. Caution You might see a warning that your security group is open to the world. That is because we used 0.0.0.0/0 as the source for the inbound security rules. That is okay for demo and testing purposes but is not best practices for production. IBM documents the minumum required inbound and outbound connectivity requirements that are needed for your location to function properly. More access may be required for services you provision into your IBM Satellite location. When you are satisfied that everything is correct, click the Launch button. A new dialog will pop up asking you to specify a key pair. Make sure the first dropdown is set to Choose an eisting key pair . If not, click the dropdown and select it. Then click the second dropdown to choose the key pair you created earlier. Click the checkbox acknowledging that you can't access the intance without the file. Click launch Instances . You will be shown a Launch Status page. Click the View Instances button at the bottom to go back to the Instances dashboard. You may also want to go back to your IBM Satellite location and view the Hosts tab. After a few minutes your new EC2 instance should show up. At this point you should see your EC2 instance being provisioned. Once created it will run the registration script you generated when you created your location and will attach itself to your location. Usually this process takes just a few minutes. When it is ready you will see your host on the Hosts page for your location. Now repeat the steps above to create two more EC2 instances. Remember to: Use a different subnet in your VPC for each instance. You want each of your three control plane hosts to be in a different availability zone. Use a different value for the Name tag so that each instance has a unique name. Once all three instances are up and running your Instances view in the EC2 dashboard will look like this: Attention The control plane for an IBM Cloud Satellite location requires at least 3 hosts that are managed from 3 different availability zones in IBM Cloud. The hosts themselves do not have to be separated from each other, but it is a good practice to separate them if possible. When they have all been attached to your satellite location you will see them in the Hosts page. Note It may take several minutes for the hosts to appear in the satellite location hosts page. Congratulations! You have successfully created EC2 instances and attached them to your IBM Satellite location. Next we will assign them to the control plane for your location.","title":"Create EC2 instances"},{"location":"satellite/aws/create-location/","text":"Overview Creating an IBM Satellite Location is actually pretty simple. We simply need to give it a name and specify the IBM Cloud Multi-Zone Region from which the satellite location will be managed. We also need to generate and download a script that will be run on each host to attach it to the satellite location. Create Location Click on the icon then click on Satellite . Click on the Create a Satellite location button. Fill out the name and (optional) description for your location. Click on the Managed from dropdown and select Washington DC . Info IBM Satellite locations can have hosts wherever you need them, but your location is still managed from a Multi-Zone Region (MZR) in IBM Cloud. There will be management traffic between hosts in your location and the MZR, so be sure to select the MZR that is closest to your location. Click the Create Location button to create your location. You will be taken to the Getting Started page for your location. It is a helpful wizard that explains what you need to do next to finish setting up your location. We will be doing some of this in the next steps, but we do need to generate a script that we will need later. Click on Next . Click on the Generate script button. Add some tags if you wish, then click the Download script button. We will come back and finish the configuration later. For now, click the Overview link on the left navigation menu to see a summary of your location. Note The status field for your location may be slighly different than the one in the screen shot above. Tip Many of the components you will create in AWS allow tags, so take note of the name and ID of your location. It is helpful to add sat-location-name and sat-location-id tags to your AWS resources using these values so that you can later link them back to which location they are attached. Update registration script In order to attach hosts to your satellite location you have to run the script you downloaded in the previous section. In many cases you will need to do this manually, but with AWS you can provide a cloud-init script that will run when the EC2 instance gets created. We will use this feature, but first we need to edit the script; we also need to run some operating system updates, as documented step 2 here , and we can add these commands to the script. Open the script you downloaded in the text editor of your choice. Add the following lines to the script after the line that sets the API_URL environment variable. # Enable AWS RHEL package updates yum update -y yum-config-manager --enable '*' yum repolist all yum install container-selinux -y echo \"repos enabled\" Your script should look like this: Save the script; we will use it later when we create the hosts. That's it for now. Next it is time to create all of the prerequisite resources in AWS. (Alternative Path) Creating AWS resources with terraform If you know how to use terraform there is a folder in this repo that contains terraform scripts for creating all of the AWS resources, including the EC2 instances. Before you run the terraform you need to create your location and generate/download the registration script. Note This terraform will create 3 EC2 instance for the control plane and 3 instances for use as worker nodes for an OpenShift cluster. If you don't want to create the worker node hosts right away you can comment them out in main.tf . To use Terraform to create your AWS resources: Create your location and download the registration script. Copy and paste the contents of the registration script into the userdata.sh script in the terraform folder. Update the userdata.sh script by following the step 2 of the instructions here . You only need to do step 2. Update the provider.tf file with the appropriate AWS region you want to use. Update the variables.tf file as appropriate: Set the satellite-location-name to a value that does not include any spaces. This does not have to match your location name in IBM Cloud, it's just a value to be used when naming resources. Set the availability zones as appropriate for the region you chose. Specify the name of the SSH key you created in AWS. Specify the AMI to use. The name of the one I used is RHEL-7.9_HVM_GA-20200917-x86_64-0-Hourly2-GP2 . In my account the AMI id is ami-0d2bf41df19c4aac7 but I am not sure that is the same ID in every account. set the control-plane-instance-type as appropriate. The type t3.xlarge does work. set the `worker-plane-instance type as appropriate. It can be the same as the control plane. Make sure you have the AWS CLI installed and a configuration profile set so that it knows what credentials to use. More information about authentication with the AWS terraform provider can be found here . If you don't want to install the AWS CLI you might be able to use environment variables to set your access key and secret key. Run terraform init to initialize the AWS terraform provider. Run terraform plan to see what the script will do, then run terraform apply to create the resources.","title":"Create Location"},{"location":"satellite/aws/create-location/#overview","text":"Creating an IBM Satellite Location is actually pretty simple. We simply need to give it a name and specify the IBM Cloud Multi-Zone Region from which the satellite location will be managed. We also need to generate and download a script that will be run on each host to attach it to the satellite location.","title":"Overview"},{"location":"satellite/aws/create-location/#create-location","text":"Click on the icon then click on Satellite . Click on the Create a Satellite location button. Fill out the name and (optional) description for your location. Click on the Managed from dropdown and select Washington DC . Info IBM Satellite locations can have hosts wherever you need them, but your location is still managed from a Multi-Zone Region (MZR) in IBM Cloud. There will be management traffic between hosts in your location and the MZR, so be sure to select the MZR that is closest to your location. Click the Create Location button to create your location. You will be taken to the Getting Started page for your location. It is a helpful wizard that explains what you need to do next to finish setting up your location. We will be doing some of this in the next steps, but we do need to generate a script that we will need later. Click on Next . Click on the Generate script button. Add some tags if you wish, then click the Download script button. We will come back and finish the configuration later. For now, click the Overview link on the left navigation menu to see a summary of your location. Note The status field for your location may be slighly different than the one in the screen shot above. Tip Many of the components you will create in AWS allow tags, so take note of the name and ID of your location. It is helpful to add sat-location-name and sat-location-id tags to your AWS resources using these values so that you can later link them back to which location they are attached.","title":"Create Location"},{"location":"satellite/aws/create-location/#update-registration-script","text":"In order to attach hosts to your satellite location you have to run the script you downloaded in the previous section. In many cases you will need to do this manually, but with AWS you can provide a cloud-init script that will run when the EC2 instance gets created. We will use this feature, but first we need to edit the script; we also need to run some operating system updates, as documented step 2 here , and we can add these commands to the script. Open the script you downloaded in the text editor of your choice. Add the following lines to the script after the line that sets the API_URL environment variable. # Enable AWS RHEL package updates yum update -y yum-config-manager --enable '*' yum repolist all yum install container-selinux -y echo \"repos enabled\" Your script should look like this: Save the script; we will use it later when we create the hosts. That's it for now. Next it is time to create all of the prerequisite resources in AWS.","title":"Update registration script"},{"location":"satellite/aws/create-location/#alternative-path-creating-aws-resources-with-terraform","text":"If you know how to use terraform there is a folder in this repo that contains terraform scripts for creating all of the AWS resources, including the EC2 instances. Before you run the terraform you need to create your location and generate/download the registration script. Note This terraform will create 3 EC2 instance for the control plane and 3 instances for use as worker nodes for an OpenShift cluster. If you don't want to create the worker node hosts right away you can comment them out in main.tf . To use Terraform to create your AWS resources: Create your location and download the registration script. Copy and paste the contents of the registration script into the userdata.sh script in the terraform folder. Update the userdata.sh script by following the step 2 of the instructions here . You only need to do step 2. Update the provider.tf file with the appropriate AWS region you want to use. Update the variables.tf file as appropriate: Set the satellite-location-name to a value that does not include any spaces. This does not have to match your location name in IBM Cloud, it's just a value to be used when naming resources. Set the availability zones as appropriate for the region you chose. Specify the name of the SSH key you created in AWS. Specify the AMI to use. The name of the one I used is RHEL-7.9_HVM_GA-20200917-x86_64-0-Hourly2-GP2 . In my account the AMI id is ami-0d2bf41df19c4aac7 but I am not sure that is the same ID in every account. set the control-plane-instance-type as appropriate. The type t3.xlarge does work. set the `worker-plane-instance type as appropriate. It can be the same as the control plane. Make sure you have the AWS CLI installed and a configuration profile set so that it knows what credentials to use. More information about authentication with the AWS terraform provider can be found here . If you don't want to install the AWS CLI you might be able to use environment variables to set your access key and secret key. Run terraform init to initialize the AWS terraform provider. Run terraform plan to see what the script will do, then run terraform apply to create the resources.","title":"(Alternative Path) Creating AWS resources with terraform"},{"location":"satellite/aws/prerequisites/","text":"Prerequisites Before we can create hosts to be used with an IBM Cloud Satellite location we must build a number of underlying resources that will be needed. We also need to set up appropriate configuration of the networking components so that traffic will be allowed to flow appropriately betweeen IBM Cloud and your location. These instructions assume that you have an AWS account (either your own account or a user in an existing acccout) where you have Administrator access. Technically you only need certain permissions to create the necessary resources, but for convenience you should try and use an account where you have administrive access. You need virtual servers (EC2 instances) to be hosts for your location. To create EC2 instance you will first need to create some other resources: VPC 3 subnets in different availability zones Internet gateway attached to VPC Route table for VPC with two routes: \"local\" - this should be created by default 0.0.0.0/0 - Internet gateway Security Group with appropriate rules SSH key So log into your AWS account and let's get started! Create VPC Navigate to the VPC console in AWS. The console will look like the screen shot below. Notice the AWS region that is selected, as indicated by the blue box in the top right corner. This is the AWS region the VPC console will use by default when you create resources. Tip Make sure that you select the region in which you want to create your IBM Satellite location before creating any resources. To create a VPC: Click on the VPCs link to see the list of VPCs for this region. AWS automatically creates a default VPC for you when you create your account, so there should already be one in the list. We will be creating a new one for your satellite location, so click on the Create VPC button. Provide a name for your VPC. Info CIDR is a notation for defining blocks of IP addresses. There are many tools on the internet for converting CIDR blocks to IP Ranges and vice versa. I commonly use IP Address Guide . For this location we will need 3 subnets, and each one only needs 16 IP addresses. The CIDR block for the VPC represents all of the IP addresses that could be used in all of its subnets. We will use /28 (16 IP Addresses) for our subnets and /24 (255 IP Addresses) for the VPC itself. Enter 10.5.0.0/24 in the IPv4 CIDR block field. Caution IP address ranges within a VPC are private to that VPC. However, there may be instances where you need to connect multiple subnets to each other to allow traffic to flow between them. Therefore, in real scenarios you should take care to use CIDR blocks for each VPC that do not overlap with other VPCs. That way each subnet will have a unique set of IP addresses and will not cause conflicting routes. Add tags to your VPC. There is already one there for the name of the VPC; I usually add tags with the name and id of my satellite location in IBM Cloud for traceability. Since we created the location already I have added two additional tags: sat-location-name and sat-location-id using the values from my location. Click Create VPC . Create Internet Gateway In order for your hosts to communicate with resources on the internet (and for them to be reachable from the internet) you need an Internet Gateway. After the previous step where you created your VPC you probably landed on a page that looks like this: To create an Internet Gateway: Click on the Internet Gateways link in the left navigation menu. There should already be an internet gateway on this list; it is attached to your default VPC. Click the Create internet gateway button. Give your gateway a name and add some tags if you wish. Click on the Create internet gateway button to create your gateway. Notice that the gateway status is Detached , as indicated by the blue box in the image below. We need to attach this gateway to the VPC we just created. If the banner is still visible, like in the image below, click on the Attach to a VPC button. If not, click on the Actions menu and choose Attach VPC . Click in the Available VPCs search box and select the VPC you just created. Click Attach internet gateway . Your internet gateway is now attached to your VPC. Create Security Group Click on the Security Groups link in the left navigation menu of the VPC dashboard. Give your security group a name and description. Click on the VPC dropdown and select your VPC. In the Outbound rules you can see that there is already a rule to allow traffic on all ports to all IPv4 destinations. Attention If you are not able to open up all protocols and ports for outbound connectivity, you must meet the minimun requirements documented here . Otherwize your location will not be able to function properly. In the Inbound rules section we will need to add some rules. Use the table below to as you create the inbound rules: Type Port range Source Description Custom TCP 30000-32767 0.0.0.0/0 Allow IBM Cloud to set up and manage your Satellite location All Traffic All Security Group Allow all traffic for hosts in this security group Custom TCP 80 0.0.0.0/0 Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network Custom TCP 443 0.0.0.0/0 Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network Custom TCP 22 0.0.0.0/0 Optional: Allow SSH traffic Attention The minimum set of inbound rules required for your location to function properly can be found here . Let's create the first inbound rule. In the Inbound rules section, click the Add rule button. set the port range to 30000-32767 . Click on the Source search field and select 0.0.0.0/0 . Add a description if you wish. Click the Add rule button again to create the third rule. Set the port range to 80 . Click on the Source search field and select 0.0.0.0/0 . Click the Add rule button again to create the fourth rule for port 443 . Click on the Source search field and select 0.0.0.0/0 . Click the Add rule button again to create the fifth rule for port 22 . Click on the Source search field and select 0.0.0.0/0 . At this point your inbound and outbound rules should look like this: Note Need to update this graphic. Security group rule is missing! Apparently we can't add a rule for this security group until we create it. Therefore we will need to create it, then come back in and add the last rule. Add some tags if you wish. Scroll to the bottom of the page and click Create security group . You should see that your group was created: We have one more rule to add to the security group - one that allows all traffic on all ports from sources inside this security group. This rule will allow all hosts attached to this security group to talk to each other on all ports. Click on the Edit inbound rules button. Click on Add rule to create another rule. Set the Type to All traffic and the Source type to Custom . Click in the Source field and choose your security group from the Security Groups section of the list. Click Save rules . The updated set of rules should look like this: Create Subnets Next we need to create some subnets in the VPC so that we can provision some hosts. In AWS subnets are tied to a specific Availability Zone (AZ) with in a region. An availability zone is defined as a unique data center/location with its own independent power, networking, cooling, etc. Availability zones allow you to separate workloads in order to build resilient, highly available applications or systems. Regions can have different nubmers of availability zones, but each region will have at least three of them. Info There is no hard and fast requirement about workload isolation for IBM Cloud Satellite, but is considered a best practices to have hosts in at least 3 different availability zones for any IBM Cloud service you provision into your satellite location. Example The table below contains information we will use to create subnets for this exercise. Each subnet will be created in a separate availability zone in AWS. This will allow us to provision hosts in each zone. Subnet Name Availability Zone CIDR Block public-us-east-1a US East (N. Virginia) / us-east-1a 10.5.0.0/28 public-us-east-1b US East (N. Virginia) / us-east-1b 10.5.0.16/28 public-us-east-1c US East (N. Virginia) / us-east-1c 10.5.0.32/28 To create a subnet: Click on the Subnets link in the left navigation menu on the VPC Dashboard. There should already be some subnets in the list; these are most likely the subnets that were automatically created with the default VPC. We will need to create new ones, so click the Create subnet button in the top right corner. Click the VPC ID field and select the VPC you created. Give your subnet a name; you can use the one from the first row of the table above or use your own. Select the Availability zone. This step is very important! Your IPv4 CIDR block must fall within the CIDR block for the VPC (which is shown at the top of the page), and it must not conflict with any other subnets. Provide a value in the IPv4 CIDR block field. Add some tags if you wish and click the Create subnet button at the bottom right. Repeat the steps above to create another subnet in a different availability zone. Repeat the steps above to create a third subnet in a different availability zone than the other two subnets. You should now have 3 new subnets, in different availability zones, in your new VPC. Create Route Table Route tables define the appropriate rules for routing traffic where it needs to go. A default route table was created when you created your VPC; it has one route in it that allows traffic within the VPC to be routed to hosts anywhere inside the CIDR block defined for the VPC. This is often called a \"local\" route because the target field is set to local . Any traffic not destined for a host within the VPC will not get routed, as there are no valid routes to IP addresses outside the VPC. This is the default routing that AWS sets up for VPCs, in order to protect resources in the VPC. When subnets are created in the VPC they are implicitly associated the default route table unless they are explicity associated with one. To create a route table: Navigate to the route tables list by clicking on the Route Tables link in the left navigation menu. Click on the blue Create route table button. Give your route table a name in the Name tag field. Click on the VPC dropdown and select your VPC. Add some tags if you wish and then click the Create button. Click the Close button. You should now be back on the list of route tables. Click the checkbox next to the name of your route table. Notice that some tabs below appear below. Click on the Routes tab. You can see that your route table has one route already; this is the default route that allows traffic to flow freely within the VPC. Attention This route is a requirement for IBM Cloud Satellite. All of the hosts within the location must be able to communicate with each other. It is standard practice to have this rule in the route table; if isolation is needed it can be implemented in security groups. We need to add another route that will cover traffic whose destination is outside the VPC. Earlier we created an internet gateway to allow access to the internet; now we need to add a route to send that traffic to the internet gateway. Click on the Edit routes button. Click the Add route button to add the new route. Enter 0.0.0.0/0 in the destination field. This will send all traffic not covered by other routes in the table. In the Target field choose Internet Gateway . This will change the field into a dropdown that should show the internet gateway you created earlier. Then Click on Save routes . Click Close . Now you can see that the route table has been updated with your new route. The last thing we need to do is associate this route table to the subnets in your VPC. Click on the Subnet Associations tab. Click the Edit subnet associations button. Click the checkbox by each of the 3 subnets and then click Save . Now you can see that all of your subnets are explicitly associated with your route table.","title":"Prerequisites"},{"location":"satellite/aws/prerequisites/#prerequisites","text":"Before we can create hosts to be used with an IBM Cloud Satellite location we must build a number of underlying resources that will be needed. We also need to set up appropriate configuration of the networking components so that traffic will be allowed to flow appropriately betweeen IBM Cloud and your location. These instructions assume that you have an AWS account (either your own account or a user in an existing acccout) where you have Administrator access. Technically you only need certain permissions to create the necessary resources, but for convenience you should try and use an account where you have administrive access. You need virtual servers (EC2 instances) to be hosts for your location. To create EC2 instance you will first need to create some other resources: VPC 3 subnets in different availability zones Internet gateway attached to VPC Route table for VPC with two routes: \"local\" - this should be created by default 0.0.0.0/0 - Internet gateway Security Group with appropriate rules SSH key So log into your AWS account and let's get started!","title":"Prerequisites"},{"location":"satellite/aws/prerequisites/#create-vpc","text":"Navigate to the VPC console in AWS. The console will look like the screen shot below. Notice the AWS region that is selected, as indicated by the blue box in the top right corner. This is the AWS region the VPC console will use by default when you create resources. Tip Make sure that you select the region in which you want to create your IBM Satellite location before creating any resources. To create a VPC: Click on the VPCs link to see the list of VPCs for this region. AWS automatically creates a default VPC for you when you create your account, so there should already be one in the list. We will be creating a new one for your satellite location, so click on the Create VPC button. Provide a name for your VPC. Info CIDR is a notation for defining blocks of IP addresses. There are many tools on the internet for converting CIDR blocks to IP Ranges and vice versa. I commonly use IP Address Guide . For this location we will need 3 subnets, and each one only needs 16 IP addresses. The CIDR block for the VPC represents all of the IP addresses that could be used in all of its subnets. We will use /28 (16 IP Addresses) for our subnets and /24 (255 IP Addresses) for the VPC itself. Enter 10.5.0.0/24 in the IPv4 CIDR block field. Caution IP address ranges within a VPC are private to that VPC. However, there may be instances where you need to connect multiple subnets to each other to allow traffic to flow between them. Therefore, in real scenarios you should take care to use CIDR blocks for each VPC that do not overlap with other VPCs. That way each subnet will have a unique set of IP addresses and will not cause conflicting routes. Add tags to your VPC. There is already one there for the name of the VPC; I usually add tags with the name and id of my satellite location in IBM Cloud for traceability. Since we created the location already I have added two additional tags: sat-location-name and sat-location-id using the values from my location. Click Create VPC .","title":"Create VPC"},{"location":"satellite/aws/prerequisites/#create-internet-gateway","text":"In order for your hosts to communicate with resources on the internet (and for them to be reachable from the internet) you need an Internet Gateway. After the previous step where you created your VPC you probably landed on a page that looks like this: To create an Internet Gateway: Click on the Internet Gateways link in the left navigation menu. There should already be an internet gateway on this list; it is attached to your default VPC. Click the Create internet gateway button. Give your gateway a name and add some tags if you wish. Click on the Create internet gateway button to create your gateway. Notice that the gateway status is Detached , as indicated by the blue box in the image below. We need to attach this gateway to the VPC we just created. If the banner is still visible, like in the image below, click on the Attach to a VPC button. If not, click on the Actions menu and choose Attach VPC . Click in the Available VPCs search box and select the VPC you just created. Click Attach internet gateway . Your internet gateway is now attached to your VPC.","title":"Create Internet Gateway"},{"location":"satellite/aws/prerequisites/#create-security-group","text":"Click on the Security Groups link in the left navigation menu of the VPC dashboard. Give your security group a name and description. Click on the VPC dropdown and select your VPC. In the Outbound rules you can see that there is already a rule to allow traffic on all ports to all IPv4 destinations. Attention If you are not able to open up all protocols and ports for outbound connectivity, you must meet the minimun requirements documented here . Otherwize your location will not be able to function properly. In the Inbound rules section we will need to add some rules. Use the table below to as you create the inbound rules: Type Port range Source Description Custom TCP 30000-32767 0.0.0.0/0 Allow IBM Cloud to set up and manage your Satellite location All Traffic All Security Group Allow all traffic for hosts in this security group Custom TCP 80 0.0.0.0/0 Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network Custom TCP 443 0.0.0.0/0 Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network Custom TCP 22 0.0.0.0/0 Optional: Allow SSH traffic Attention The minimum set of inbound rules required for your location to function properly can be found here . Let's create the first inbound rule. In the Inbound rules section, click the Add rule button. set the port range to 30000-32767 . Click on the Source search field and select 0.0.0.0/0 . Add a description if you wish. Click the Add rule button again to create the third rule. Set the port range to 80 . Click on the Source search field and select 0.0.0.0/0 . Click the Add rule button again to create the fourth rule for port 443 . Click on the Source search field and select 0.0.0.0/0 . Click the Add rule button again to create the fifth rule for port 22 . Click on the Source search field and select 0.0.0.0/0 . At this point your inbound and outbound rules should look like this: Note Need to update this graphic. Security group rule is missing! Apparently we can't add a rule for this security group until we create it. Therefore we will need to create it, then come back in and add the last rule. Add some tags if you wish. Scroll to the bottom of the page and click Create security group . You should see that your group was created: We have one more rule to add to the security group - one that allows all traffic on all ports from sources inside this security group. This rule will allow all hosts attached to this security group to talk to each other on all ports. Click on the Edit inbound rules button. Click on Add rule to create another rule. Set the Type to All traffic and the Source type to Custom . Click in the Source field and choose your security group from the Security Groups section of the list. Click Save rules . The updated set of rules should look like this:","title":"Create Security Group"},{"location":"satellite/aws/prerequisites/#create-subnets","text":"Next we need to create some subnets in the VPC so that we can provision some hosts. In AWS subnets are tied to a specific Availability Zone (AZ) with in a region. An availability zone is defined as a unique data center/location with its own independent power, networking, cooling, etc. Availability zones allow you to separate workloads in order to build resilient, highly available applications or systems. Regions can have different nubmers of availability zones, but each region will have at least three of them. Info There is no hard and fast requirement about workload isolation for IBM Cloud Satellite, but is considered a best practices to have hosts in at least 3 different availability zones for any IBM Cloud service you provision into your satellite location. Example The table below contains information we will use to create subnets for this exercise. Each subnet will be created in a separate availability zone in AWS. This will allow us to provision hosts in each zone. Subnet Name Availability Zone CIDR Block public-us-east-1a US East (N. Virginia) / us-east-1a 10.5.0.0/28 public-us-east-1b US East (N. Virginia) / us-east-1b 10.5.0.16/28 public-us-east-1c US East (N. Virginia) / us-east-1c 10.5.0.32/28 To create a subnet: Click on the Subnets link in the left navigation menu on the VPC Dashboard. There should already be some subnets in the list; these are most likely the subnets that were automatically created with the default VPC. We will need to create new ones, so click the Create subnet button in the top right corner. Click the VPC ID field and select the VPC you created. Give your subnet a name; you can use the one from the first row of the table above or use your own. Select the Availability zone. This step is very important! Your IPv4 CIDR block must fall within the CIDR block for the VPC (which is shown at the top of the page), and it must not conflict with any other subnets. Provide a value in the IPv4 CIDR block field. Add some tags if you wish and click the Create subnet button at the bottom right. Repeat the steps above to create another subnet in a different availability zone. Repeat the steps above to create a third subnet in a different availability zone than the other two subnets. You should now have 3 new subnets, in different availability zones, in your new VPC.","title":"Create Subnets"},{"location":"satellite/aws/prerequisites/#create-route-table","text":"Route tables define the appropriate rules for routing traffic where it needs to go. A default route table was created when you created your VPC; it has one route in it that allows traffic within the VPC to be routed to hosts anywhere inside the CIDR block defined for the VPC. This is often called a \"local\" route because the target field is set to local . Any traffic not destined for a host within the VPC will not get routed, as there are no valid routes to IP addresses outside the VPC. This is the default routing that AWS sets up for VPCs, in order to protect resources in the VPC. When subnets are created in the VPC they are implicitly associated the default route table unless they are explicity associated with one. To create a route table: Navigate to the route tables list by clicking on the Route Tables link in the left navigation menu. Click on the blue Create route table button. Give your route table a name in the Name tag field. Click on the VPC dropdown and select your VPC. Add some tags if you wish and then click the Create button. Click the Close button. You should now be back on the list of route tables. Click the checkbox next to the name of your route table. Notice that some tabs below appear below. Click on the Routes tab. You can see that your route table has one route already; this is the default route that allows traffic to flow freely within the VPC. Attention This route is a requirement for IBM Cloud Satellite. All of the hosts within the location must be able to communicate with each other. It is standard practice to have this rule in the route table; if isolation is needed it can be implemented in security groups. We need to add another route that will cover traffic whose destination is outside the VPC. Earlier we created an internet gateway to allow access to the internet; now we need to add a route to send that traffic to the internet gateway. Click on the Edit routes button. Click the Add route button to add the new route. Enter 0.0.0.0/0 in the destination field. This will send all traffic not covered by other routes in the table. In the Target field choose Internet Gateway . This will change the field into a dropdown that should show the internet gateway you created earlier. Then Click on Save routes . Click Close . Now you can see that the route table has been updated with your new route. The last thing we need to do is associate this route table to the subnets in your VPC. Click on the Subnet Associations tab. Click the Edit subnet associations button. Click the checkbox by each of the 3 subnets and then click Save . Now you can see that all of your subnets are explicitly associated with your route table.","title":"Create Route Table"},{"location":"satellite/ibm-cloud/classic/assign-hosts/","text":"Assign hosts to the control plane Hosts will become available when the attach script on each host runs. You will need a minimum of three hosts for the satellite location control plane. Ideally these three machines will be geographically dispursed in some way so that the loss of one control plane host does not cause an outage. As each of your three hosts that you want to use for your control plane become available on the Hosts tab, you can assign them to the control plane. When you assign a host to the control plane you choose which Availability zone in the IBM Cloud Multi-Zone Region (MZR) you want to manage the host. You will be required to assign each host to a different Availability Zone; for resiliency purposes the location will not become available until it has 3 control plane hosts managed by 3 different Availability Zones! To assign hosts to the control plane: Go to your satellite location in IBM Cloud and go to the Hosts page using the left navigation menu. You should see your host attached hosts. If some are not there and you recently ran the registration script, wait a few minutes for them to show up. The registration process typically only takes a few minutes. Click the three dots at the right end of the line containing one of your hosts and choose Assign host . In the dialog that pops up, choose Control plane in the Cluster field. At this point it will be your only choice, but once you have the control plane provisioned and you assign other hosts, you could assign more hosts to your control plane. Or, you could assign them to other clusters that you may have created. Repeat these steps for your other two hosts. Be sure to assign each one to a different zone in the Zone field; your location will not become available until your control plane has three hosts assigned to different zones!! After you have assigned all three hosts you will see their status change to Provisioning in the table. That's it! Now you just wait until provisioning is complete. You can monitor the progress from the Overview page for your location. You will see various messages in the Deployment messages field as it progresses. When your location is ready the Overview page will look like this:","title":"Assign Hosts to Location"},{"location":"satellite/ibm-cloud/classic/assign-hosts/#assign-hosts-to-the-control-plane","text":"Hosts will become available when the attach script on each host runs. You will need a minimum of three hosts for the satellite location control plane. Ideally these three machines will be geographically dispursed in some way so that the loss of one control plane host does not cause an outage. As each of your three hosts that you want to use for your control plane become available on the Hosts tab, you can assign them to the control plane. When you assign a host to the control plane you choose which Availability zone in the IBM Cloud Multi-Zone Region (MZR) you want to manage the host. You will be required to assign each host to a different Availability Zone; for resiliency purposes the location will not become available until it has 3 control plane hosts managed by 3 different Availability Zones! To assign hosts to the control plane: Go to your satellite location in IBM Cloud and go to the Hosts page using the left navigation menu. You should see your host attached hosts. If some are not there and you recently ran the registration script, wait a few minutes for them to show up. The registration process typically only takes a few minutes. Click the three dots at the right end of the line containing one of your hosts and choose Assign host . In the dialog that pops up, choose Control plane in the Cluster field. At this point it will be your only choice, but once you have the control plane provisioned and you assign other hosts, you could assign more hosts to your control plane. Or, you could assign them to other clusters that you may have created. Repeat these steps for your other two hosts. Be sure to assign each one to a different zone in the Zone field; your location will not become available until your control plane has three hosts assigned to different zones!! After you have assigned all three hosts you will see their status change to Provisioning in the table. That's it! Now you just wait until provisioning is complete. You can monitor the progress from the Overview page for your location. You will see various messages in the Deployment messages field as it progresses. When your location is ready the Overview page will look like this:","title":"Assign hosts to the control plane"},{"location":"satellite/ibm-cloud/classic/attach-hosts/","text":"Attach hosts In order for your hosts to be available in your location you have to attach them by running the script that you downloaded when you created your location. Before you can do that you first need to update your host by running the commands documented here . SSH into your virtual server instance. If you have a mac, the command would look like this: ssh root@<your host ip address> Without the <> , of course. For example: ssh root@163.75.64.199 Note You might see a prompt like the one below. It's telling you that the host has returned a fingerprint that your machine does not recognize. Type in yes at the prompt and SSH will add that fingerprint to the known_hosts file on your machine. The authenticity of host '163.75.64.199 (163.75.64.199)' can't be established. ECDSA key fingerprint is SHA256:pTS6heGnq2hBa4XAQzP1vWx4qFQFD20TgliZ1vc4u9E. Are you sure you want to continue connecting (yes/no/[fingerprint])? Run this command: subscription-manager refresh The output should look like this: [root@sat-control-plane-tor01-01 ~]# subscription-manager refresh All local data refreshed Run this command: subscription-manager repos --enable=* The output should look like this: [root@sat-control-plane-tor01-01 ~]# subscription-manager repos --enable=* Repository 'rhel-ha-for-rhel-7-server-eus-rpms' is enabled for this system. Repository 'rhel-server-rhscl-7-eus-rpms' is enabled for this system. Repository 'rhel-server-rhscl-7-rpms' is enabled for this system. Repository 'rhel-7-server-optional-rpms' is enabled for this system. Repository 'rhel-7-server-eus-optional-rpms' is enabled for this system. Repository 'rhel-7-server-eus-rpms' is enabled for this system. Repository 'rhel-7-server-devtools-rpms' is enabled for this system. Repository 'rhel-ha-for-rhel-7-server-rpms' is enabled for this system. Repository 'rhel-rs-for-rhel-7-server-eus-rpms' is enabled for this system. Repository 'rhel-rs-for-rhel-7-server-rpms' is enabled for this system. Repository 'rhel-7-server-rpms' is enabled for this system. Repository 'rhel-7-server-supplementary-rpms' is enabled for this system. Repository 'rhel-7-server-extras-rpms' is enabled for this system. Repository 'rhel-7-server-eus-supplementary-rpms' is enabled for this system. Exit out of the SSH command by typing exit . You are now back in your local terminal. You need to secure copy the registration script that you downloaded when you created your location over to your host. The command will look like this: scp file.txt username@to_host:/remote/directory/ for example: scp attachHost-Toronto.sh root@<your ip address>:/tmp Note The above example assumes that the script attachHost-Toronto.sh is in the current directory from which you are running the command. If it is in a different location you can specify a fully qualified path. Also, the name of the script will be different, as the location name is part of the script name. For example: Samaritan:tmp dwakeman$ scp attachHost-Toronto.sh root@169.55.188.70:/tmp attachHost-Toronto.sh 100% 5435 108.8KB/s 00:00 Now you can SSH back into your host and run the script, as documented in Step 6 here . nohup bash /tmp/<your-script-name>.sh & Note After you type the command and hit Enter it will show you a line about ignoring input. Hit Enter again when you see that. Repeat the steps above for your other two hosts. At this point you should have 3 hosts attached to your location that can assign to your control plane in the next step. Tip IBM Cloud Satellite treats all hosts alike; when you create them you can use them for anything that is supported by IBM Cloud Satellite. In these steps we are using them for the control plane, but the exact same steps can be followed to create other hosts that can be used for services like OpenShift on IBM Cloud.","title":"Attach Hosts"},{"location":"satellite/ibm-cloud/classic/attach-hosts/#attach-hosts","text":"In order for your hosts to be available in your location you have to attach them by running the script that you downloaded when you created your location. Before you can do that you first need to update your host by running the commands documented here . SSH into your virtual server instance. If you have a mac, the command would look like this: ssh root@<your host ip address> Without the <> , of course. For example: ssh root@163.75.64.199 Note You might see a prompt like the one below. It's telling you that the host has returned a fingerprint that your machine does not recognize. Type in yes at the prompt and SSH will add that fingerprint to the known_hosts file on your machine. The authenticity of host '163.75.64.199 (163.75.64.199)' can't be established. ECDSA key fingerprint is SHA256:pTS6heGnq2hBa4XAQzP1vWx4qFQFD20TgliZ1vc4u9E. Are you sure you want to continue connecting (yes/no/[fingerprint])? Run this command: subscription-manager refresh The output should look like this: [root@sat-control-plane-tor01-01 ~]# subscription-manager refresh All local data refreshed Run this command: subscription-manager repos --enable=* The output should look like this: [root@sat-control-plane-tor01-01 ~]# subscription-manager repos --enable=* Repository 'rhel-ha-for-rhel-7-server-eus-rpms' is enabled for this system. Repository 'rhel-server-rhscl-7-eus-rpms' is enabled for this system. Repository 'rhel-server-rhscl-7-rpms' is enabled for this system. Repository 'rhel-7-server-optional-rpms' is enabled for this system. Repository 'rhel-7-server-eus-optional-rpms' is enabled for this system. Repository 'rhel-7-server-eus-rpms' is enabled for this system. Repository 'rhel-7-server-devtools-rpms' is enabled for this system. Repository 'rhel-ha-for-rhel-7-server-rpms' is enabled for this system. Repository 'rhel-rs-for-rhel-7-server-eus-rpms' is enabled for this system. Repository 'rhel-rs-for-rhel-7-server-rpms' is enabled for this system. Repository 'rhel-7-server-rpms' is enabled for this system. Repository 'rhel-7-server-supplementary-rpms' is enabled for this system. Repository 'rhel-7-server-extras-rpms' is enabled for this system. Repository 'rhel-7-server-eus-supplementary-rpms' is enabled for this system. Exit out of the SSH command by typing exit . You are now back in your local terminal. You need to secure copy the registration script that you downloaded when you created your location over to your host. The command will look like this: scp file.txt username@to_host:/remote/directory/ for example: scp attachHost-Toronto.sh root@<your ip address>:/tmp Note The above example assumes that the script attachHost-Toronto.sh is in the current directory from which you are running the command. If it is in a different location you can specify a fully qualified path. Also, the name of the script will be different, as the location name is part of the script name. For example: Samaritan:tmp dwakeman$ scp attachHost-Toronto.sh root@169.55.188.70:/tmp attachHost-Toronto.sh 100% 5435 108.8KB/s 00:00 Now you can SSH back into your host and run the script, as documented in Step 6 here . nohup bash /tmp/<your-script-name>.sh & Note After you type the command and hit Enter it will show you a line about ignoring input. Hit Enter again when you see that. Repeat the steps above for your other two hosts. At this point you should have 3 hosts attached to your location that can assign to your control plane in the next step. Tip IBM Cloud Satellite treats all hosts alike; when you create them you can use them for anything that is supported by IBM Cloud Satellite. In these steps we are using them for the control plane, but the exact same steps can be followed to create other hosts that can be used for services like OpenShift on IBM Cloud.","title":"Attach hosts"},{"location":"satellite/ibm-cloud/classic/attachHost-Toronto/","text":"Click to expand #!/usr/bin/env bash cat << 'HERE' >>/usr/local/bin/ibm-host-attach.sh #!/usr/bin/env bash set -ex mkdir -p /etc/satelliteflags HOST_ASSIGN_FLAG=\"/etc/satelliteflags/hostattachflag\" if [[ -f \"$HOST_ASSIGN_FLAG\" ]]; then echo \"host has already been assigned. need to reload before you try the attach again\" exit 0 fi set +x HOST_QUEUE_TOKEN=\"<a really long token will be here....>\" set -x ACCOUNT_ID=\"<your IBM Cloud Account ID will be here>\" CONTROLLER_ID=\"<your satellite location ID will be here>\" SELECTOR_LABELS='{\"location\":\"toronto\",\"provider\":\"ibm\"}' API_URL=\"https://origin.us-east.containers.cloud.ibm.com/\" #shutdown known blacklisted services for Satellite (these will break kube) set +e systemctl stop -f iptables.service systemctl disable iptables.service systemctl mask iptables.service systemctl stop -f firewalld.service systemctl disable firewalld.service systemctl mask firewalld.service set -e # ensure you can successfully communicate with redhat mirrors (this is a prereq to the rest of the automation working) yum install rh-python36 -y mkdir -p /etc/satellitemachineidgeneration if [[ ! -f /etc/satellitemachineidgeneration/machineidgenerated ]]; then rm -f /etc/machine-id systemd-machine-id-setup touch /etc/satellitemachineidgeneration/machineidgenerated fi #STEP 1: GATHER INFORMATION THAT WILL BE USED TO REGISTER THE HOST HOSTNAME=$(hostname -s) HOSTNAME=${HOSTNAME,,} MACHINE_ID=$(cat /etc/machine-id ) CPUS=$(nproc) MEMORY=$(grep MemTotal /proc/meminfo | awk '{print $2}') export CPUS export MEMORY SELECTOR_LABELS=$(echo \"${SELECTOR_LABELS}\" | python -c \"import sys, json, os; z = json.load(sys.stdin); y = {\\\"cpu\\\": os.getenv('CPUS'), \\\"memory\\\": os.getenv('MEMORY')}; z.update(y); print(json.dumps(z))\") #Step 2: SETUP METADATA cat << EOF > register.json { \"controller\": \"$CONTROLLER_ID\", \"name\": \"$HOSTNAME\", \"identifier\": \"$MACHINE_ID\", \"labels\": $SELECTOR_LABELS } EOF set +x #STEP 3: REGISTER HOST TO THE HOSTQUEUE. NEED TO EVALUATE HTTP STATUS 409 EXISTS, 201 created. ALL OTHERS FAIL. HTTP_RESPONSE=$(curl --write-out \"HTTPSTATUS:%{http_code}\" --retry 100 --retry-delay 10 --retry-max-time 1800 -X POST \\ -H \"X-Auth-Hostqueue-APIKey: $HOST_QUEUE_TOKEN\" \\ -H \"X-Auth-Hostqueue-Account: $ACCOUNT_ID\" \\ -H \"Content-Type: application/json\" \\ -d @register.json \\ \"${API_URL}v2/multishift/hostqueue/host/register\") set -x HTTP_BODY=$(echo \"$HTTP_RESPONSE\" | sed -E 's/HTTPSTATUS\\:[0-9]{3}$//') HTTP_STATUS=$(echo \"$HTTP_RESPONSE\" | tr -d '\\n' | sed -E 's/.*HTTPSTATUS:([0-9]{3})$/\\1/') echo \"$HTTP_BODY\" echo \"$HTTP_STATUS\" if [ \"$HTTP_STATUS\" -ne 201 ]; then echo \"Error [HTTP status: $HTTP_STATUS]\" exit 1 fi HOST_ID=$(echo \"$HTTP_BODY\" | python -c \"import sys, json; print(json.load(sys.stdin)['id'])\") #STEP 4: WAIT FOR MEMBERSHIP TO BE ASSIGNED while true do set +x ASSIGNMENT=$(curl --retry 100 --retry-delay 10 --retry-max-time 1800 -G -X GET \\ -H \"X-Auth-Hostqueue-APIKey: $HOST_QUEUE_TOKEN\" \\ -H \"X-Auth-Hostqueue-Account: $ACCOUNT_ID\" \\ -d controllerID=\"$CONTROLLER_ID\" \\ -d hostID=\"$HOST_ID\" \\ \"${API_URL}v2/multishift/hostqueue/host/getAssignment\") set -x isAssigned=$(echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['isAssigned'])\" | awk '{print tolower($0)}') if [[ \"$isAssigned\" == \"true\" ]] ; then break fi if [[ \"$isAssigned\" != \"false\" ]]; then echo \"unexpected value for assign retrying\" fi sleep 10 done #STEP 5: ASSIGNMENT HAS BEEN MADE. SAVE SCRIPT AND RUN echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['script'])\" > /usr/local/bin/ibm-host-agent.sh export HOST_ID ASSIGNMENT_ID=$(echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['id'])\") cat << EOF > /etc/satelliteflags/ibm-host-agent-vars export HOST_ID=${HOST_ID} export ASSIGNMENT_ID=${ASSIGNMENT_ID} EOF chmod 0600 /etc/satelliteflags/ibm-host-agent-vars chmod 0700 /usr/local/bin/ibm-host-agent.sh cat << EOF > /etc/systemd/system/ibm-host-agent.service [Unit] Description=IBM Host Agent Service After=network.target [Service] Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ExecStart=/usr/local/bin/ibm-host-agent.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF chmod 0644 /etc/systemd/system/ibm-host-agent.service systemctl daemon-reload systemctl start ibm-host-agent.service touch \"$HOST_ASSIGN_FLAG\" HERE chmod 0700 /usr/local/bin/ibm-host-attach.sh cat << 'EOF' >/etc/systemd/system/ibm-host-attach.service [Unit] Description=IBM Host Attach Service After=network.target [Service] Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ExecStart=/usr/local/bin/ibm-host-attach.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF chmod 0644 /etc/systemd/system/ibm-host-attach.service systemctl daemon-reload systemctl enable ibm-host-attach.service systemctl start ibm-host-attach.service","title":"attachHost Toronto"},{"location":"satellite/ibm-cloud/classic/create-hosts/","text":"Create hosts We should create 3 hosts for the Satellite control plane. If possible they should be in three different availability zones. They do NOT need to be in an MZR, but they will be managed from one, so they should be located geographically as close as possible to the MZR you will choose. In the Beta, only US East (Washington DC) is available in North America, and London in Europe. To create a host: Log into the IBM Cloud Portal click the icon and choose Classic Infrastructure . You will probably land on the Devices page, but if you don't click on Devices on the left navigation menu and then click Order . Click on the Virtual Server for Classic tile. On the next screen, fill in the details for your virtual server. This example assumes that a satellite location in Toronto is being created. At a minimum, make these selections: Specify a unique hostname. For location, select the NA East tile and TOR01 - Toronto in the dropdown. In the profile section, click the View all profiles link and choose B1.4x16 . Click Save profile . Select your SSH key (the one you created earlier) in the SSH keys field. In the Image field select the Red Hat tile and choose 7.x Minimal (64-bit) - HVM . In Attached storage disks change the size of the boot disk to 100 GB. (Optional) In the Network interface field click the Uplink port speeds dropdown and choose 1 Gbps non rate-limited public & private network uplinks . Click the Public Security Group dropdown and pick the ibm_satellite security group you created earlier. When done the order page will look like this: Click the checkbox agreeing to the third party service agreements and click Create . Repeat the steps above to create two more virtual servers to be used for your location's control plane. Make sure they have unique host names and choose different locations: TOR04 and TOR05 . This will ensure that these three servers are in different data centers for resiliency purposes.","title":"Create Hosts"},{"location":"satellite/ibm-cloud/classic/create-hosts/#create-hosts","text":"We should create 3 hosts for the Satellite control plane. If possible they should be in three different availability zones. They do NOT need to be in an MZR, but they will be managed from one, so they should be located geographically as close as possible to the MZR you will choose. In the Beta, only US East (Washington DC) is available in North America, and London in Europe. To create a host: Log into the IBM Cloud Portal click the icon and choose Classic Infrastructure . You will probably land on the Devices page, but if you don't click on Devices on the left navigation menu and then click Order . Click on the Virtual Server for Classic tile. On the next screen, fill in the details for your virtual server. This example assumes that a satellite location in Toronto is being created. At a minimum, make these selections: Specify a unique hostname. For location, select the NA East tile and TOR01 - Toronto in the dropdown. In the profile section, click the View all profiles link and choose B1.4x16 . Click Save profile . Select your SSH key (the one you created earlier) in the SSH keys field. In the Image field select the Red Hat tile and choose 7.x Minimal (64-bit) - HVM . In Attached storage disks change the size of the boot disk to 100 GB. (Optional) In the Network interface field click the Uplink port speeds dropdown and choose 1 Gbps non rate-limited public & private network uplinks . Click the Public Security Group dropdown and pick the ibm_satellite security group you created earlier. When done the order page will look like this: Click the checkbox agreeing to the third party service agreements and click Create . Repeat the steps above to create two more virtual servers to be used for your location's control plane. Make sure they have unique host names and choose different locations: TOR04 and TOR05 . This will ensure that these three servers are in different data centers for resiliency purposes.","title":"Create hosts"},{"location":"satellite/ibm-cloud/classic/create-location/","text":"Overview Create the location Now that we have some hosts we can create a new Satellite location and attached them to it. In the IBM Cloud Portal, use the left navigation menu to go to the Satellite landing page. Click on Create a satellite location . Provide a name for your location, a description (optional) and choose Washington DC in the Managed from dropdown. Click on Create Location . You will be taken to the Getting Started page for your location. Click Next . Click on the Generate script link. Provide some Labels if you wish, then click Download script . An example of the content of the script can be viewed here: Click here to expand the script ```sh #!/usr/bin/env bash cat << 'HERE' >>/usr/local/bin/ibm-host-attach.sh #!/usr/bin/env bash set -ex mkdir -p /etc/satelliteflags HOST_ASSIGN_FLAG=\"/etc/satelliteflags/hostattachflag\" if [[ -f \"$HOST_ASSIGN_FLAG\" ]]; then echo \"host has already been assigned. need to reload before you try the attach again\" exit 0 fi set +x HOST_QUEUE_TOKEN=\"<a really long token will be here....>\" set -x ACCOUNT_ID=\"<your IBM Cloud Account ID will be here>\" CONTROLLER_ID=\"<your satellite location ID will be here>\" SELECTOR_LABELS='{\"location\":\"toronto\",\"provider\":\"ibm\"}' API_URL=\"https://origin.us-east.containers.cloud.ibm.com/\" #shutdown known blacklisted services for Satellite (these will break kube) set +e systemctl stop -f iptables.service systemctl disable iptables.service systemctl mask iptables.service systemctl stop -f firewalld.service systemctl disable firewalld.service systemctl mask firewalld.service set -e # ensure you can successfully communicate with redhat mirrors (this is a prereq to the rest of the automation working) yum install rh-python36 -y mkdir -p /etc/satellitemachineidgeneration if [[ ! -f /etc/satellitemachineidgeneration/machineidgenerated ]]; then rm -f /etc/machine-id systemd-machine-id-setup touch /etc/satellitemachineidgeneration/machineidgenerated fi #STEP 1: GATHER INFORMATION THAT WILL BE USED TO REGISTER THE HOST HOSTNAME=$(hostname -s) HOSTNAME=${HOSTNAME,,} MACHINE_ID=$(cat /etc/machine-id ) CPUS=$(nproc) MEMORY=$(grep MemTotal /proc/meminfo | awk '{print $2}') export CPUS export MEMORY SELECTOR_LABELS=$(echo \"${SELECTOR_LABELS}\" | python -c \"import sys, json, os; z = json.load(sys.stdin); y = {\\\"cpu\\\": os.getenv('CPUS'), \\\"memory\\\": os.getenv('MEMORY')}; z.update(y); print(json.dumps(z))\") #Step 2: SETUP METADATA cat << EOF > register.json { \"controller\": \"$CONTROLLER_ID\", \"name\": \"$HOSTNAME\", \"identifier\": \"$MACHINE_ID\", \"labels\": $SELECTOR_LABELS } EOF set +x #STEP 3: REGISTER HOST TO THE HOSTQUEUE. NEED TO EVALUATE HTTP STATUS 409 EXISTS, 201 created. ALL OTHERS FAIL. HTTP_RESPONSE=$(curl --write-out \"HTTPSTATUS:%{http_code}\" --retry 100 --retry-delay 10 --retry-max-time 1800 -X POST \\ -H \"X-Auth-Hostqueue-APIKey: $HOST_QUEUE_TOKEN\" \\ -H \"X-Auth-Hostqueue-Account: $ACCOUNT_ID\" \\ -H \"Content-Type: application/json\" \\ -d @register.json \\ \"${API_URL}v2/multishift/hostqueue/host/register\") set -x HTTP_BODY=$(echo \"$HTTP_RESPONSE\" | sed -E 's/HTTPSTATUS\\:[0-9]{3}$//') HTTP_STATUS=$(echo \"$HTTP_RESPONSE\" | tr -d '\\n' | sed -E 's/.*HTTPSTATUS:([0-9]{3})$/\\1/') echo \"$HTTP_BODY\" echo \"$HTTP_STATUS\" if [ \"$HTTP_STATUS\" -ne 201 ]; then echo \"Error [HTTP status: $HTTP_STATUS]\" exit 1 fi HOST_ID=$(echo \"$HTTP_BODY\" | python -c \"import sys, json; print(json.load(sys.stdin)['id'])\") #STEP 4: WAIT FOR MEMBERSHIP TO BE ASSIGNED while true do set +x ASSIGNMENT=$(curl --retry 100 --retry-delay 10 --retry-max-time 1800 -G -X GET \\ -H \"X-Auth-Hostqueue-APIKey: $HOST_QUEUE_TOKEN\" \\ -H \"X-Auth-Hostqueue-Account: $ACCOUNT_ID\" \\ -d controllerID=\"$CONTROLLER_ID\" \\ -d hostID=\"$HOST_ID\" \\ \"${API_URL}v2/multishift/hostqueue/host/getAssignment\") set -x isAssigned=$(echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['isAssigned'])\" | awk '{print tolower($0)}') if [[ \"$isAssigned\" == \"true\" ]] ; then break fi if [[ \"$isAssigned\" != \"false\" ]]; then echo \"unexpected value for assign retrying\" fi sleep 10 done #STEP 5: ASSIGNMENT HAS BEEN MADE. SAVE SCRIPT AND RUN echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['script'])\" > /usr/local/bin/ibm-host-agent.sh export HOST_ID ASSIGNMENT_ID=$(echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['id'])\") cat << EOF > /etc/satelliteflags/ibm-host-agent-vars export HOST_ID=${HOST_ID} export ASSIGNMENT_ID=${ASSIGNMENT_ID} EOF chmod 0600 /etc/satelliteflags/ibm-host-agent-vars chmod 0700 /usr/local/bin/ibm-host-agent.sh cat << EOF > /etc/systemd/system/ibm-host-agent.service [Unit] Description=IBM Host Agent Service After=network.target [Service] Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ExecStart=/usr/local/bin/ibm-host-agent.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF chmod 0644 /etc/systemd/system/ibm-host-agent.service systemctl daemon-reload systemctl start ibm-host-agent.service touch \"$HOST_ASSIGN_FLAG\" HERE chmod 0700 /usr/local/bin/ibm-host-attach.sh cat << 'EOF' >/etc/systemd/system/ibm-host-attach.service [Unit] Description=IBM Host Attach Service After=network.target [Service] Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ExecStart=/usr/local/bin/ibm-host-attach.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF chmod 0644 /etc/systemd/system/ibm-host-attach.service systemctl daemon-reload systemctl enable ibm-host-attach.service systemctl start ibm-host-attach.service ``` Click Next . You can see that the next step is to assign some hosts to your control plane. Since we haven't attached any hosts yet we will do this later. Click Next . This page tells you what will happen once you have attached three hosts and assigned them to the control plane. You can access this wizard at any time by clicking the Getting Started link on the left. If you do come back to it later you won't have to generate the script again if you already downloaded it. Click the Complete link to exit the wizard.","title":"Create Location"},{"location":"satellite/ibm-cloud/classic/create-location/#overview","text":"","title":"Overview"},{"location":"satellite/ibm-cloud/classic/create-location/#create-the-location","text":"Now that we have some hosts we can create a new Satellite location and attached them to it. In the IBM Cloud Portal, use the left navigation menu to go to the Satellite landing page. Click on Create a satellite location . Provide a name for your location, a description (optional) and choose Washington DC in the Managed from dropdown. Click on Create Location . You will be taken to the Getting Started page for your location. Click Next . Click on the Generate script link. Provide some Labels if you wish, then click Download script . An example of the content of the script can be viewed here: Click here to expand the script ```sh #!/usr/bin/env bash cat << 'HERE' >>/usr/local/bin/ibm-host-attach.sh #!/usr/bin/env bash set -ex mkdir -p /etc/satelliteflags HOST_ASSIGN_FLAG=\"/etc/satelliteflags/hostattachflag\" if [[ -f \"$HOST_ASSIGN_FLAG\" ]]; then echo \"host has already been assigned. need to reload before you try the attach again\" exit 0 fi set +x HOST_QUEUE_TOKEN=\"<a really long token will be here....>\" set -x ACCOUNT_ID=\"<your IBM Cloud Account ID will be here>\" CONTROLLER_ID=\"<your satellite location ID will be here>\" SELECTOR_LABELS='{\"location\":\"toronto\",\"provider\":\"ibm\"}' API_URL=\"https://origin.us-east.containers.cloud.ibm.com/\" #shutdown known blacklisted services for Satellite (these will break kube) set +e systemctl stop -f iptables.service systemctl disable iptables.service systemctl mask iptables.service systemctl stop -f firewalld.service systemctl disable firewalld.service systemctl mask firewalld.service set -e # ensure you can successfully communicate with redhat mirrors (this is a prereq to the rest of the automation working) yum install rh-python36 -y mkdir -p /etc/satellitemachineidgeneration if [[ ! -f /etc/satellitemachineidgeneration/machineidgenerated ]]; then rm -f /etc/machine-id systemd-machine-id-setup touch /etc/satellitemachineidgeneration/machineidgenerated fi #STEP 1: GATHER INFORMATION THAT WILL BE USED TO REGISTER THE HOST HOSTNAME=$(hostname -s) HOSTNAME=${HOSTNAME,,} MACHINE_ID=$(cat /etc/machine-id ) CPUS=$(nproc) MEMORY=$(grep MemTotal /proc/meminfo | awk '{print $2}') export CPUS export MEMORY SELECTOR_LABELS=$(echo \"${SELECTOR_LABELS}\" | python -c \"import sys, json, os; z = json.load(sys.stdin); y = {\\\"cpu\\\": os.getenv('CPUS'), \\\"memory\\\": os.getenv('MEMORY')}; z.update(y); print(json.dumps(z))\") #Step 2: SETUP METADATA cat << EOF > register.json { \"controller\": \"$CONTROLLER_ID\", \"name\": \"$HOSTNAME\", \"identifier\": \"$MACHINE_ID\", \"labels\": $SELECTOR_LABELS } EOF set +x #STEP 3: REGISTER HOST TO THE HOSTQUEUE. NEED TO EVALUATE HTTP STATUS 409 EXISTS, 201 created. ALL OTHERS FAIL. HTTP_RESPONSE=$(curl --write-out \"HTTPSTATUS:%{http_code}\" --retry 100 --retry-delay 10 --retry-max-time 1800 -X POST \\ -H \"X-Auth-Hostqueue-APIKey: $HOST_QUEUE_TOKEN\" \\ -H \"X-Auth-Hostqueue-Account: $ACCOUNT_ID\" \\ -H \"Content-Type: application/json\" \\ -d @register.json \\ \"${API_URL}v2/multishift/hostqueue/host/register\") set -x HTTP_BODY=$(echo \"$HTTP_RESPONSE\" | sed -E 's/HTTPSTATUS\\:[0-9]{3}$//') HTTP_STATUS=$(echo \"$HTTP_RESPONSE\" | tr -d '\\n' | sed -E 's/.*HTTPSTATUS:([0-9]{3})$/\\1/') echo \"$HTTP_BODY\" echo \"$HTTP_STATUS\" if [ \"$HTTP_STATUS\" -ne 201 ]; then echo \"Error [HTTP status: $HTTP_STATUS]\" exit 1 fi HOST_ID=$(echo \"$HTTP_BODY\" | python -c \"import sys, json; print(json.load(sys.stdin)['id'])\") #STEP 4: WAIT FOR MEMBERSHIP TO BE ASSIGNED while true do set +x ASSIGNMENT=$(curl --retry 100 --retry-delay 10 --retry-max-time 1800 -G -X GET \\ -H \"X-Auth-Hostqueue-APIKey: $HOST_QUEUE_TOKEN\" \\ -H \"X-Auth-Hostqueue-Account: $ACCOUNT_ID\" \\ -d controllerID=\"$CONTROLLER_ID\" \\ -d hostID=\"$HOST_ID\" \\ \"${API_URL}v2/multishift/hostqueue/host/getAssignment\") set -x isAssigned=$(echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['isAssigned'])\" | awk '{print tolower($0)}') if [[ \"$isAssigned\" == \"true\" ]] ; then break fi if [[ \"$isAssigned\" != \"false\" ]]; then echo \"unexpected value for assign retrying\" fi sleep 10 done #STEP 5: ASSIGNMENT HAS BEEN MADE. SAVE SCRIPT AND RUN echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['script'])\" > /usr/local/bin/ibm-host-agent.sh export HOST_ID ASSIGNMENT_ID=$(echo \"$ASSIGNMENT\" | python -c \"import sys, json; print(json.load(sys.stdin)['id'])\") cat << EOF > /etc/satelliteflags/ibm-host-agent-vars export HOST_ID=${HOST_ID} export ASSIGNMENT_ID=${ASSIGNMENT_ID} EOF chmod 0600 /etc/satelliteflags/ibm-host-agent-vars chmod 0700 /usr/local/bin/ibm-host-agent.sh cat << EOF > /etc/systemd/system/ibm-host-agent.service [Unit] Description=IBM Host Agent Service After=network.target [Service] Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ExecStart=/usr/local/bin/ibm-host-agent.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF chmod 0644 /etc/systemd/system/ibm-host-agent.service systemctl daemon-reload systemctl start ibm-host-agent.service touch \"$HOST_ASSIGN_FLAG\" HERE chmod 0700 /usr/local/bin/ibm-host-attach.sh cat << 'EOF' >/etc/systemd/system/ibm-host-attach.service [Unit] Description=IBM Host Attach Service After=network.target [Service] Environment=\"PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\" ExecStart=/usr/local/bin/ibm-host-attach.sh Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF chmod 0644 /etc/systemd/system/ibm-host-attach.service systemctl daemon-reload systemctl enable ibm-host-attach.service systemctl start ibm-host-attach.service ``` Click Next . You can see that the next step is to assign some hosts to your control plane. Since we haven't attached any hosts yet we will do this later. Click Next . This page tells you what will happen once you have attached three hosts and assigned them to the control plane. You can access this wizard at any time by clicking the Getting Started link on the left. If you do come back to it later you won't have to generate the script again if you already downloaded it. Click the Complete link to exit the wizard.","title":"Create the location"},{"location":"satellite/ibm-cloud/classic/prerequisites/","text":"Prerequisites Connectivity Requirements Connectivity requirements are documented here . This is a summary of the rules to be created in a security group that will be attached to each host. Inbound Rules Description Protocol Ports Source Required: Allow IBM Cloud to set up and manage your Satellite location TCP 30000-32767 Any Required: Allow access for all protocols and all ports to the security group All All Security Group Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network All 80 Any Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network All 443 Any Optional: Access hosts via SSH on the public network TCP 22 Any Outbound Rules Description Protocol Ports Source Required: Allow IBM Cloud to set up and manage your Satellite location All All Any Caution These are not the minimum requirements for outbound connectivity; it's just the easiest set of outbound rules for demos. The actual minimum requirements are here . Creating a security group In IBM Cloud, navigate to the Classic Infrastructure page using the left navigation menu. Expand the Security section on the menu. Choose Network Security -> Security Groups . You will see a screen similar to the one below. Click the Create Group link. Fill in the dialog box with the name and description of your choice, then click Create group . You will be taken back to the list of security groups where you will see your group in the list. Click the link for your group to edit it; we need to add some inbound security rules. Click the Create rule link at the top right of the Inbound Rules table. The first rule will be to grant access to ports 30000-32767 for the TCP protocol to all incoming source IP addresses. The CIDR notation for \"all IPv4 addresses\" is 0.0.0.0/0 . Change the port range to match the screen shot below, set the Source to 0.0.0.0/0 and click Ok. Repeat these steps for port 80, which will allow HTTP traffic. Set both port range fields to 80 . Repeat these steps for port 443, which will allow HTTPs traffic. Set both port range fields to 443 . Repeat these steps for port 22, which will allow you to SSh into hosts in this security group. Set both port range fields to 22 . We need one more inbound rule that will allow all traffic among nodes that belong to this security group. For this rule, change the Protocol dropdown to All . This will remove the ports from the dialog. Change the Source Type to Security Group and change the Source dropdown to your security group. Note that in the screen shot below, the name of my Security group at the top is ibm_satellite and that is what is also selected in the Source field. Click Ok . The final set of rules should look like this: That's it! We now have a security group that we can attach to the hosts we create to allow the proper inbound and outbound traffic to support IBM Cloud Satellite. Create an SSH Key You will need to SSH into your hosts to run some updates and a registration script to attach your hosts to your location. To do this you will need an SSH key that you will provide when you create your hosts. If you don't have one you will need to create one before you can import it into IBM Cloud. Check out this page in the Github Docs for more information on creating one or finding an existing one if it exists. To create an SSH Key in IBM Cloud Classic Infrastructure: In the IBM Cloud Portal use the icon to access the left navigation menu. Choose Classic Infrastructure . You should be on the Classic Infrastructure landing page. On the left navigation menu choose Devices -> Manage -> SSH Keys . Click the Add button to add a new SSH Key. Copy and paste your public key into the Public Key field. The public key can typically be found in the ~/.ssh directory on a mac. The file name will have a .pub extension, like id_rsa.pub . cat ~/.ssh/id_rsa.pub Copy everything from the ssh-rsa to the email address or whatever is at the end of that string of characters and paste it into the Public key field. Click Add . Now you are ready to create some hosts.","title":"Prerequisites"},{"location":"satellite/ibm-cloud/classic/prerequisites/#prerequisites","text":"","title":"Prerequisites"},{"location":"satellite/ibm-cloud/classic/prerequisites/#connectivity-requirements","text":"Connectivity requirements are documented here . This is a summary of the rules to be created in a security group that will be attached to each host.","title":"Connectivity Requirements"},{"location":"satellite/ibm-cloud/classic/prerequisites/#inbound-rules","text":"Description Protocol Ports Source Required: Allow IBM Cloud to set up and manage your Satellite location TCP 30000-32767 Any Required: Allow access for all protocols and all ports to the security group All All Security Group Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network All 80 Any Optional: Access the Red Hat OpenShift on IBM Cloud console on the public network All 443 Any Optional: Access hosts via SSH on the public network TCP 22 Any","title":"Inbound Rules"},{"location":"satellite/ibm-cloud/classic/prerequisites/#outbound-rules","text":"Description Protocol Ports Source Required: Allow IBM Cloud to set up and manage your Satellite location All All Any Caution These are not the minimum requirements for outbound connectivity; it's just the easiest set of outbound rules for demos. The actual minimum requirements are here .","title":"Outbound Rules"},{"location":"satellite/ibm-cloud/classic/prerequisites/#creating-a-security-group","text":"In IBM Cloud, navigate to the Classic Infrastructure page using the left navigation menu. Expand the Security section on the menu. Choose Network Security -> Security Groups . You will see a screen similar to the one below. Click the Create Group link. Fill in the dialog box with the name and description of your choice, then click Create group . You will be taken back to the list of security groups where you will see your group in the list. Click the link for your group to edit it; we need to add some inbound security rules. Click the Create rule link at the top right of the Inbound Rules table. The first rule will be to grant access to ports 30000-32767 for the TCP protocol to all incoming source IP addresses. The CIDR notation for \"all IPv4 addresses\" is 0.0.0.0/0 . Change the port range to match the screen shot below, set the Source to 0.0.0.0/0 and click Ok. Repeat these steps for port 80, which will allow HTTP traffic. Set both port range fields to 80 . Repeat these steps for port 443, which will allow HTTPs traffic. Set both port range fields to 443 . Repeat these steps for port 22, which will allow you to SSh into hosts in this security group. Set both port range fields to 22 . We need one more inbound rule that will allow all traffic among nodes that belong to this security group. For this rule, change the Protocol dropdown to All . This will remove the ports from the dialog. Change the Source Type to Security Group and change the Source dropdown to your security group. Note that in the screen shot below, the name of my Security group at the top is ibm_satellite and that is what is also selected in the Source field. Click Ok . The final set of rules should look like this: That's it! We now have a security group that we can attach to the hosts we create to allow the proper inbound and outbound traffic to support IBM Cloud Satellite.","title":"Creating a security group"},{"location":"satellite/ibm-cloud/classic/prerequisites/#create-an-ssh-key","text":"You will need to SSH into your hosts to run some updates and a registration script to attach your hosts to your location. To do this you will need an SSH key that you will provide when you create your hosts. If you don't have one you will need to create one before you can import it into IBM Cloud. Check out this page in the Github Docs for more information on creating one or finding an existing one if it exists. To create an SSH Key in IBM Cloud Classic Infrastructure: In the IBM Cloud Portal use the icon to access the left navigation menu. Choose Classic Infrastructure . You should be on the Classic Infrastructure landing page. On the left navigation menu choose Devices -> Manage -> SSH Keys . Click the Add button to add a new SSH Key. Copy and paste your public key into the Public Key field. The public key can typically be found in the ~/.ssh directory on a mac. The file name will have a .pub extension, like id_rsa.pub . cat ~/.ssh/id_rsa.pub Copy everything from the ssh-rsa to the email address or whatever is at the end of that string of characters and paste it into the Public key field. Click Add . Now you are ready to create some hosts.","title":"Create an SSH Key"},{"location":"satellite/ibm-cloud/ic4g/create-location/","text":"Overview This page will contain instructions for creating a location in IBM Cloud for Government.","title":"Overview"},{"location":"satellite/ibm-cloud/ic4g/create-location/#overview","text":"This page will contain instructions for creating a location in IBM Cloud for Government.","title":"Overview"},{"location":"satellite/ibm-cloud/vpc/create-location/","text":"Overview This page will contain instructions for creating a location in IBM Cloud VPC (Gen2). Note: in cases where cloud-init can be used when creating a virtual server instance you will want to wait to create hosts until after you create the location. To attach a host to a location you need to run a script that you generate when you create the location. You can run that script using cloud-init, or you can wait until after the host starts up to do it.","title":"Create Location"},{"location":"satellite/ibm-cloud/vpc/create-location/#overview","text":"This page will contain instructions for creating a location in IBM Cloud VPC (Gen2). Note: in cases where cloud-init can be used when creating a virtual server instance you will want to wait to create hosts until after you create the location. To attach a host to a location you need to run a script that you generate when you create the location. You can run that script using cloud-init, or you can wait until after the host starts up to do it.","title":"Overview"},{"location":"satellite/openshift/create-cluster/","text":"Overview This page will contain instructions for provisioning an OpenShift cluster in your location. Prerequisites In order to create an OpenShift cluster in an IBM Cloud Satellite location, you must first create an IBM Cloud Satellite location! If you do not yet have one you can create one using any of the options documented in the Create a satellite location section. You will also need some hosts to assign to your OpenShift cluster; preferrably at least 3 hosts in geographically dispersed locations for resiliency. If you have three hosts available you will see them on the Hosts page for your satellite location Warning If you do not have any unassigned hosts please create them before proceeding. You can create these hosts using the same steps you used to create the hosts for your satellite location control plane. Navigate to the OpenShift page in the IBM Cloud Console by clicking the icon, then Kubernetes -> Clusters . Fill in the details....... When ready, click the blue Create button to create your cluster. You will be taken to the Getting Started page for your cluster. This page has helpful tips for setting up your computer to access your cluster. You can come back to this page at any time. Click Overview on the left navigation menu to see the overview page for your cluster. You can see that the Master status fields shows Deploy in progress . At this time the master nodes for your cluster are being provisioned in the control plane for your satellite location. Once that is done the Master status field will change to Ready . Next you will need to go back to your satellite location and assign some hosts to act as worker nodes for your cluster. Navigate back to your satellite location and go to the Hosts tab. You will need to assign the three hosts that you provisioned (and that are currently Unassigned ) to your cluster, in the same way you assigned hosts to your control plane. Note When you assign hosts to a service like OpenShift on IBM Cloud, you don't specify a Managed from zone like you did for your control plane. The reason is that these hosts are managed completely by your control plane in your satellite location. Click the \"three dots\" icon at the right of the line for your host and select Assign host . Click the Cluster dropdown and select the openshift cluster you just created. Repeat the previous step for the other two hosts. Go back to your OpenShift cluster and click on the Worker Nodes link in the left navigation menu. You should now see that your cluster has three worker nodes. Go back to the Overview tab for your cluster and wait for it to show Normal in the cluster status field at the top of the page, just to the right of the cluster name. If it says Finalizing workers... it is not yet done. Note It is common for the worker nodes to report Critical status during provisioning. Do not be alarmed; this occurs when the nodes get rebooted, which is a normal part of the process. Usually this is a good sign that the provisioning process is almost complete. Note If your hosts are running in AWS then the DNS for the ingress subdomain will be incorrect, as was the DNS for your location. The reason is the same - the script that registers the hosts reports the private IP address instead of the public IP address. This must be fixed before you can access the OpenShift console. (Optional) If your cluster is running in AWS you will need to run the commands starting in step 4 in the DNS for cluster load balancing section of the documentation. Run this command to get the list of private IP addresses currently registered in your cluster's default ingress subdomain: ibmcloud oc nlb-dns ls -c <your_cluster_name For example: ibmcloud oc nlb-dns ls -c sbx-fairfax-dev-01 OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud 10.5.0.38 None created sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000 openshift-ingress run this command to register the public IP addresses instead: ibmcloud oc nlb-dns add --cluster sbx-fairfax-dev-01 --nlb-host sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud --ip <aws_public_ip> --ip <aws_public_ip> --ip <aws_public_ip> For example: ibmcloud oc nlb-dns add --cluster sbx-fairfax-dev-01 --nlb-host sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud --ip 3.82.241.35 --ip 54.242.214.78 --ip 3.81.82.210 Adding IP(s) 3.82.241.35, 54.242.214.78, 3.81.82.210 to NLB host name sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud in cluster sbx-fairfax-dev-01 ... OK Run this command again to review the new list of IP addresses: ibmcloud oc nlb-dns ls -c <your_cluster_name> For example: ibmcloud oc nlb-dns ls -c sbx-fairfax-dev-01 OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud 3.81.82.210,3.82.241.35,10.5.0.38,54.242.214.78 None created sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000 openshift-ingress You will notice that the three new IP addresses are there, but the original private IP address is still there as well. We will need to remove the private IP address. Run this command to remove all of the private IP addresses that were originally registered: ibmcloud oc nlb-dns rm classic --cluster sbx-fairfax-dev-01 --nlb-host <your_hostname> --ip <aws_private_ip> --ip <aws_private_ip> --ip <aws_private_ip> Note You may have more or fewer private IP addresses than in the sample command. Include the appropriate number of --ip <aws_private_ip> paramters to match the number of private IP addresses you retrieved in the command above. For example: ibmcloud oc nlb-dns rm classic --cluster sbx-fairfax-dev-01 --nlb-host sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud --ip 10.5.0.38 Deleting IP 10.5.0.38 from NLB host name sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud in cluster sbx-fairfax-dev-01... OK Try to access the OpenShift console. Go back to the Overview page for your OpenShift cluster and click the blue OpenShift web console link at the top right of the page. Question At this point the console functions, but the ingress operator shows Degraded status. Is that because there is no load balancer? In IBM Cloud the service automatically creates a load balancer for the ingress, but in satellite it does not. Should there be steps here to create one? What would the steps be? Congratulations! You have successfully deployed an OpenShift cluster in your IBM Cloud Satellite location!!","title":"Create an OpenShift Cluster"},{"location":"satellite/openshift/create-cluster/#overview","text":"This page will contain instructions for provisioning an OpenShift cluster in your location.","title":"Overview"},{"location":"satellite/openshift/create-cluster/#prerequisites","text":"In order to create an OpenShift cluster in an IBM Cloud Satellite location, you must first create an IBM Cloud Satellite location! If you do not yet have one you can create one using any of the options documented in the Create a satellite location section. You will also need some hosts to assign to your OpenShift cluster; preferrably at least 3 hosts in geographically dispersed locations for resiliency. If you have three hosts available you will see them on the Hosts page for your satellite location Warning If you do not have any unassigned hosts please create them before proceeding. You can create these hosts using the same steps you used to create the hosts for your satellite location control plane. Navigate to the OpenShift page in the IBM Cloud Console by clicking the icon, then Kubernetes -> Clusters . Fill in the details....... When ready, click the blue Create button to create your cluster. You will be taken to the Getting Started page for your cluster. This page has helpful tips for setting up your computer to access your cluster. You can come back to this page at any time. Click Overview on the left navigation menu to see the overview page for your cluster. You can see that the Master status fields shows Deploy in progress . At this time the master nodes for your cluster are being provisioned in the control plane for your satellite location. Once that is done the Master status field will change to Ready . Next you will need to go back to your satellite location and assign some hosts to act as worker nodes for your cluster. Navigate back to your satellite location and go to the Hosts tab. You will need to assign the three hosts that you provisioned (and that are currently Unassigned ) to your cluster, in the same way you assigned hosts to your control plane. Note When you assign hosts to a service like OpenShift on IBM Cloud, you don't specify a Managed from zone like you did for your control plane. The reason is that these hosts are managed completely by your control plane in your satellite location. Click the \"three dots\" icon at the right of the line for your host and select Assign host . Click the Cluster dropdown and select the openshift cluster you just created. Repeat the previous step for the other two hosts. Go back to your OpenShift cluster and click on the Worker Nodes link in the left navigation menu. You should now see that your cluster has three worker nodes. Go back to the Overview tab for your cluster and wait for it to show Normal in the cluster status field at the top of the page, just to the right of the cluster name. If it says Finalizing workers... it is not yet done. Note It is common for the worker nodes to report Critical status during provisioning. Do not be alarmed; this occurs when the nodes get rebooted, which is a normal part of the process. Usually this is a good sign that the provisioning process is almost complete. Note If your hosts are running in AWS then the DNS for the ingress subdomain will be incorrect, as was the DNS for your location. The reason is the same - the script that registers the hosts reports the private IP address instead of the public IP address. This must be fixed before you can access the OpenShift console. (Optional) If your cluster is running in AWS you will need to run the commands starting in step 4 in the DNS for cluster load balancing section of the documentation. Run this command to get the list of private IP addresses currently registered in your cluster's default ingress subdomain: ibmcloud oc nlb-dns ls -c <your_cluster_name For example: ibmcloud oc nlb-dns ls -c sbx-fairfax-dev-01 OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud 10.5.0.38 None created sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000 openshift-ingress run this command to register the public IP addresses instead: ibmcloud oc nlb-dns add --cluster sbx-fairfax-dev-01 --nlb-host sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud --ip <aws_public_ip> --ip <aws_public_ip> --ip <aws_public_ip> For example: ibmcloud oc nlb-dns add --cluster sbx-fairfax-dev-01 --nlb-host sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud --ip 3.82.241.35 --ip 54.242.214.78 --ip 3.81.82.210 Adding IP(s) 3.82.241.35, 54.242.214.78, 3.81.82.210 to NLB host name sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud in cluster sbx-fairfax-dev-01 ... OK Run this command again to review the new list of IP addresses: ibmcloud oc nlb-dns ls -c <your_cluster_name> For example: ibmcloud oc nlb-dns ls -c sbx-fairfax-dev-01 OK Hostname IP(s) Health Monitor SSL Cert Status SSL Cert Secret Name Secret Namespace sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud 3.81.82.210,3.82.241.35,10.5.0.38,54.242.214.78 None created sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000 openshift-ingress You will notice that the three new IP addresses are there, but the original private IP address is still there as well. We will need to remove the private IP address. Run this command to remove all of the private IP addresses that were originally registered: ibmcloud oc nlb-dns rm classic --cluster sbx-fairfax-dev-01 --nlb-host <your_hostname> --ip <aws_private_ip> --ip <aws_private_ip> --ip <aws_private_ip> Note You may have more or fewer private IP addresses than in the sample command. Include the appropriate number of --ip <aws_private_ip> paramters to match the number of private IP addresses you retrieved in the command above. For example: ibmcloud oc nlb-dns rm classic --cluster sbx-fairfax-dev-01 --nlb-host sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud --ip 10.5.0.38 Deleting IP 10.5.0.38 from NLB host name sbx-fairfax-dev-01-e648741016b5b16f9b585588dcd0ed80-0000.upi.containers.appdomain.cloud in cluster sbx-fairfax-dev-01... OK Try to access the OpenShift console. Go back to the Overview page for your OpenShift cluster and click the blue OpenShift web console link at the top right of the page. Question At this point the console functions, but the ingress operator shows Degraded status. Is that because there is no load balancer? In IBM Cloud the service automatically creates a load balancer for the ingress, but in satellite it does not. Should there be steps here to create one? What would the steps be? Congratulations! You have successfully deployed an OpenShift cluster in your IBM Cloud Satellite location!!","title":"Prerequisites"},{"location":"satellite/terraform/aws/","text":"ibm-satellite-location","title":"ibm-satellite-location"},{"location":"satellite/terraform/aws/#ibm-satellite-location","text":"","title":"ibm-satellite-location"},{"location":"tools/scripts/ibmcos/","text":"#!/bin/sh #set -x #----------------------------------------------------------------------------------------------------- # Script: ibmcos # Version: 1.0.0 #----------------------------------------------------------------------------------------------------- # IBM Cloud Object Storage - Utility functions #----------------------------------------------------------------------------------------------------- # Copyright (c) 2020, International Business Machines. All Rights Reserved. #----------------------------------------------------------------------------------------------------- cos_command=\"$1\" service_name=\"$2\" bucket_name=\"$3\" location_constraint=\"$4\" key_crn=\"$5\" region=\"us\" # Note: Region should be set based on the location constraint, e.g. \"us\" for us-standard or # \"us-south\" for us-south-standard activity_tracker_crn=\"crn:v1:bluemix:public:logdnaat:us-south:a/9d5d52********************8a:55d90d9c-****-****-****-********785::\" #----------------------------------------------------------------------------------------------------- # Display usage information for the commands in this script #----------------------------------------------------------------------------------------------------- display_usage() { echo echo \"NAME:\" echo \"ibmcos.sh - Manage features of IBM Cloud Object Storage\" echo echo \"USAGE:\" echo \"ibmcos.sh command [service instance name] [options]\" echo echo \"COMMANDS:\" echo \"-------------------------------------------------------------------------------------------\" echo echo \"view-buckets View the buckets in the specified COS instance in XML format\" echo \"create-bucket Create a new bucket with predefined configuration\" echo \"delete-bucket Delete a bucket from the specified COS instance\" echo \"disable-bucket-firewall Remove the COS Firewall configuration from a bucket\" echo \"view-bucket-config Get the headers and configuration for the specified bucket\" echo \"view-objects List the objects in the specified bucket\" echo \"location-constraints Returns a list of valid location constraints for creating buckets\" echo \"view-key-protect Returns a list of Key Protect instances\" echo \"view-keys-list Returns a list of keys in the specified Key Protect instance\" echo \"help, h View help for this script\" echo echo echo \"Note: For your convenience this command executes the ibmcloud cli to look up certain\" echo \" information needed to perform these tasks. It requires you to be logged into\" echo \" the ibmcloud cli before you run this command.\" echo echo } #----------------------------------------------------------------------------------------------------- # Display the list of valid location constraints for creating buckets #----------------------------------------------------------------------------------------------------- display_location_constraints() { echo echo \"LOCATION CONSTRAINTS:\" echo \"-------------------------------------------------------------------------------------------\" echo echo \"us-south-standard Regional Bucket in US South with storage class STANDARD\" echo \"us-east-standard Regional Bucket in US East with storage class STANDARD\" echo \"us-standard Cross regional Bucket in US Geo with storage class STANDARD\" echo \"us-south-flex Regional Bucket in US South with storage class FLEX\" echo \"us-east-flex Regional Bucket in US East with storage class FLEX\" echo \"us-flex Cross regional Bucket in US Geo with storage class FLEX\" echo \"us-south-cold Regional Bucket in US South with storage class COLD\" echo \"us-east-cold Regional Bucket in US East with storage class COLD\" echo \"us-cold Cross regional Bucket in US Geo with storage class COLD\" echo \"us-south-vault Regional Bucket in US South with storage class VAULT\" echo \"us-east-vault Regional Bucket in US East with storage class VAULT\" echo \"us-vault Cross regional Bucket in US Geo with storage class VAULT\" echo echo } #----------------------------------------------------------------------------------------------------- # Display the headers and configuration of a bucket #----------------------------------------------------------------------------------------------------- view_bucket_configuration() { echo echo \"Bucket Headers:\" echo curl -s -I \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' echo echo \"Bucket Configuration:\" echo curl -s \"https://config.cloud-object-storage.cloud.ibm.com/v1/b/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' | jq --color-output echo } #----------------------------------------------------------------------------------------------------- # Validate the location constraint specified on a create-bucket request #----------------------------------------------------------------------------------------------------- validate_location_constraint() { constraint=\"$1\" #echo \"The location constraint passed to the function is ${constraint}\" locationCount=$(echo $constraints | jq '.locations | length') locationValid=false #echo \"Value of locationValid is ${locationValid}\" case $constraint in us-standard) locationValid=true;; us-south-standard) locationValid=true;; us-south-flex) locationValid=true;; us-south-cold) locationValid=true;; us-south-vault) locationValid=true;; us-east-standard) locationValid=true;; us-east-flex) locationValid=true;; us-east-cold) locationValid=true;; us-east-vault) locationValid=true;; us-standard) locationValue=true;; us-flex) locationValue=true;; us-cold) locationValue=true;; esac #echo \"Value of locationValid is ${locationValid}\" if [[ $locationValid = false ]]; then echo echo \"ERROR: ${constraint} is not a valid location constraint.\" echo display_location_constraints exit 1 fi } #----------------------------------------------------------------------------------------------------- # Check base input parameters #----------------------------------------------------------------------------------------------------- if [[ $# -lt 1 ]]; then display_usage exit 1 fi # Only need to do these commands if a service name is present if [[ $# -ge 2 ]]; then # Get the GUID for the specified service instance service_instance_guid=$(ic resource service-instance $service_name --output JSON | jq -r '.[0].guid') # Get the bearer token to use for authentication. # Note: This assumes that the user has previously logged into the IBM Cloud CLI ibm_auth_token=$(ic iam oauth-tokens | awk '{ print $4}') fi #----------------------------------------------------------------------------------------------------- # View Help #----------------------------------------------------------------------------------------------------- if [ $cos_command == \"help\" ] || [ $cos_command == \"h\" ]; then echo display_usage exit 0 fi #----------------------------------------------------------------------------------------------------- # List all the buckets in the specified COS instance in XML format #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-buckets\" ]]; then echo echo \"Listing buckets for service ${service_name}...\" echo curl -s \"https://s3.${region}.cloud-object-storage.appdomain.cloud/?extended\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' | xmllint --format - echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View all the objects in the specified COS instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-objects\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for view-objects a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-objects [service instance name] [bucket-name]\" echo echo exit 1 fi echo echo \"viewing objects for bucket ${bucket_name}...\" echo curl -s \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}?List-type=2\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' | xmllint --format - echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View all the objects in the specified COS instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"get-object\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for get-object a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-objects [service instance name] [bucket-name] [object-name]\" echo echo exit 1 fi if [[ $# -lt 4 ]]; then echo echo \"for get-object an object name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-objects [service instance name] [bucket-name] [object-name]\" echo echo exit 1 fi object_name=\"$4\" echo echo \"getting object ${object_name}...\" echo curl -s \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}/${object_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' > ${object_name}.pdf echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # Create a bucket in the specified COS instance with Key Protect, Activity Tracker # and IP Firewall enabled #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"create-bucket\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for create-bucket a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh create-bucket [service instance name] [bucket-name] [location-constraint] [key-crn]\" echo echo exit 1 fi if [[ $# -lt 4 ]]; then echo echo \"for create-bucket a location constraint is required\" echo echo \"USAGE:\" echo \"ibmcos.sh create-bucket [service instance name] [bucket-name] [location-constraint] [key-crn]\" display_location_constraints echo exit 1 fi if [[ $# -lt 5 ]]; then echo echo \"for create-bucket a Key Protect Key is required\" echo echo \"USAGE:\" echo \"ibmcos.sh create-bucket [service instance name] [bucket-name] [location-constraint] [key-crn]\" echo exit 1 fi # Validate the specified bucket constraint validate_location_constraint $location_constraint if [[ $location_constraint == \"us-standard\" ]]; then region=\"us\" fi echo \"The updated region is ${region}\" echo echo \"Creating bucket ${bucket_name} with location constraint ${location_constraint}...\" echo curl -s -X PUT \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' \\ -H 'ibm-sse-kp-encryption-algorithm: AES256' \\ -H 'ibm-sse-kp-customer-root-key-crn: '\"$key_crn\"'' \\ -d '<CreateBucketConfiguration><LocationConstraint>'\"${location_constraint}\"'</LocationConstraint></CreateBucketConfiguration>' # Next we need to update the config for Activity Tracker and COS Firewall # Note: A CIDR block of 0.0.0.0/0 allows all IP Addresses # Note: To remove the COS Firewall pass an empty array: {\"allowed_ip\": []} curl -s -X PATCH \"https://config.cloud-object-storage.cloud.ibm.com/v1/b/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -d '{ \"activity_tracking\": { \"read_data_events\": false, \"write_data_events\": true, \"activity_tracker_crn\": \"'\"$activity_tracker_crn\"'\" }, \"firewall\": { \"allowed_ip\": [\"0.0.0.0/0\"] } }' | jq --color-output echo echo \"Bucket Created, retrieving bucket headers and configuration...\" echo view_bucket_configuration echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # Remove COS Firewall # Note: This will probably only be used if the firewall config is invalid #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"disable-bucket-firewall\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for disable-bucket-firewall a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh disable-bucket-firewall [service instance name] [bucket-name]\" echo echo exit 1 fi echo echo \"Disabling COS Firewall for bucket ${bucket_name}...\" echo curl -s -X PATCH \"https://config.cloud-object-storage.cloud.ibm.com/v1/b/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -d '{ \"firewall\": {\"allowed_ip\": []} }' | jq --color-output echo echo \"Bucket firewall disabled...\" echo view_bucket_configuration echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # Delete a bucket #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"delete-bucket\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for delete-bucket a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh delete-bucket [service instance name] [bucket-name]\" echo echo exit 1 fi echo echo \"Deleting bucket ${bucket_name}...\" echo curl -s -X DELETE \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' echo echo \"Request complete.\" echo echo exit 0 fi if [[ $cos_command == \"location-constraints\" ]]; then display_location_constraints exit 0 fi #----------------------------------------------------------------------------------------------------- # Get Bucket Config #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-bucket-config\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for view-bucket-config a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-bucket-config [service instance name] [bucket-name]\" echo echo exit 1 fi view_bucket_configuration echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View a list of keys in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-keys\" ]]; then echo echo \"Listing keys for service ${service_name}...\" echo # Get the region for the Key Protect Instance location=$(ic resource service-instance $service_name --output json | jq -r '.[0].region_id') curl -s \"https://${location}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq --color-output echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View a list of keys in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-keys-list\" ]]; then echo echo \"Listing keys for service ${service_name}...\" echo # Get the region for the Key Protect Instance location=$(ic resource service-instance $service_name --output json | jq -r '.[0].region_id') curl -s \"https://${location}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq -r '[.resources | .[] | { name: .name, id: .id, crn: .crn} ] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View a list of keys in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-key-protect\" ]]; then echo echo \"Listing instance of Key Protect...\" echo ic resource service-instances --output JSON | jq -r '[ .[] | select(.sub_type != null) | select(.sub_type | contains(\"kms\"))] | [ .[] | {name: .name, type: .type, sub_type: .sub_type }] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" echo echo exit 0 fi echo echo \"Error: Invalid command\" echo display_usage","title":"Ibmcos"},{"location":"tools/scripts/keyprotect/","text":"#!/bin/sh #set -x #----------------------------------------------------------------------------------------------------- # Script: keyprotect # Version: v1.0.2 #----------------------------------------------------------------------------------------------------- # IBM Key Protect - Utility functions #----------------------------------------------------------------------------------------------------- # Copyright (c) 2020, International Business Machines. All Rights Reserved. #----------------------------------------------------------------------------------------------------- display_usage() { echo echo \"NAME:\" echo \"keyprotect.sh - Manage features of Key Protect\" echo echo \"USAGE:\" echo \"keyprotect.sh <key-protect-instance-name> command [options]\" echo echo \"COMMANDS:\" echo \"-------------------------------------------------------------------------------------------\" echo echo \"view-policies List the current policies for the Key Protect Instance\" echo \"enable-dual-auth Enable the Dual Authorization policy for key deletes for all keys\" echo \"disable-dual-auth Disable the Dual Authorization policy for key deletes for all keys\" echo \"disable-public-endpoint Disable the public endpoint for the Key Protect Instance\" echo \"enable-public-endpoint Enable the public endpoint for the Key Protect Instance\" echo \"view-keys List the keys in the Key Protect Instance in JSON format\" echo \"view-keys-list List the keys in the Key Protect Instance in list format\" echo \"view-deleted-keys List the deleted keys in the Key Protect Instance in JSON format\" echo \"view-deleted-keys-list List the deleted keys in the Key Protect Instance in list format\" echo \"view-key View the details of a key\" echo \"view-key-material View the material for a standard key\" echo \"view-key-policies View the current polices for the specified key\" echo \"import-key Import a standard or root key\" echo \"restore-key Restore an imported key that has been deleted\" echo \"set-key-deletion Set the specified key for deletion (first auth)\" echo \"unset-key-deletion Unset the specified key for deletion, which removes the first auth\" echo \"help, h View help for this script\" echo echo echo \"Note: For your convenience this command executes the ibmcloud cli to look up certain\" echo \" information needed to perform these tasks. It requires you to be logged into\" echo \" the ibmcloud cli before you run this command.\" echo echo } view_policies() { echo echo \"Checking current policies for service ${service_name}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' | jq --color-output } #----------------------------------------------------------------------------------------------------- # View policies for a key # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- view_key_policies() { if [[ $# -lt 1 ]]; then echo echo \"for view-key-policies a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh <key-protect-instance-name] view-key-policies [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"Checking current policies for key ${keyId} in service ${service_name}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' | jq --color-output } #----------------------------------------------------------------------------------------------------- # View keys in a Key Protect instance in JSON format # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_keys() { echo echo \"Listing keys for service ${service_name}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View keys in a Key Protect instance in JSON format # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_deleted_keys() { echo echo \"Listing keys for service ${service_name}...\" echo echo \"Values for State field:\" echo \"-------------------------\" echo \"0 Pre-activation\" echo \"1 Active\" echo \"2 Suspended\" echo \"3 Deactivated\" echo \"5 Destroyed\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys?state=5\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View a list of keys in a Key Protect instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_deleted_keys_list() { echo echo \"Listing deleted keys for service ${service_name}...\" echo echo \"Values for State column:\" echo \"-------------------------\" echo \"0 Pre-activation\" echo \"1 Active\" echo \"2 Suspended\" echo \"3 Deactivated\" echo \"5 Destroyed\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys?state=5\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq -r '[.resources | .[] | { name: .name, id: .id, state: .state, crn: .crn} ] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View a list of keys in a Key Protect instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_keys_list() { echo echo \"Listing keys for service ${service_name}...\" echo echo \"Values for State column:\" echo \"-------------------------\" echo \"0 Pre-activation\" echo \"1 Active\" echo \"2 Suspended\" echo \"3 Deactivated\" echo \"5 Destroyed\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq -r '[.resources | .[] | { name: .name, id: .id, state: .state, crn: .crn} ] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Import a key # # Arguments: key-type, key-name, key-payload # #----------------------------------------------------------------------------------------------------- import_key() { display_usage() { echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] import-key [key-type] [key-name] [key-payload]\" echo echo \"Key Type\" echo \"-------------------------------------------------------------------------------------------\" echo \"standard Creates a standard key. Material from a standard key can be exported\" echo \"root Creates a root key. material from a root key can never be exported\" echo echo \"Note: Key material must be base64 encoded. For a root key, the base64 decoded value must be\" echo \" 128, 192 or 256 bits. This is 16 (128 bit), 24 (192 bit) or 32 (256 bit) Hex value\" echo echo \" This command can create a 256 bit base64 encoded key for demo purposes:\" echo echo \" openssl rand -base64 32\" echo exit 1 } if [[ $# -lt 3 ]]; then # Key type is missing if [[ $# -lt 1 ]]; then echo echo \"FAILED: a key type is required\" # there is a problem so display usage display_usage fi # Key name is missing if [[ $# -lt 2 ]]; then echo echo \"FAILED: a key name is required\" # there is a problem so display usage display_usage fi # Key payload is missing if [[ $# -lt 3 ]]; then echo echo \"FAILED: a key payload is required\" # there is a problem so display usage display_usage fi # there is a problem so display usage display_usage fi # Get parameter values keyType=\"$1\" keyName=\"$2\" keyPayload=\"$3\" # Validate the key type if [[ $keyType == \"standard\" || $keyType == \"root\" ]]; then echo else echo echo \"FAILED: key type: $keyType is invalid\" display_usage exit 1 fi # if [[ $keyType == \"root\" ]]; then isExtractable=false else isExtractable=true fi echo \"The value of isExtractable is ${isExtractable}\" echo echo \"Importing key ...\" echo #set -x curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'content-type: application/vnd.ibm.kms.key+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"'\"$keyName\"'\", \"extractable\": '\"$isExtractable\"', \"payload\": \"'\"$keyPayload\"'\" } ] }' | jq --color-output echo echo \"Done.\" echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Restore a key # # Arguments: key-id, key-payload # #----------------------------------------------------------------------------------------------------- restore_key() { display_usage() { echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] restore-key [key-id] [key-payload]\" echo echo \"Note: The key-id can be found using the view-keys command. You can only restore a key that\" echo \" previously existed. Deleted keys have a State of Destroyed. You must wait 30 seconds\" echo \" after deleting a key before it can be restored.\" echo exit 1 } if [[ $# -lt 2 ]]; then # Key id is missing if [[ $# -lt 1 ]]; then echo echo \"FAILED: a key id is required\" # there is a problem so display usage display_usage fi # Key payload is missing if [[ $# -lt 2 ]]; then echo echo \"FAILED: a key payload is required\" # there is a problem so display usage display_usage fi # there is a problem so display usage display_usage fi # Get parameter values keyId=\"$1\" keyPayload=\"$2\" echo echo \"Restoring key ...\" echo set -x curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}?action=restore\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"payload\": \"'\"$keyPayload\"'\" } ] }' | jq --color-output echo echo \"Done.\" echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Enable Dual Authorization for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- enable_dual_auth() { echo echo \"Enabling Dual Authorization for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # disable Dual Authorization for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- disable_dual_auth() { echo echo \"disabling Dual Authorization for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": false } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Set Allowed Networks policy for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- disable_public_endpoint() { echo echo \"Disabling public endpoint for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"allowedNetwork\", \"policy_data\": { \"enabled\": true, \"attributes\": { \"allowed_network\": \"private-only\" } } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Set Allowed Networks policy for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- enable_public_endpoint() { echo echo \"enabling public endpoint for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"allowedNetwork\", \"policy_data\": { \"enabled\": true, \"attributes\": { \"allowed_network\": \"public-and-private\" } } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View the details for a key # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- view_key() { if [[ $# -lt 1 ]]; then echo echo \"for view-key a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh <key-protect-instance-name] view-key [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"viewing attributes for key ${keyId}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View the material for a key <-- I think this is redundant with view_key # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- view_key_material() { if [[ $# -lt 1 ]]; then echo echo \"for view-key-material a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] view-key-material [key-id]\" echo echo exit 1 fi keyId=$1 curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${key_id}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq -r ' .resources[] | .payload ' exit 0 } #----------------------------------------------------------------------------------------------------- # Set a key for deletion - the first step in Dual Deletion # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- set_key_deletion() { if [[ $# -lt 1 ]]; then echo echo \"for set-key-deletion a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] set-key-deletion [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"Setting key ${keyId} in service ${service_name} for deletion...\" echo curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}?action=setKeyForDeletion\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key_action+json' \\ -H 'content-type: application/vnd.ibm.kms.key_action+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true } } ] }' | jq --color-output echo echo \"Done.\" echo echo \"viewing policies for key ${keyId}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Unet a key for deletion - Use this function to \"undo\" a \"set key for deletion\" # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- unset_key_deletion() { if [[ $# -lt 1 ]]; then echo echo \"for set-key-deletion a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] set-key-deletion [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"Unsetting key ${keyId} in service ${service_name} for deletion...\" echo curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}?action=unsetKeyForDeletion\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key_action+json' \\ -H 'content-type: application/vnd.ibm.kms.key_action+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true } } ] }' | jq --color-output echo echo \"Done.\" echo echo \"viewing policies for key ${keyId}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Main script execution starts here #----------------------------------------------------------------------------------------------------- service_name=\"$1\" kp_command=\"$2\" key_id=\"$3\" # Note: original way was to always have key-id as 3rd arg. New way is to move command logic to functions and # pass in argument 3 and higher in raw form directly to the function and let it decide what they mean. # For now, key_id is still set to $3 so that logic outside of functions still works. # Always need to arguments: service-instance-name and a command if [[ $# -lt 2 ]]; then display_usage exit 1 fi # Only need to do these commands if a service name is present if [[ $# -ge 1 ]]; then # Get the service instance details for $service_name service_instance_details=$(ibmcloud resource service-instance $service_name --output JSON) # Get the GUID for the specified Key Protect instance service_instance_guid=$(echo $service_instance_details| jq -r '.[0].guid') # service_instance_guid=$(ibmcloud resource service-instance $service_name --output JSON | jq -r '.[0].guid') # Get the region for the Key Protect Instance # region=$(echo $service_instance_details | jq -r '.[0].region_id') # Note: the line below sets the API endpoint to the private network. # To use the public network comment the line below and uncomment the one above. region=private.$(echo $service_instance_details | jq -r '.[0].region_id') # Get the bearer token to use for authentication. # Note: This assumes that the user has previously logged into the IBM Cloud CLI ibm_auth_token=$(ibmcloud iam oauth-tokens | awk '{ print $4}') fi #----------------------------------------------------------------------------------------------------- # View the policies for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [ $kp_command == \"help\" ] || [ $kp_command == \"h\" ]; then display_usage exit 0 fi #----------------------------------------------------------------------------------------------------- # View the policies for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-policies\" ]]; then view_policies exit 0 fi #----------------------------------------------------------------------------------------------------- # View the policies for a key in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-key-policies\" ]]; then view_key_policies $3 fi #----------------------------------------------------------------------------------------------------- # View the keys for the specified Key Protect instance in JSON format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-keys\" ]]; then view_keys exit 0 fi #----------------------------------------------------------------------------------------------------- # View the keys for the specified Key Protect instance in JSON format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-deleted-keys\" ]]; then view_deleted_keys exit 0 fi #----------------------------------------------------------------------------------------------------- # View the deleted keys for the specified Key Protect instance in List format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-deleted-keys-list\" ]]; then view_deleted_keys_list exit 0 fi #----------------------------------------------------------------------------------------------------- # View the keys for the specified Key Protect instance in List format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-keys-list\" ]]; then view_keys_list exit 0 fi #----------------------------------------------------------------------------------------------------- # View the attributes for a key in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-key\" ]]; then view_key $3 fi #----------------------------------------------------------------------------------------------------- # Import a key into the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"import-key\" ]]; then import_key $3 $4 $5 exit 0 fi #----------------------------------------------------------------------------------------------------- # Import a key into the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"restore-key\" ]]; then restore_key $3 $4 exit 0 fi #----------------------------------------------------------------------------------------------------- # Enable dual authorization delete policy for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"enable-dual-auth\" ]]; then enable_dual_auth fi #----------------------------------------------------------------------------------------------------- # Disable dual authorization delete policy for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"disable-dual-auth\" ]]; then disable_dual_auth fi #----------------------------------------------------------------------------------------------------- # Disable public network for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"disable-public-endpoint\" ]]; then disable_public_endpoint fi #----------------------------------------------------------------------------------------------------- # enable public network for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"enable-public-endpoint\" ]]; then enable_public_endpoint fi #----------------------------------------------------------------------------------------------------- # View the attributes for a key in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-key-material\" ]]; then view_key_material $3 fi #----------------------------------------------------------------------------------------------------- # Set the specified key for deletion (first auth) #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"set-key-deletion\" ]]; then set_key_deletion $3 fi #----------------------------------------------------------------------------------------------------- # Unset the deletion (first auth) of the specified Key #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"unset-key-deletion\" ]]; then unset_key_deletion $3 fi #----------------------------------------------------------------------------------------------------- echo echo \"Error: Invalid command\" echo display_usage","title":"Keyprotect"}]}