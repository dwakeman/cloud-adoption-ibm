{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview This site serves as documentation for work being done for the adoption of IBM Cloud.","title":"Home"},{"location":"#overview","text":"This site serves as documentation for work being done for the adoption of IBM Cloud.","title":"Overview"},{"location":"architecture/","text":"Architecture","title":"Architecture"},{"location":"architecture/#architecture","text":"","title":"Architecture"},{"location":"cloud-platform/ibm-cloud-operator/","text":"IBM Cloud Operator This operator is used to provision services in IBM Cloud and to create bindings that applications need to consume these services. IBM Cloud Operator Instructions will be provided here when available. Installing the IBM Cloud Operator The IBM Cloud Operator should be available by default in the OperatorHub console in OpenShift. It can be installed using the OperatorHub Console in OpenShift, or be using the OpenShift CLI. Install using the UI Install using the OpenShift CLI Documentation on installing operators in OpenShift can be found here . In the case of the IBM Cloud Operator, it is a matter of creating a yaml file and using the oc cli. On that page it discusses two objects that are needed - the OperatorGroup object and the Subscription object. If you intend to make the operator available in all projects you do not need to create a new OperatorGroup object; one already exists. You only need to create one if you want to install the operator into a single project. Here is sample yaml for creating a Subscription object: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibmcloud-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: ibmcloud-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: ibmcloud-operator.v0.1.7 Note: If you specify the openshift-operators namespace the operator will be installed in all namespaces in the cluster. This is the default behavior; if you specify any other namespace the operator will only be installed in that namespace. More documentation can be found here . Create a yaml file with the content above. Make sure to change the startingCSV value to the correct version of the IBM Cloud Operator as it appears in the OperatorHub console. Login to the oc cli. You can copy the command from the OpenShift Web Console: 1. Click the \"Display Token\" link. Select the command in the \"Log in with this token\" section: Paste the command in a terminal and run it. ``` oc login --token=5y7lF * * ****BOm2cSE --server=https://c106-e.us-south.containers.cloud.ibm.com:31777 Logged into \"https://c106-e.us-south.containers.cloud.ibm.com:31777\" as \"IAM#dwakeman@us.ibm.com\" using the token provided. You have access to 55 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". 1. Run this command: `oc apply -f <yaml_file_name>` oc apply -f ibmcloud-operator-subscription.yaml subscription.operators.coreos.com/ibmcloud-operator created ``` You should see a message that the subscription was created. It should now show up in the OpenShift console in the \"Installed Operators\" page. Congratulations, the IBM Cloud Operator has been installed!! Configuring the IBM Cloud Operator There are a few things that need to be configured in order for the IBM Cloud Operator to work. First, it needs an API key to use to authenticate with IBM Cloud. That user must have sufficient permissions to create services and bindings. More on that in a bit. That API Key gets loaded into a secret named seed-secret . The operator will first look for a secret by that name in the current project, and use it if found. If not, it will look in the default project, and if it finds one there it will use it. This behavior will allow Kaiser Permanente to provide each team with a separate seed-secret that uses an API key for a service ID that only has permission to create certain services, and only in that team's resource group. More discussion is needed to finalize the design for configuration of the operator for KP. If there is no seed-secret in the default project then there must be one in the current project in order for it to work. Each team can be given their own Service ID with permission to only create specific services, and only in the team's resource group. An API Key for that Service ID can be loaded into a seed-secret in the team's project. Similarly, a seed-defaults configmap can be created in the team's project that points to the team's resource group. Using the IBM Cloud Operator Note: For more details see the User Guide or the Readme . The IBM Cloud Operator has two objects that can be created: Service - The service object represents the actual service instance in IBM Cloud. Binding - The binding object creates the service credentials that an application can use to interact with the service. Creating a Service apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: dw-elasticsearch-operator-2 namespace: dw-services spec: plan: standard serviceClass: databases-for-elasticsearch tags: [\"operator\", \"sandbox-roks-cluster\",\"ibm-cloud-utility\"] parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private Creating a Binding apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-dw-elasticsearch-operator-2 namespace: dw-services spec: serviceName: dw-elasticsearch-operator-2 role: Editor Content from OpenShift Page (needs to be reviewed) There are many sample yaml files that show how to create various services. Here is one that creates an instance of Cloudant Standard Plan and a binding for it: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: cloudant-operator-7 namespace: ibm-cloud-utility spec: plan: standard serviceClass: cloudantnosqldb parameters: - name: legacyCredentials value: false - name: environment_crn value: \"crn:v1:bluemix:public:cloudantnosqldb:us-south:a/bd****************************:5d774b30-****-****-****-d7**********::\" --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant-operator-7 spec: serviceName: cloudant-operator-7 role: Manager serviceNamespace: ibm-cloud-utility In this example the application that will be using the service is in the ibm-cloud-utility project in OpenShift. This service will be configured to use only IAM credentials, and the corresponding service credentials will be generated with the Manager role. Note: one additional parameter that Kaiser Permanente will need to use is the environment_crn parameter, which \"pins\" the standard plan instance onto dedicated hardware as defined by an Enterprise Plan instance of Cloudant. At the time this testing was done there was no such instance available to use in testing. To create this service simply save this content to a file (cloudant.yml in this example) and run these commands: oc project ibm-cloud-utility This command targets the ibm-cloud-utility project in the oc cli. oc apply -f cloudant.yml After you run the command you can check the status of the service using this command: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 inactive 72s mycos-operator-1 Online 12h The command will continue to show inactive status until the service completes the provisioning process in IBM Cloud and shows Active status there. Sometime after that the service will show Online status: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 Online 2m42s mycos-operator-1 Online 12h There is a similar command to view the status of the binding: oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Failed 27s binding-mycos-operator-1 Online 12h Note: It is okay that the status is initially Failed for the binding. It fails because the service has not been created. The operator will keep trying to create the binding, and it will eventually succeed when the service comes online. oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Online 2m49s binding-mycos-operator-1 Online 12h Once provisioned you will see that the service instance in IBM Cloud is using only IAM credentials: The credentials have the Manager role: Example for ElasticSearch Here is another yaml file for creating an instance of ElasticSearch: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: myes4 namespace: ibm-cloud-utility spec: plan: standard serviceClass: databases-for-elasticsearch parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-myes4 namespace: ibm-cloud-utility spec: serviceName: myes4 role: Editor In this example, there are several parameters that we used to pass via a -c '{<some JSON>}' option on the cf cli. In this case it provides alternate values for the disk and memory allocation, and specifies that the instance should only use private endpoints. There are no screen shots to share here, but testing did verify that all of these parameters did get applied to the resulting service instance. Viewing Dashboard Links When the operator provisions a service in IBM Cloud it the data about the service that it stores includes the dashboard URL for that service. When viewing the Service in the OpenShift console the UI includes a link to the dashboard. Services can be viewed by following these steps: Login to the OpenShift Web Console. Make sure to be in the Administrator view. Click on Operators -> Installed Operators in the left navigation menu. Use the dropdown at the top of the page to switch to the project where the service has been created. Click on the IBM Cloud Operator. Make sure it is version 0.1.11; if not it will first need to be updated before proceeding. Go to the Services tab. Click on the name of one of the services in the list. It should have a property named DashboardURL at the bottom right. Click the link to go to the service dashboard.","title":"IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#ibm-cloud-operator","text":"This operator is used to provision services in IBM Cloud and to create bindings that applications need to consume these services. IBM Cloud Operator Instructions will be provided here when available.","title":"IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#installing-the-ibm-cloud-operator","text":"The IBM Cloud Operator should be available by default in the OperatorHub console in OpenShift. It can be installed using the OperatorHub Console in OpenShift, or be using the OpenShift CLI.","title":"Installing the IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#install-using-the-ui","text":"","title":"Install using the UI"},{"location":"cloud-platform/ibm-cloud-operator/#install-using-the-openshift-cli","text":"Documentation on installing operators in OpenShift can be found here . In the case of the IBM Cloud Operator, it is a matter of creating a yaml file and using the oc cli. On that page it discusses two objects that are needed - the OperatorGroup object and the Subscription object. If you intend to make the operator available in all projects you do not need to create a new OperatorGroup object; one already exists. You only need to create one if you want to install the operator into a single project. Here is sample yaml for creating a Subscription object: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: ibmcloud-operator namespace: openshift-operators spec: channel: alpha installPlanApproval: Automatic name: ibmcloud-operator source: community-operators sourceNamespace: openshift-marketplace startingCSV: ibmcloud-operator.v0.1.7 Note: If you specify the openshift-operators namespace the operator will be installed in all namespaces in the cluster. This is the default behavior; if you specify any other namespace the operator will only be installed in that namespace. More documentation can be found here . Create a yaml file with the content above. Make sure to change the startingCSV value to the correct version of the IBM Cloud Operator as it appears in the OperatorHub console. Login to the oc cli. You can copy the command from the OpenShift Web Console: 1. Click the \"Display Token\" link. Select the command in the \"Log in with this token\" section: Paste the command in a terminal and run it. ``` oc login --token=5y7lF * * ****BOm2cSE --server=https://c106-e.us-south.containers.cloud.ibm.com:31777 Logged into \"https://c106-e.us-south.containers.cloud.ibm.com:31777\" as \"IAM#dwakeman@us.ibm.com\" using the token provided. You have access to 55 projects, the list has been suppressed. You can list all projects with 'oc projects' Using project \"default\". 1. Run this command: `oc apply -f <yaml_file_name>` oc apply -f ibmcloud-operator-subscription.yaml subscription.operators.coreos.com/ibmcloud-operator created ``` You should see a message that the subscription was created. It should now show up in the OpenShift console in the \"Installed Operators\" page. Congratulations, the IBM Cloud Operator has been installed!!","title":"Install using the OpenShift CLI"},{"location":"cloud-platform/ibm-cloud-operator/#configuring-the-ibm-cloud-operator","text":"There are a few things that need to be configured in order for the IBM Cloud Operator to work. First, it needs an API key to use to authenticate with IBM Cloud. That user must have sufficient permissions to create services and bindings. More on that in a bit. That API Key gets loaded into a secret named seed-secret . The operator will first look for a secret by that name in the current project, and use it if found. If not, it will look in the default project, and if it finds one there it will use it. This behavior will allow Kaiser Permanente to provide each team with a separate seed-secret that uses an API key for a service ID that only has permission to create certain services, and only in that team's resource group. More discussion is needed to finalize the design for configuration of the operator for KP. If there is no seed-secret in the default project then there must be one in the current project in order for it to work. Each team can be given their own Service ID with permission to only create specific services, and only in the team's resource group. An API Key for that Service ID can be loaded into a seed-secret in the team's project. Similarly, a seed-defaults configmap can be created in the team's project that points to the team's resource group.","title":"Configuring the IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#using-the-ibm-cloud-operator","text":"Note: For more details see the User Guide or the Readme . The IBM Cloud Operator has two objects that can be created: Service - The service object represents the actual service instance in IBM Cloud. Binding - The binding object creates the service credentials that an application can use to interact with the service.","title":"Using the IBM Cloud Operator"},{"location":"cloud-platform/ibm-cloud-operator/#creating-a-service","text":"apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: dw-elasticsearch-operator-2 namespace: dw-services spec: plan: standard serviceClass: databases-for-elasticsearch tags: [\"operator\", \"sandbox-roks-cluster\",\"ibm-cloud-utility\"] parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private","title":"Creating a Service"},{"location":"cloud-platform/ibm-cloud-operator/#creating-a-binding","text":"apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-dw-elasticsearch-operator-2 namespace: dw-services spec: serviceName: dw-elasticsearch-operator-2 role: Editor","title":"Creating a Binding"},{"location":"cloud-platform/ibm-cloud-operator/#content-from-openshift-page-needs-to-be-reviewed","text":"There are many sample yaml files that show how to create various services. Here is one that creates an instance of Cloudant Standard Plan and a binding for it: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: cloudant-operator-7 namespace: ibm-cloud-utility spec: plan: standard serviceClass: cloudantnosqldb parameters: - name: legacyCredentials value: false - name: environment_crn value: \"crn:v1:bluemix:public:cloudantnosqldb:us-south:a/bd****************************:5d774b30-****-****-****-d7**********::\" --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-cloudant-operator-7 spec: serviceName: cloudant-operator-7 role: Manager serviceNamespace: ibm-cloud-utility In this example the application that will be using the service is in the ibm-cloud-utility project in OpenShift. This service will be configured to use only IAM credentials, and the corresponding service credentials will be generated with the Manager role. Note: one additional parameter that Kaiser Permanente will need to use is the environment_crn parameter, which \"pins\" the standard plan instance onto dedicated hardware as defined by an Enterprise Plan instance of Cloudant. At the time this testing was done there was no such instance available to use in testing. To create this service simply save this content to a file (cloudant.yml in this example) and run these commands: oc project ibm-cloud-utility This command targets the ibm-cloud-utility project in the oc cli. oc apply -f cloudant.yml After you run the command you can check the status of the service using this command: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 inactive 72s mycos-operator-1 Online 12h The command will continue to show inactive status until the service completes the provisioning process in IBM Cloud and shows Active status there. Sometime after that the service will show Online status: oc get service.ibmcloud NAME STATUS AGE cloudant-operator-3 Online 14h cloudant-operator-7 Online 2m42s mycos-operator-1 Online 12h There is a similar command to view the status of the binding: oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Failed 27s binding-mycos-operator-1 Online 12h Note: It is okay that the status is initially Failed for the binding. It fails because the service has not been created. The operator will keep trying to create the binding, and it will eventually succeed when the service comes online. oc get binding.ibmcloud NAME STATUS AGE binding-cloudant-operator-3 Online 14h binding-cloudant-operator-7 Online 2m49s binding-mycos-operator-1 Online 12h Once provisioned you will see that the service instance in IBM Cloud is using only IAM credentials: The credentials have the Manager role:","title":"Content from OpenShift Page (needs to be reviewed)"},{"location":"cloud-platform/ibm-cloud-operator/#example-for-elasticsearch","text":"Here is another yaml file for creating an instance of ElasticSearch: apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Service metadata: name: myes4 namespace: ibm-cloud-utility spec: plan: standard serviceClass: databases-for-elasticsearch parameters: - name: members_disk_allocation_mb value: 30720 - name: members_memory_allocation_mb value: 6144 - name: service-endpoints value: private --- apiVersion: ibmcloud.ibm.com/v1alpha1 kind: Binding metadata: name: binding-myes4 namespace: ibm-cloud-utility spec: serviceName: myes4 role: Editor In this example, there are several parameters that we used to pass via a -c '{<some JSON>}' option on the cf cli. In this case it provides alternate values for the disk and memory allocation, and specifies that the instance should only use private endpoints. There are no screen shots to share here, but testing did verify that all of these parameters did get applied to the resulting service instance.","title":"Example for ElasticSearch"},{"location":"cloud-platform/ibm-cloud-operator/#viewing-dashboard-links","text":"When the operator provisions a service in IBM Cloud it the data about the service that it stores includes the dashboard URL for that service. When viewing the Service in the OpenShift console the UI includes a link to the dashboard. Services can be viewed by following these steps: Login to the OpenShift Web Console. Make sure to be in the Administrator view. Click on Operators -> Installed Operators in the left navigation menu. Use the dropdown at the top of the page to switch to the project where the service has been created. Click on the IBM Cloud Operator. Make sure it is version 0.1.11; if not it will first need to be updated before proceeding. Go to the Services tab. Click on the name of one of the services in the list. It should have a property named DashboardURL at the bottom right. Click the link to go to the service dashboard.","title":"Viewing Dashboard Links"},{"location":"cloud-platform/key-protect/","text":"Key Protect Restricting access to keys Key Protect now supports the ability to assign access to a single key within a Key Protect instance to a given user or access group via Access Policy. If Kaiser Permanente wishes to maintain a single, shared instance of Key Protect and assign individual keys to any development team, this feature will allow it. When a user needs to specify a key for encryption of a particular service, that user will only see the instance of Key Protect to which the user has access and the specific key to which the user has been granted access. In the UI this will filter any dropdowns appropriately and in the CLI or API, access will also be limited. Documentation for setting this level of access can be found here . Note: Key level access is currently only available via access policy for a user or access group. It cannot be applied when granting Service-to-Service Authorization, which is access between two IBM services, such as COS and Key Protect. Dual Authorization Delete This is a new feature that was added to Key Protect that provides the ability to create an instance level policy requiring that two different users approve the deletion of a key. In order to delete a key, a user with appropriate permissions must \"set the key for deletion\", which updates a flag on the key. After that, another user with permission to do so can delete the key. The person who does the first authorization cannot actually delete the key; it requires two different users to complete the key deletion. Note: At this time the \"first\" action to set the key for deletion can only be done via API. Once done, any available option for deleting a key (UI, CLI, API) can be used to actually delete the key. In order to make it easier to use this new feature IBM has provided (As-Is, not part of the product and not supported) a script that automates some of these actions using the API. The script can be found here . Considerations The Dual Auth Delete policy is applied at the individual key level, although it can be defined at the service instance level. Once the Dual Auth policy is set for a service instance, all keys created after that will automatically inherit that policy. Any keys that existed prior to the creation of the service instance level policy will NOT inherit the policy. If need be the policy can be set on individual keys. Note: This still needs to be validated. Using the script The script leverages the Key Protect API via cURL commands. These commands require an auth token, which can be retrieved via the ibmcloud CLI. It also uses the CLI to look up information needed by some of the APIs. The script will automatically get the auth token and use the CLI to look things up, but it does require the user of the script to first login to the CLI in the same terminal/shell where the script will be run. Command Options NAME: keyprotect.sh - Manage features of Key Protect USAGE: keyprotect.sh <key-protect-instance-name> command [options] COMMANDS: ------------------------------------------------------------------------------------------- view-policies List the current policies for the Key Protect Instance enable-dual-auth Enable the Dual Authorization policy for key deletes for all keys disable-dual-auth Disable the Dual Authorization policy for key deletes for all keys view-keys List the keys in the Key Protect Instance in JSON format view-keys-list List the keys in the Key Protect Instance in list format view-deleted-keys List the deleted keys in the Key Protect Instance in JSON format view-deleted-keys-list List the deleted keys in the Key Protect Instance in list format view-key-material View the material for a standard key view-key-policies View the current polices for the specified key import-key Import a standard or root key restore-key Restore an imported key that has been deleted set-key-deletion Set the specified key for deletion (first auth) unset-key-deletion Unset the specified key for deletion, which removes the first auth help, h View help for this script Note: For your convenience this command executes the ibmcloud cli to look up certain information needed to perform these tasks. It requires you to be logged into the ibmcloud cli before you run this command. Viewing policies for a Key Protect Instance Command ./keyprotect.sh key-protect-dallas-dw view-policies Output Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 }, \"resources\": [] } Enabling the Dual Authorization policy for a Key Protect Instance Command ./keyprotect.sh key-protect-dallas-dw enable-dual-auth Output Enabling Dual Authorization for service key-protect-dallas-dw... Done. Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true }, \"creation_date\": \"2020-01-16T20:47:30Z\", \"created_by\": \"IBMid-110000J6VA\", \"updated_by\": \"IBMid-110000J6VA\", \"last_updated\": \"2020-01-16T20:47:30Z\" } ] } Request complete. View Keys in JSON format Command ./keyprotect.sh key-protect-dallas-dw view-keys Output Listing keys for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 2 }, \"resources\": [ { \"id\": \"a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-delete-crk\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 }, { \"id\": \"f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-cos-crk-1\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-17T21:44:39Z\", \"lastUpdateDate\": \"2020-01-17T21:44:39Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 } ] } Request complete. View Keys in list format Command ./keyprotect.sh key-protect-dallas-dw view-keys-list Output Listing keys for service key-protect-dallas-dw... name id crn dw-test-delete-crk a15604c8-e5c6-4fc9-ac92-5be79bdb1424 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424 dw-test-cos-crk-1 f41d77aa-f357-4234-ba46-6aec1b4a7f92 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92 Request complete. View Deleted Keys in JSON Format Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys Output Listing keys for service key-protect-dallas-dw... Values for State field: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 4 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"name\": \"dw-ui-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"imported\": false, \"creationDate\": \"2020-01-16T22:40:15Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T22:40:15Z\", \"keyVersion\": { \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-01-23T22:41:02Z\" } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\", \"name\": \"dw-test-import-delete-restore-01\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7\", \"imported\": true, \"creationDate\": \"2020-05-01T21:31:47Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T21:31:47Z\", \"keyVersion\": { \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"name\": \"dw-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"imported\": false, \"creationDate\": \"2020-01-16T20:53:55Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T20:53:55Z\", \"keyVersion\": { \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"name\": \"${keyName}\", \"state\": 5, \"extractable\": true, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"imported\": false, \"creationDate\": \"2020-05-01T14:37:44Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T14:37:44Z\", \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-05-08T16:16:10Z\" } } ] } View Deleted Keys in List Format Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys-list Output Listing deleted keys for service key-protect-dallas-dw... Values for State column: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed name id state crn dw-ui-test-crk-1 38d3c46d-0f94-4843-ba36-9ac6c08c4c3c 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c dw-test-import-delete-restore-01 4b559191-e10e-4bf6-9891-9af1948b55f7 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7 dw-test-crk-1 75588eb5-81d4-4cdb-997c-b6deb6be06cf 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf ${keyName} 814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 Request complete. View Key Policies Command ./keyprotect.sh key-protect-dallas-dw view-key-policies a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Note: The key-specific commands require an extra parameter, the key id, which can be found in the UI, CLI or API. If not provided these commands will produce this error: for view-key-policies a key id is required USAGE: keyprotect.sh [service instance name] view-key-policies [key-id] Output viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\" } ] } Request complete. Set Key for Deletion Command ./keyprotect.sh key-protect-dallas-dw set-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Setting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:05:09Z\" } ] } Request complete. Note: If the key has not been enabled for Dual Auth Delete, the command will return the error below. Setting key 75588eb5-81d4-4cdb-997c-b6deb6be06cf in service key-protect-dallas-dw for deletion... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.error+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"errorMsg\": \"Action could not be performed on key. Please see `reasons` for more details.\", \"reasons\": [ { \"code\": \"NOT_DUAL_AUTH_ERR\", \"message\": \"The key is not dual auth enabled and cannot be set for deletion\", \"status\": 409, \"more_info\": \"https://cloud.ibm.com/apidocs/key-protect\" } ] } ] } Done. viewing policies for key 75588eb5-81d4-4cdb-997c-b6deb6be06cf... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 } } Request complete. Unset key for deletion In some cases it may turn out that the first authorization was done in error. This command can be used to unset the key for deletion, which removes the first authorization. Command ./keyprotect.sh key-protect-dallas-dw unset-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Unsetting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:15:44Z\" } ] } Request complete. Note: Setting or unsetting the key for deletion does NOT affect the policy itself; as seen in the output above the policy is still set to true. At this time the \"set deletion\" status and its lastUpdated date do not show up in the API response. IBM is going to update the API in the next iteration to include these two fields. Deleting a Key from the UI IMPORTANT WARNING: New functionality for Key Protect has been delivered that will prevent a key from being deleted when using the Key Protect API if that key is being used by any services. This feature has NOT YET been updated in the UI!!! If you delete the key in the UI and the key (or the Key Protect instance) does NOT have Dual Authorization Delete policy enabled, that key WILL get deleted, regardless of whether or not is being used. To protect against this possibility make sure that all Key Protect instances have the Dual Authorization Delete policy enabled and use the Registration API before deleting any key to see if it is being used by any services or resources. Note: The steps below for deleting a kay via the UI assume that the Dual Authorization Policy has been enabled. If you try to delete a key from the UI and it has not yet been set for deletion you will see an error like this: If you try to delete the key after it has been set for deletion and you're the one who did that you will see this error: Note: If you try to delete the key and somebody else set the key for deletion you will be able to delete the key. Viewing usage of keys It is possible in IBM Cloud to track where Key Protect keys are being used. When a service is provisioned or configure to use a Key Protect key, it \"registers\" that usage with Key Protect. You can see the list of resources using a given key with a new operation in the Key Protect API . GET https://us-south.kms.cloud.ibm.com/api/v2/keys/4442aa89-9749-4cad-9a6e-73a77508a616/registrations Headers: Authorization: 'Bearer ' bluemix-instance: ' ' The URL endpoint will vary by region, and it contains the GUID of the key. { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 3 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8:bucket:dw-kp-bucket-test-registration-api-1578085-01\", \"createdBy\": \"crn-crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8::\", \"creationDate\": \"2020-03-30T15:44:47Z\", \"lastUpdated\": \"2020-03-30T15:44:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } In the JSON example above, the key is used with two service instances - an instance Databases for Redis and a Cloud Object Storage bucket. Notice that the Redis instance has two entries; that is because Key Protect registers usage at the resource level, not necessarily at the service instance level. In the case of Redis, the key is used in two separate PVCs, so there are two registrations, both that have the same value for the createdBy field, which contains the CRN of the Redis instance. Later in the key's lifecycle, operations on that key (rotation, deletion, crypto erasure) can be tracked by individual resource instance. Crypto Erasure It is sometimes necessary to rotate Key Protect Keys, whether it be proactive security measures to rotate periodically, or reactive, when it is possible that the key has become compromised. Key rotation actually involves two steps: Key Rotation : - action initiated in Key Protect to rotate the key. Key Protect replaces the old material with newly generated material. Only works for IBM-generated keys today. Key Replacement - action initiated by the service using the key when notified by Key Protect that the key has been rotated. The service does what it needs to to to replace the old material with the new material. Validation Steps To validate the behavior of this feature the following general steps can be followed. Prerequisites Key Protect Instance with an IBM-generated root key a service (i.e. Databases for Redis, Databases for PostGreS, Cloudant, Event Streams, Cloud Object Storage) that was provisioned using the key Validation steps Use the Key Protect API to verify that the key is being used. In this example, the key is being used for two service instances, one Redis and one ElasticSearch. The API will return JSON like this: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 5 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-4061efea-2c14-48b7-b016-86da46238b2b\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:48Z\", \"lastUpdated\": \"2020-03-31T13:15:48Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-781b92eb-c93a-49b8-96cc-d4b484d3bdeb\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-85b7b18d-e186-4a0e-be9c-09343571914e\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } Use the Key Protect API to determine the current version of the key: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"name\": \"dw-kp-reg-test-crk-1\", \"state\": 1, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4442aa89-9749-4cad-9a6e-73a77508a616\", \"imported\": false, \"creationDate\": \"2020-03-30T13:43:29Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-03-31T14:18:33Z\", \"lastRotateDate\": \"2020-03-31T14:18:33Z\", \"keyVersion\": { \"id\": \"b959b827-9453-4707-a92b-6e9d632c5b98\", \"creationDate\": \"2020-03-31T14:18:33Z\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": false }, \"deleted\": false } ] } Notice the keyVersion object; it shows the id and creation date of the current version of the key. In this example, the JSON above was obtained after the key rotation, so it shows a different version than what appears in the registrations. Go into the Key Protect instance and rotate the key. Once the key has been rotated you will see a popup saying that it has been done. This means that Key Protect replaced the material and notified the services about it. Review the Activity Tracker instance for the region in which the services are provisioned. Filter by sources, selecting kms and ibm-cloud-databases-prod . You should see an event from Key Protect (kms) that the key was rotated, and events from ICD that they completed the key replacement. Here is the detail for each event: Key Rotation: ElasticSearch: Redis: Note: if Key Protect doesn't get a report back from a service that the rotation has been completed it will send failure events to Activity Tracker: In this case, it turns out that there is a defect in the acknowledgement that ICD sends back to Key Protect to indicate that the rotation is complete. The error message shows that the Ack failed, but the rotation did complete. The ICD engineering team has already submitted a pull request with a fix for this issue, and as soon as it gets deployed these \"ack failures\" should no longer occur.","title":"Key Protect"},{"location":"cloud-platform/key-protect/#key-protect","text":"","title":"Key Protect"},{"location":"cloud-platform/key-protect/#restricting-access-to-keys","text":"Key Protect now supports the ability to assign access to a single key within a Key Protect instance to a given user or access group via Access Policy. If Kaiser Permanente wishes to maintain a single, shared instance of Key Protect and assign individual keys to any development team, this feature will allow it. When a user needs to specify a key for encryption of a particular service, that user will only see the instance of Key Protect to which the user has access and the specific key to which the user has been granted access. In the UI this will filter any dropdowns appropriately and in the CLI or API, access will also be limited. Documentation for setting this level of access can be found here . Note: Key level access is currently only available via access policy for a user or access group. It cannot be applied when granting Service-to-Service Authorization, which is access between two IBM services, such as COS and Key Protect.","title":"Restricting access to keys"},{"location":"cloud-platform/key-protect/#dual-authorization-delete","text":"This is a new feature that was added to Key Protect that provides the ability to create an instance level policy requiring that two different users approve the deletion of a key. In order to delete a key, a user with appropriate permissions must \"set the key for deletion\", which updates a flag on the key. After that, another user with permission to do so can delete the key. The person who does the first authorization cannot actually delete the key; it requires two different users to complete the key deletion. Note: At this time the \"first\" action to set the key for deletion can only be done via API. Once done, any available option for deleting a key (UI, CLI, API) can be used to actually delete the key. In order to make it easier to use this new feature IBM has provided (As-Is, not part of the product and not supported) a script that automates some of these actions using the API. The script can be found here . Considerations The Dual Auth Delete policy is applied at the individual key level, although it can be defined at the service instance level. Once the Dual Auth policy is set for a service instance, all keys created after that will automatically inherit that policy. Any keys that existed prior to the creation of the service instance level policy will NOT inherit the policy. If need be the policy can be set on individual keys. Note: This still needs to be validated.","title":"Dual Authorization Delete"},{"location":"cloud-platform/key-protect/#using-the-script","text":"The script leverages the Key Protect API via cURL commands. These commands require an auth token, which can be retrieved via the ibmcloud CLI. It also uses the CLI to look up information needed by some of the APIs. The script will automatically get the auth token and use the CLI to look things up, but it does require the user of the script to first login to the CLI in the same terminal/shell where the script will be run. Command Options NAME: keyprotect.sh - Manage features of Key Protect USAGE: keyprotect.sh <key-protect-instance-name> command [options] COMMANDS: ------------------------------------------------------------------------------------------- view-policies List the current policies for the Key Protect Instance enable-dual-auth Enable the Dual Authorization policy for key deletes for all keys disable-dual-auth Disable the Dual Authorization policy for key deletes for all keys view-keys List the keys in the Key Protect Instance in JSON format view-keys-list List the keys in the Key Protect Instance in list format view-deleted-keys List the deleted keys in the Key Protect Instance in JSON format view-deleted-keys-list List the deleted keys in the Key Protect Instance in list format view-key-material View the material for a standard key view-key-policies View the current polices for the specified key import-key Import a standard or root key restore-key Restore an imported key that has been deleted set-key-deletion Set the specified key for deletion (first auth) unset-key-deletion Unset the specified key for deletion, which removes the first auth help, h View help for this script Note: For your convenience this command executes the ibmcloud cli to look up certain information needed to perform these tasks. It requires you to be logged into the ibmcloud cli before you run this command.","title":"Using the script"},{"location":"cloud-platform/key-protect/#viewing-policies-for-a-key-protect-instance","text":"Command ./keyprotect.sh key-protect-dallas-dw view-policies Output Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 }, \"resources\": [] }","title":"Viewing policies for a Key Protect Instance"},{"location":"cloud-platform/key-protect/#enabling-the-dual-authorization-policy-for-a-key-protect-instance","text":"Command ./keyprotect.sh key-protect-dallas-dw enable-dual-auth Output Enabling Dual Authorization for service key-protect-dallas-dw... Done. Checking current policies for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true }, \"creation_date\": \"2020-01-16T20:47:30Z\", \"created_by\": \"IBMid-110000J6VA\", \"updated_by\": \"IBMid-110000J6VA\", \"last_updated\": \"2020-01-16T20:47:30Z\" } ] } Request complete.","title":"Enabling the Dual Authorization policy for a Key Protect Instance"},{"location":"cloud-platform/key-protect/#view-keys-in-json-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-keys Output Listing keys for service key-protect-dallas-dw... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 2 }, \"resources\": [ { \"id\": \"a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-delete-crk\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 }, { \"id\": \"f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"dw-test-cos-crk-1\", \"state\": 1, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92\", \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-17T21:44:39Z\", \"lastUpdateDate\": \"2020-01-17T21:44:39Z\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"extractable\": false, \"imported\": false, \"algorithmMode\": \"CBC_PAD\", \"algorithmBitSize\": 256 } ] } Request complete.","title":"View Keys in JSON format"},{"location":"cloud-platform/key-protect/#view-keys-in-list-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-keys-list Output Listing keys for service key-protect-dallas-dw... name id crn dw-test-delete-crk a15604c8-e5c6-4fc9-ac92-5be79bdb1424 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:a15604c8-e5c6-4fc9-ac92-5be79bdb1424 dw-test-cos-crk-1 f41d77aa-f357-4234-ba46-6aec1b4a7f92 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:f41d77aa-f357-4234-ba46-6aec1b4a7f92 Request complete.","title":"View Keys in list format"},{"location":"cloud-platform/key-protect/#view-deleted-keys-in-json-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys Output Listing keys for service key-protect-dallas-dw... Values for State field: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 4 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"name\": \"dw-ui-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\", \"imported\": false, \"creationDate\": \"2020-01-16T22:40:15Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T22:40:15Z\", \"keyVersion\": { \"id\": \"38d3c46d-0f94-4843-ba36-9ac6c08c4c3c\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-01-23T22:41:02Z\" } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\", \"name\": \"dw-test-import-delete-restore-01\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7\", \"imported\": true, \"creationDate\": \"2020-05-01T21:31:47Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T21:31:47Z\", \"keyVersion\": { \"id\": \"4b559191-e10e-4bf6-9891-9af1948b55f7\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"name\": \"dw-test-crk-1\", \"state\": 5, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf\", \"imported\": false, \"creationDate\": \"2020-01-16T20:53:55Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-01-16T20:53:55Z\", \"keyVersion\": { \"id\": \"75588eb5-81d4-4cdb-997c-b6deb6be06cf\" }, \"dualAuthDelete\": { \"enabled\": false } }, { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"name\": \"${keyName}\", \"state\": 5, \"extractable\": true, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3\", \"imported\": false, \"creationDate\": \"2020-05-01T14:37:44Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-05-01T14:37:44Z\", \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": true, \"authExpiration\": \"2020-05-08T16:16:10Z\" } } ] }","title":"View Deleted Keys in JSON Format"},{"location":"cloud-platform/key-protect/#view-deleted-keys-in-list-format","text":"Command ./keyprotect.sh key-protect-dallas-dw view-deleted-keys-list Output Listing deleted keys for service key-protect-dallas-dw... Values for State column: ------------------------- 0 Pre-activation 1 Active 2 Suspended 3 Deactivated 5 Destroyed name id state crn dw-ui-test-crk-1 38d3c46d-0f94-4843-ba36-9ac6c08c4c3c 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:38d3c46d-0f94-4843-ba36-9ac6c08c4c3c dw-test-import-delete-restore-01 4b559191-e10e-4bf6-9891-9af1948b55f7 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4b559191-e10e-4bf6-9891-9af1948b55f7 dw-test-crk-1 75588eb5-81d4-4cdb-997c-b6deb6be06cf 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:75588eb5-81d4-4cdb-997c-b6deb6be06cf ${keyName} 814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 5 crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:814f0fa7-d343-4a76-b0c2-9ebb8ecea2c3 Request complete.","title":"View Deleted Keys in List Format"},{"location":"cloud-platform/key-protect/#view-key-policies","text":"Command ./keyprotect.sh key-protect-dallas-dw view-key-policies a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Note: The key-specific commands require an extra parameter, the key id, which can be found in the UI, CLI or API. If not provided these commands will produce this error: for view-key-policies a key id is required USAGE: keyprotect.sh [service instance name] view-key-policies [key-id] Output viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:03:41Z\" } ] } Request complete.","title":"View Key Policies"},{"location":"cloud-platform/key-protect/#set-key-for-deletion","text":"Command ./keyprotect.sh key-protect-dallas-dw set-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Setting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:05:09Z\" } ] } Request complete. Note: If the key has not been enabled for Dual Auth Delete, the command will return the error below. Setting key 75588eb5-81d4-4cdb-997c-b6deb6be06cf in service key-protect-dallas-dw for deletion... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.error+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"errorMsg\": \"Action could not be performed on key. Please see `reasons` for more details.\", \"reasons\": [ { \"code\": \"NOT_DUAL_AUTH_ERR\", \"message\": \"The key is not dual auth enabled and cannot be set for deletion\", \"status\": 409, \"more_info\": \"https://cloud.ibm.com/apidocs/key-protect\" } ] } ] } Done. viewing policies for key 75588eb5-81d4-4cdb-997c-b6deb6be06cf... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 0 } } Request complete.","title":"Set Key for Deletion"},{"location":"cloud-platform/key-protect/#unset-key-for-deletion","text":"In some cases it may turn out that the first authorization was done in error. This command can be used to unset the key for deletion, which removes the first authorization. Command ./keyprotect.sh key-protect-dallas-dw unset-key-deletion a15604c8-e5c6-4fc9-ac92-5be79bdb1424 Output Unsetting key a15604c8-e5c6-4fc9-ac92-5be79bdb1424 in service key-protect-dallas-dw for deletion... Done. viewing policies for key a15604c8-e5c6-4fc9-ac92-5be79bdb1424... { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"id\": \"3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:policy:3b7a1be5-ab6b-4139-b856-a4c0fafc82e8\", \"dualAuthDelete\": { \"enabled\": true }, \"createdBy\": \"IBMid-110000J6VA\", \"creationDate\": \"2020-01-16T21:03:41Z\", \"updatedBy\": \"IBMid-110000J6VA\", \"lastUpdateDate\": \"2020-01-16T21:15:44Z\" } ] } Request complete. Note: Setting or unsetting the key for deletion does NOT affect the policy itself; as seen in the output above the policy is still set to true. At this time the \"set deletion\" status and its lastUpdated date do not show up in the API response. IBM is going to update the API in the next iteration to include these two fields.","title":"Unset key for deletion"},{"location":"cloud-platform/key-protect/#deleting-a-key-from-the-ui","text":"IMPORTANT WARNING: New functionality for Key Protect has been delivered that will prevent a key from being deleted when using the Key Protect API if that key is being used by any services. This feature has NOT YET been updated in the UI!!! If you delete the key in the UI and the key (or the Key Protect instance) does NOT have Dual Authorization Delete policy enabled, that key WILL get deleted, regardless of whether or not is being used. To protect against this possibility make sure that all Key Protect instances have the Dual Authorization Delete policy enabled and use the Registration API before deleting any key to see if it is being used by any services or resources. Note: The steps below for deleting a kay via the UI assume that the Dual Authorization Policy has been enabled. If you try to delete a key from the UI and it has not yet been set for deletion you will see an error like this: If you try to delete the key after it has been set for deletion and you're the one who did that you will see this error: Note: If you try to delete the key and somebody else set the key for deletion you will be able to delete the key.","title":"Deleting a Key from the UI"},{"location":"cloud-platform/key-protect/#viewing-usage-of-keys","text":"It is possible in IBM Cloud to track where Key Protect keys are being used. When a service is provisioned or configure to use a Key Protect key, it \"registers\" that usage with Key Protect. You can see the list of resources using a given key with a new operation in the Key Protect API . GET https://us-south.kms.cloud.ibm.com/api/v2/keys/4442aa89-9749-4cad-9a6e-73a77508a616/registrations Headers: Authorization: 'Bearer ' bluemix-instance: ' ' The URL endpoint will vary by region, and it contains the GUID of the key. { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 3 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8:bucket:dw-kp-bucket-test-registration-api-1578085-01\", \"createdBy\": \"crn-crn:v1:bluemix:public:cloud-object-storage:global:a/9d5d528aa786af01ce99593a827cd68a:b3f6085f-87fd-4b1a-945f-57ba00958fe8::\", \"creationDate\": \"2020-03-30T15:44:47Z\", \"lastUpdated\": \"2020-03-30T15:44:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } In the JSON example above, the key is used with two service instances - an instance Databases for Redis and a Cloud Object Storage bucket. Notice that the Redis instance has two entries; that is because Key Protect registers usage at the resource level, not necessarily at the service instance level. In the case of Redis, the key is used in two separate PVCs, so there are two registrations, both that have the same value for the createdBy field, which contains the CRN of the Redis instance. Later in the key's lifecycle, operations on that key (rotation, deletion, crypto erasure) can be tracked by individual resource instance.","title":"Viewing usage of keys"},{"location":"cloud-platform/key-protect/#crypto-erasure","text":"It is sometimes necessary to rotate Key Protect Keys, whether it be proactive security measures to rotate periodically, or reactive, when it is possible that the key has become compromised. Key rotation actually involves two steps: Key Rotation : - action initiated in Key Protect to rotate the key. Key Protect replaces the old material with newly generated material. Only works for IBM-generated keys today. Key Replacement - action initiated by the service using the key when notified by Key Protect that the key has been rotated. The service does what it needs to to to replace the old material with the new material.","title":"Crypto Erasure"},{"location":"cloud-platform/key-protect/#validation-steps","text":"To validate the behavior of this feature the following general steps can be followed. Prerequisites Key Protect Instance with an IBM-generated root key a service (i.e. Databases for Redis, Databases for PostGreS, Cloudant, Event Streams, Cloud Object Storage) that was provisioned using the key Validation steps Use the Key Protect API to verify that the key is being used. In this example, the key is being used for two service instances, one Redis and one ElasticSearch. The API will return JSON like this: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.registration+json\", \"collectionTotal\": 5 }, \"resources\": [ { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-4061efea-2c14-48b7-b016-86da46238b2b\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:48Z\", \"lastUpdated\": \"2020-03-31T13:15:48Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-781b92eb-c93a-49b8-96cc-d4b484d3bdeb\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::/agentid=pvc-85b7b18d-e186-4a0e-be9c-09343571914e\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-elasticsearch:us-south:a/9d5d528aa786af01ce99593a827cd68a:82251542-de4d-4bec-bf3c-5a87fae71e04::\", \"creationDate\": \"2020-03-31T13:15:44Z\", \"lastUpdated\": \"2020-03-31T13:15:44Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-895c12bd-5e11-4f5d-9890-91ffb800bf21\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:32Z\", \"lastUpdated\": \"2020-03-30T13:48:32Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } }, { \"keyId\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"resourceCrn\": \"crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::/agentid=pvc-e365120e-7839-464d-940a-abd93a9ab5d1\", \"createdBy\": \"crn-crn:v1:bluemix:public:databases-for-redis:us-south:a/9d5d528aa786af01ce99593a827cd68a:9054779f-16f9-405c-8c1b-5a3cfda2b744::\", \"creationDate\": \"2020-03-30T13:48:47Z\", \"lastUpdated\": \"2020-03-30T13:48:47Z\", \"preventKeyDeletion\": false, \"keyVersion\": { \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"creationDate\": \"2020-03-30T13:43:29Z\" } } ] } Use the Key Protect API to determine the current version of the key: { \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"id\": \"4442aa89-9749-4cad-9a6e-73a77508a616\", \"name\": \"dw-kp-reg-test-crk-1\", \"state\": 1, \"extractable\": false, \"crn\": \"crn:v1:bluemix:public:kms:us-south:a/9d5d528aa786af01ce99593a827cd68a:a58587c8-857a-4846-808d-3562de511fd6:key:4442aa89-9749-4cad-9a6e-73a77508a616\", \"imported\": false, \"creationDate\": \"2020-03-30T13:43:29Z\", \"createdBy\": \"IBMid-110000J6VA\", \"algorithmType\": \"AES\", \"algorithmMetadata\": { \"bitLength\": \"256\", \"mode\": \"CBC_PAD\" }, \"algorithmBitSize\": 256, \"algorithmMode\": \"CBC_PAD\", \"lastUpdateDate\": \"2020-03-31T14:18:33Z\", \"lastRotateDate\": \"2020-03-31T14:18:33Z\", \"keyVersion\": { \"id\": \"b959b827-9453-4707-a92b-6e9d632c5b98\", \"creationDate\": \"2020-03-31T14:18:33Z\" }, \"dualAuthDelete\": { \"enabled\": true, \"keySetForDeletion\": false }, \"deleted\": false } ] } Notice the keyVersion object; it shows the id and creation date of the current version of the key. In this example, the JSON above was obtained after the key rotation, so it shows a different version than what appears in the registrations. Go into the Key Protect instance and rotate the key. Once the key has been rotated you will see a popup saying that it has been done. This means that Key Protect replaced the material and notified the services about it. Review the Activity Tracker instance for the region in which the services are provisioned. Filter by sources, selecting kms and ibm-cloud-databases-prod . You should see an event from Key Protect (kms) that the key was rotated, and events from ICD that they completed the key replacement. Here is the detail for each event: Key Rotation: ElasticSearch: Redis: Note: if Key Protect doesn't get a report back from a service that the rotation has been completed it will send failure events to Activity Tracker: In this case, it turns out that there is a defect in the acknowledgement that ICD sends back to Key Protect to indicate that the rotation is complete. The error message shows that the Ack failed, but the rotation did complete. The ICD engineering team has already submitted a pull request with a fix for this issue, and as soon as it gets deployed these \"ack failures\" should no longer occur.","title":"Validation Steps"},{"location":"cloud-platform/openshift-ibm-cloud/","text":"OpenShift on IBM Cloud This page will contain details and documentation about OpenShift 4.x on IBM Cloud as work progresses. Base Images Many IBM Cloud Dedicated customers will be primarily using two images that correspond to two commonly used buildpacks in Cloud Foundry today: WebSphere Liberty - Both the standard liberty and open liberty images are available now. The current version is 19.0.0.2, with many older versions available in the repo. The current version of the buildpack also uses 19.0.0.2. Node.js - IBM is no longer delivering builds of Node.js for buildpacks and is not delivering any images; instead IBM is providing support for the community buid. An IBM Team (the Cloud Engagement Hub) has built a guide for customizing images prior to loading them into an enterprise registry: https://medium.com/cloud-engagement-hub/so-you-want-customized-websphere-container-images-heres-how-42f0e598733f Operators Using the IBM Cloud Operator The IBM Cloud Operator is used to create services in IBM Cloud (i.e. Cloudant, PostGreSQL, Redis, etc.) and bindings in an OpenShift project that contain the credentials needed to programmatically access the service. The IBM Cloud Operator can be found on operatorhub.io . It is currently in the alpha channel. It may not appear in the OperatorHub Console in OpenShift right away after a new cluster is provisioned. There is post-provisioning configuration that will update the OperatorHub Console. If you don't see it right away and this is a new cluster, please wait awhile and look again. For more information see the Operators page in this wiki. Provision OpenShift on IBM Cloud As of June 16, 2020 OpenShift on IBM Cloud is now generally available on VPC Gen 2! It is now possible to provision a cluster on VPC Gen 2 with only private endpoints and a publicly resolvable but only privately accessible ingress subdomain. When provisioning with the CLI or Terraform use the disable_public_service_endpoint to provision the cluster with private only networking. Worker pools are required to have a minimum count of 2 nodes per availability zone. This change was made during the Early Access period, but really applies to all OpenShift clusters. The UI, CLI and Terraform will all enforce a minimum worker_count of 2. One other change that occurred during the Early Access period is that the storage configuration for the OpenShift internal registry was updated to use Cloud Object Storage. Prior to that the configuration was set to emptyDir: {} , which used ephemeral storage on the worker nodes. Images would be stored on the nodes, and if the node was rebooted or replaced the images would be lost. With the GA release cluster provisioning requires you to provide a valid CRN for a Cloud Object Storage instance. The docs have been updated to reflect new user access permissions required for Cloud Object Storage. When the cluster is provisioned it will perform the following additional actions: Create a new bucket in the COS instance (US-Geo, Standard tier) Create Service Credentials (including HMAC) Configure the Image Registry to use the bucket for storage of images. If for any reason these steps fail and the cluster is not able to access the instance or the bucket it will \"fail gracefully\" and revert back to the emptyDir: {} configuration. If that happens there are instructions in the troubleshooting docs for how to manually configure the cluster to use a bucket once the problems have been resolved. Provisioning with Terraform The IBM Terraform Provider now supports OpenShift on IBM Cloud in VPC Gen 2 as of v1.8.0 . It includes support for the new cos_instance_crn parameter that is required for OpenShift clusters. Examples for provisioning a cluster can be found here . Here is example terraform that was used to perform validation testing: Provider variable \"ibmcloud_api_key\" {} provider \"ibm\" { generation = var.generation region = var.region version = \"~> 1.8\" ibmcloud_api_key = var.ibmcloud_api_key } Variables variable \"environment\" { default = \"sandbox\" } variable \"vpc_name\" { default = \"sandbox-dallas\" } variable \"vpc_resource_group\" { default = \"vpc-sandbox\" } variable \"adm_resource_group\" { default = \"account-admin-services\" } variable \"env_resource_group\" { default = \"sandbox-tf-env\" } variable \"region\" { default = \"us-south\" } variable \"generation\" { default = 2 } Main data \"ibm_resource_group\" \"vpc_resource_group\" { name = \"${var.vpc_resource_group}\" } data \"ibm_resource_group\" \"adm_resource_group\" { name = \"${var.adm_resource_group}\" } data \"ibm_resource_group\" \"env_resource_group\" { name = \"${var.env_resource_group}\" } data \"ibm_resource_group\" \"cos_group\" { name = \"roks-cos-test-deleteme\" } data \"ibm_resource_instance\" \"cos_instance\" { name = \"cos-roks-internal-registry-test\" resource_group_id = data.ibm_resource_group.cos_group.id service = \"cloud-object-storage\" } ############################################################################## # Create OCP Cluster ############################################################################## resource \"ibm_container_vpc_cluster\" \"app_cluster2\" { name = \"${var.environment}-tf-02\" vpc_id = \"r006-a1705bea-d7ab-429c-8e17-00d21c6ffe83\" flavor = \"bx2.4x16\" kube_version = \"4.3_openshift\" worker_count = \"2\" entitlement = \"cloud_pak\" wait_till = \"MasterNodeReady\" disable_public_service_endpoint = false cos_instance_crn = data.ibm_resource_instance.cos_instance.id resource_group_id = data.ibm_resource_group.env_resource_group.id tags = [\"env:${var.environment}\",\"vpc:${var.vpc_name}\"] zones { subnet_id = \"0717-c63180ac-2765-4f91-b184-9373fb1514f2\" name = \"${var.region}-1\" } } Remove items from OpenShift Catalog This came up in a discussion with KP where we landed on the \"Add\" page in the Developer view of the OpenShift Console. If you click on \"From Catalog\" or \"Database\" you get taken to a Developer Catalog. Kaiser Permanente would like to completely remove the Developer Catalog, or at least remove all of the items in it. The items that show up in the \"From Catalog\" image above are controlled by the Samples Operator . Control over what shows up can be managed using Samples Operator configuration parameters . Specifically, if the managementState parameter is changed from Managed to Removed the set of managed imagestreams and templates in the openshift namespace are removed from the catalog. Using the OpenShift Console You can access the Samples Operator configuration using these instructions . It can also be viewed and changed from the UI using the Administrator -> Administration -> Custom Resource Definitions panel. Click on the Config CRD for samples.operator.openshift.io . Go to the Instances tab and click on cluster . Go to the YAML tab and change the managementState parameter from Managed to Removed : Click Save. Then you can go back to the Developer view, click Add and click on the From Catalog tile. The panel should be empty. A variation of this option would be to set the skippedTemplates parameter. This parameter tells the operator to ignore the templates in the list provided as the parameter's value. This is an easy way to drop a few charts from the catalog. The configuration above (setting managedState to Removed ) only affects the templates managed by the Samples operator. It will ignore any templates that a cluster administrator has added. For this reason, if there is need to remove most of the ones in the catalog but leave a few in, you can export the charts you want to keep, then set the managedState to Removed , then import back in the templates that you want to add back in. To export templates go to the openshift project in the Administrator view and search for Template Custom Resources. Click on the template you want to export, go to the YAML tab and copy/paste the yaml to a file. To do it with oc target the openshift project and run this command (using 3scale-gateway as an example): oc get template 3scale-gateway -o yaml > 3scale-gateway.yaml Using the CLI or Ansible Here is the patch command to use. I created a file called disablecatalog.yml with this content: spec: managementState: Removed Then I made sure I was logged into oc in a terminal and ran this command: oc patch configs.samples.operator.openshift.io/cluster --type merge --patch \"$(cat disablecatalog.yml)\" The command will return this: config.samples.operator.openshift.io/cluster patched After a minute or two the operator will finish removing the content and it will be gone from the From Catalog page. Note: Even with the tiles removed from the Samples catalog there may still be tiles for any operators that are installed in the cluster. See my comments above for more information about the object that gets patched. This Ansible playbook will also apply the patch. Forwarding logs to external LogStash (on-premise) These instructions can be run from a command line using oc authenticated to the target cluster. They were derived from the OpenShift documentation for installing the Cluster Logging Operator using the CLI . Here is an ansible playbook that was also used during testing. Create the namespace for logging Create a file named logging_namespace.yml with the following content: apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" Run this command to create the namespace: oc create -f logging_namespace.yml Create the Operator Group for the Cluster Logging Operator Create a file named og-clo.yml with the following content: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging Run this command: oc create -f og-clo.yml Install the Cluster Logging Operator Create a file named clo_subscription.yml with this content: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"4.3\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace Run this command: oc create -f clo_subscription.yml At this point the Cluster Logging Operator should be installed. Verify that the installation succeeded by viewing the Installed Operators page in the openshift-logging project in the web console. Create the ClusterLogging Custom Resource This custom resource tells the operator how to configure cluster logging. A \"full stack\" configuration includes a highly-available ElasticSearch cluster, fluentd and Kibana for viewing logs. Since Kaiser Permanente is going to be shipping logs on-premise to Splunk (via LogStash) the full stack is not required. Create a file named clusterlogging.yml with the following content: apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging annotations: clusterlogging.openshift.io/logforwardingtechpreview: enabled spec: managementState: Managed logStore: type: elasticsearch elasticsearch: # no elasticsearch pods nodeCount: 0 visualization: type: kibana kibana: # no kibana pods replicas: 0 curation: type: curator curator: # schedule the cron job to never run curator schedule: \"* * 31 2 *\" collection: logs: type: fluentd fluentd: {} Note: This configuration was originally tested with the expectation that at least one ElasticSearch pod would be required. The settings above in the ElasticSearch section would create a pod using the minimum CPU, memory and storage that would still run. During testing it was discovered that this configuration would work without ElasticSearch at all, so the nodeCount is set to 0, but the minimal configuration is still included for reference. This configuration will not install or configure ElasticSearch, and it will create a Kibana deployment with 0 replicas. Run this command: oc create -f clusterlogging.yml Configure Log Forwarding At this point the fluentd daemonset has been created and fluentd pods are running but they have nowhere to send the logs. The LogForwarding custom resource provides the additional configuration to set up log forwarding to LogStash. When you create the LogForwarding custom resource in this step, the fluentd pods will automatically be deleted and recreated by the operator to pick up the changes. The same thing happens if you delete the LogForwarding custom resource. Create a file named logforwarding.yml with the following content: NOTE: Be sure to update the endpoint parameter with the url for the on-premise logstash server. apiVersion: logging.openshift.io/v1alpha1 kind: LogForwarding metadata: name: instance namespace: openshift-logging spec: disableDefaultForwarding: true outputs: - endpoint: 'host:port' name: insecureforward type: forward pipelines: - inputSource: logs.app name: container-logs outputRefs: - insecureforward Disclaimer: This is a test configuration that uses insecure communication between the fluentd pods and the logstash server. Be sure to update the configuration to do secure forwarding. Details can be found in the OpenShift documentation . Run this command: oc create -f logforwarding.yml Once the fluentd pods restart the logs will start flowing to the external server.","title":"OpenShift on IBM Cloud"},{"location":"cloud-platform/openshift-ibm-cloud/#openshift-on-ibm-cloud","text":"This page will contain details and documentation about OpenShift 4.x on IBM Cloud as work progresses.","title":"OpenShift on IBM Cloud"},{"location":"cloud-platform/openshift-ibm-cloud/#base-images","text":"Many IBM Cloud Dedicated customers will be primarily using two images that correspond to two commonly used buildpacks in Cloud Foundry today: WebSphere Liberty - Both the standard liberty and open liberty images are available now. The current version is 19.0.0.2, with many older versions available in the repo. The current version of the buildpack also uses 19.0.0.2. Node.js - IBM is no longer delivering builds of Node.js for buildpacks and is not delivering any images; instead IBM is providing support for the community buid. An IBM Team (the Cloud Engagement Hub) has built a guide for customizing images prior to loading them into an enterprise registry: https://medium.com/cloud-engagement-hub/so-you-want-customized-websphere-container-images-heres-how-42f0e598733f","title":"Base Images"},{"location":"cloud-platform/openshift-ibm-cloud/#operators","text":"","title":"Operators"},{"location":"cloud-platform/openshift-ibm-cloud/#using-the-ibm-cloud-operator","text":"The IBM Cloud Operator is used to create services in IBM Cloud (i.e. Cloudant, PostGreSQL, Redis, etc.) and bindings in an OpenShift project that contain the credentials needed to programmatically access the service. The IBM Cloud Operator can be found on operatorhub.io . It is currently in the alpha channel. It may not appear in the OperatorHub Console in OpenShift right away after a new cluster is provisioned. There is post-provisioning configuration that will update the OperatorHub Console. If you don't see it right away and this is a new cluster, please wait awhile and look again. For more information see the Operators page in this wiki.","title":"Using the IBM Cloud Operator"},{"location":"cloud-platform/openshift-ibm-cloud/#provision-openshift-on-ibm-cloud","text":"As of June 16, 2020 OpenShift on IBM Cloud is now generally available on VPC Gen 2! It is now possible to provision a cluster on VPC Gen 2 with only private endpoints and a publicly resolvable but only privately accessible ingress subdomain. When provisioning with the CLI or Terraform use the disable_public_service_endpoint to provision the cluster with private only networking. Worker pools are required to have a minimum count of 2 nodes per availability zone. This change was made during the Early Access period, but really applies to all OpenShift clusters. The UI, CLI and Terraform will all enforce a minimum worker_count of 2. One other change that occurred during the Early Access period is that the storage configuration for the OpenShift internal registry was updated to use Cloud Object Storage. Prior to that the configuration was set to emptyDir: {} , which used ephemeral storage on the worker nodes. Images would be stored on the nodes, and if the node was rebooted or replaced the images would be lost. With the GA release cluster provisioning requires you to provide a valid CRN for a Cloud Object Storage instance. The docs have been updated to reflect new user access permissions required for Cloud Object Storage. When the cluster is provisioned it will perform the following additional actions: Create a new bucket in the COS instance (US-Geo, Standard tier) Create Service Credentials (including HMAC) Configure the Image Registry to use the bucket for storage of images. If for any reason these steps fail and the cluster is not able to access the instance or the bucket it will \"fail gracefully\" and revert back to the emptyDir: {} configuration. If that happens there are instructions in the troubleshooting docs for how to manually configure the cluster to use a bucket once the problems have been resolved.","title":"Provision OpenShift on IBM Cloud"},{"location":"cloud-platform/openshift-ibm-cloud/#provisioning-with-terraform","text":"The IBM Terraform Provider now supports OpenShift on IBM Cloud in VPC Gen 2 as of v1.8.0 . It includes support for the new cos_instance_crn parameter that is required for OpenShift clusters. Examples for provisioning a cluster can be found here . Here is example terraform that was used to perform validation testing: Provider variable \"ibmcloud_api_key\" {} provider \"ibm\" { generation = var.generation region = var.region version = \"~> 1.8\" ibmcloud_api_key = var.ibmcloud_api_key } Variables variable \"environment\" { default = \"sandbox\" } variable \"vpc_name\" { default = \"sandbox-dallas\" } variable \"vpc_resource_group\" { default = \"vpc-sandbox\" } variable \"adm_resource_group\" { default = \"account-admin-services\" } variable \"env_resource_group\" { default = \"sandbox-tf-env\" } variable \"region\" { default = \"us-south\" } variable \"generation\" { default = 2 } Main data \"ibm_resource_group\" \"vpc_resource_group\" { name = \"${var.vpc_resource_group}\" } data \"ibm_resource_group\" \"adm_resource_group\" { name = \"${var.adm_resource_group}\" } data \"ibm_resource_group\" \"env_resource_group\" { name = \"${var.env_resource_group}\" } data \"ibm_resource_group\" \"cos_group\" { name = \"roks-cos-test-deleteme\" } data \"ibm_resource_instance\" \"cos_instance\" { name = \"cos-roks-internal-registry-test\" resource_group_id = data.ibm_resource_group.cos_group.id service = \"cloud-object-storage\" } ############################################################################## # Create OCP Cluster ############################################################################## resource \"ibm_container_vpc_cluster\" \"app_cluster2\" { name = \"${var.environment}-tf-02\" vpc_id = \"r006-a1705bea-d7ab-429c-8e17-00d21c6ffe83\" flavor = \"bx2.4x16\" kube_version = \"4.3_openshift\" worker_count = \"2\" entitlement = \"cloud_pak\" wait_till = \"MasterNodeReady\" disable_public_service_endpoint = false cos_instance_crn = data.ibm_resource_instance.cos_instance.id resource_group_id = data.ibm_resource_group.env_resource_group.id tags = [\"env:${var.environment}\",\"vpc:${var.vpc_name}\"] zones { subnet_id = \"0717-c63180ac-2765-4f91-b184-9373fb1514f2\" name = \"${var.region}-1\" } }","title":"Provisioning with Terraform"},{"location":"cloud-platform/openshift-ibm-cloud/#remove-items-from-openshift-catalog","text":"This came up in a discussion with KP where we landed on the \"Add\" page in the Developer view of the OpenShift Console. If you click on \"From Catalog\" or \"Database\" you get taken to a Developer Catalog. Kaiser Permanente would like to completely remove the Developer Catalog, or at least remove all of the items in it. The items that show up in the \"From Catalog\" image above are controlled by the Samples Operator . Control over what shows up can be managed using Samples Operator configuration parameters . Specifically, if the managementState parameter is changed from Managed to Removed the set of managed imagestreams and templates in the openshift namespace are removed from the catalog.","title":"Remove items from OpenShift Catalog"},{"location":"cloud-platform/openshift-ibm-cloud/#using-the-openshift-console","text":"You can access the Samples Operator configuration using these instructions . It can also be viewed and changed from the UI using the Administrator -> Administration -> Custom Resource Definitions panel. Click on the Config CRD for samples.operator.openshift.io . Go to the Instances tab and click on cluster . Go to the YAML tab and change the managementState parameter from Managed to Removed : Click Save. Then you can go back to the Developer view, click Add and click on the From Catalog tile. The panel should be empty. A variation of this option would be to set the skippedTemplates parameter. This parameter tells the operator to ignore the templates in the list provided as the parameter's value. This is an easy way to drop a few charts from the catalog. The configuration above (setting managedState to Removed ) only affects the templates managed by the Samples operator. It will ignore any templates that a cluster administrator has added. For this reason, if there is need to remove most of the ones in the catalog but leave a few in, you can export the charts you want to keep, then set the managedState to Removed , then import back in the templates that you want to add back in. To export templates go to the openshift project in the Administrator view and search for Template Custom Resources. Click on the template you want to export, go to the YAML tab and copy/paste the yaml to a file. To do it with oc target the openshift project and run this command (using 3scale-gateway as an example): oc get template 3scale-gateway -o yaml > 3scale-gateway.yaml","title":"Using the OpenShift Console"},{"location":"cloud-platform/openshift-ibm-cloud/#using-the-cli-or-ansible","text":"Here is the patch command to use. I created a file called disablecatalog.yml with this content: spec: managementState: Removed Then I made sure I was logged into oc in a terminal and ran this command: oc patch configs.samples.operator.openshift.io/cluster --type merge --patch \"$(cat disablecatalog.yml)\" The command will return this: config.samples.operator.openshift.io/cluster patched After a minute or two the operator will finish removing the content and it will be gone from the From Catalog page. Note: Even with the tiles removed from the Samples catalog there may still be tiles for any operators that are installed in the cluster. See my comments above for more information about the object that gets patched. This Ansible playbook will also apply the patch.","title":"Using the CLI or Ansible"},{"location":"cloud-platform/openshift-ibm-cloud/#forwarding-logs-to-external-logstash-on-premise","text":"These instructions can be run from a command line using oc authenticated to the target cluster. They were derived from the OpenShift documentation for installing the Cluster Logging Operator using the CLI . Here is an ansible playbook that was also used during testing.","title":"Forwarding logs to external LogStash (on-premise)"},{"location":"cloud-platform/openshift-ibm-cloud/#create-the-namespace-for-logging","text":"Create a file named logging_namespace.yml with the following content: apiVersion: v1 kind: Namespace metadata: name: openshift-logging annotations: openshift.io/node-selector: \"\" labels: openshift.io/cluster-monitoring: \"true\" Run this command to create the namespace: oc create -f logging_namespace.yml","title":"Create the namespace for logging"},{"location":"cloud-platform/openshift-ibm-cloud/#create-the-operator-group-for-the-cluster-logging-operator","text":"Create a file named og-clo.yml with the following content: apiVersion: operators.coreos.com/v1 kind: OperatorGroup metadata: name: cluster-logging namespace: openshift-logging spec: targetNamespaces: - openshift-logging Run this command: oc create -f og-clo.yml","title":"Create the Operator Group for the Cluster Logging Operator"},{"location":"cloud-platform/openshift-ibm-cloud/#install-the-cluster-logging-operator","text":"Create a file named clo_subscription.yml with this content: apiVersion: operators.coreos.com/v1alpha1 kind: Subscription metadata: name: cluster-logging namespace: openshift-logging spec: channel: \"4.3\" name: cluster-logging source: redhat-operators sourceNamespace: openshift-marketplace Run this command: oc create -f clo_subscription.yml At this point the Cluster Logging Operator should be installed. Verify that the installation succeeded by viewing the Installed Operators page in the openshift-logging project in the web console.","title":"Install the Cluster Logging Operator"},{"location":"cloud-platform/openshift-ibm-cloud/#create-the-clusterlogging-custom-resource","text":"This custom resource tells the operator how to configure cluster logging. A \"full stack\" configuration includes a highly-available ElasticSearch cluster, fluentd and Kibana for viewing logs. Since Kaiser Permanente is going to be shipping logs on-premise to Splunk (via LogStash) the full stack is not required. Create a file named clusterlogging.yml with the following content: apiVersion: logging.openshift.io/v1 kind: ClusterLogging metadata: name: instance namespace: openshift-logging annotations: clusterlogging.openshift.io/logforwardingtechpreview: enabled spec: managementState: Managed logStore: type: elasticsearch elasticsearch: # no elasticsearch pods nodeCount: 0 visualization: type: kibana kibana: # no kibana pods replicas: 0 curation: type: curator curator: # schedule the cron job to never run curator schedule: \"* * 31 2 *\" collection: logs: type: fluentd fluentd: {} Note: This configuration was originally tested with the expectation that at least one ElasticSearch pod would be required. The settings above in the ElasticSearch section would create a pod using the minimum CPU, memory and storage that would still run. During testing it was discovered that this configuration would work without ElasticSearch at all, so the nodeCount is set to 0, but the minimal configuration is still included for reference. This configuration will not install or configure ElasticSearch, and it will create a Kibana deployment with 0 replicas. Run this command: oc create -f clusterlogging.yml","title":"Create the ClusterLogging Custom Resource"},{"location":"cloud-platform/openshift-ibm-cloud/#configure-log-forwarding","text":"At this point the fluentd daemonset has been created and fluentd pods are running but they have nowhere to send the logs. The LogForwarding custom resource provides the additional configuration to set up log forwarding to LogStash. When you create the LogForwarding custom resource in this step, the fluentd pods will automatically be deleted and recreated by the operator to pick up the changes. The same thing happens if you delete the LogForwarding custom resource. Create a file named logforwarding.yml with the following content: NOTE: Be sure to update the endpoint parameter with the url for the on-premise logstash server. apiVersion: logging.openshift.io/v1alpha1 kind: LogForwarding metadata: name: instance namespace: openshift-logging spec: disableDefaultForwarding: true outputs: - endpoint: 'host:port' name: insecureforward type: forward pipelines: - inputSource: logs.app name: container-logs outputRefs: - insecureforward Disclaimer: This is a test configuration that uses insecure communication between the fluentd pods and the logstash server. Be sure to update the configuration to do secure forwarding. Details can be found in the OpenShift documentation . Run this command: oc create -f logforwarding.yml Once the fluentd pods restart the logs will start flowing to the external server.","title":"Configure Log Forwarding"},{"location":"custom-domain/create-domain/","text":"Custom Domains Creating your own domain A domain is created by using one of the many services called Registrars that will do all of the things that are required for your domain. Specifically they will be the custodian of your domain with all of the appropriate information about you. You can use any registrar you like; there are many of them out there. IBM provides this capability with IBM Cloud. This document will use IBM Cloud as the registrar. To create your domain: Login to IBM Cloud and navigate to the catalog. Search for \"domain\". Click on the Domain Name Service tile as seen in the image above. On this page click the blue Create button. Expand the Register menu, enter your domain and click the Check Availability button, as shown in the image above. If your domain is available, click Continue . Provide the required information as shown in the screen above and click Order Now . You should see a confirmation window: Click OK . Then click on the refresh icon in the list menu. You can see your new domain! Notice that it's not \"verified\" yet. You will get an email from Softlayer that confirms that you've ordered your domain. The email will look something like this: After some time (could be minutes) Softlayer does things to verify your domain. Wait a few minutes and refresh the domain list. When your domain is verified you will see the Verified column change, as shown below: That's it! Your domain is now ready for use.","title":"Create a domain"},{"location":"custom-domain/create-domain/#custom-domains","text":"","title":"Custom Domains"},{"location":"custom-domain/create-domain/#creating-your-own-domain","text":"A domain is created by using one of the many services called Registrars that will do all of the things that are required for your domain. Specifically they will be the custodian of your domain with all of the appropriate information about you. You can use any registrar you like; there are many of them out there. IBM provides this capability with IBM Cloud. This document will use IBM Cloud as the registrar. To create your domain: Login to IBM Cloud and navigate to the catalog. Search for \"domain\". Click on the Domain Name Service tile as seen in the image above. On this page click the blue Create button. Expand the Register menu, enter your domain and click the Check Availability button, as shown in the image above. If your domain is available, click Continue . Provide the required information as shown in the screen above and click Order Now . You should see a confirmation window: Click OK . Then click on the refresh icon in the list menu. You can see your new domain! Notice that it's not \"verified\" yet. You will get an email from Softlayer that confirms that you've ordered your domain. The email will look something like this: After some time (could be minutes) Softlayer does things to verify your domain. Wait a few minutes and refresh the domain list. When your domain is verified you will see the Verified column change, as shown below: That's it! Your domain is now ready for use.","title":"Creating your own domain"},{"location":"custom-domain/domain-with-openshift/","text":"Using your custom domain with OpenShift Now that you have your spiffy new custom domain, let's put it to work! The steps documented here assume that you have done the following: Registered your custom domain Provisioned an instance of IBM Cloud Internet Services and configured it to manage your domain Ordered origin certificates in IBM Cloud Internet Services Ordered edge certificates in IBM Cloud Internet Services I used this page in the OpenShift docs to create the secured route for my application. Creating an edge route with a custom certificate","title":"Configure domain for OpenShift"},{"location":"custom-domain/domain-with-openshift/#using-your-custom-domain-with-openshift","text":"Now that you have your spiffy new custom domain, let's put it to work! The steps documented here assume that you have done the following: Registered your custom domain Provisioned an instance of IBM Cloud Internet Services and configured it to manage your domain Ordered origin certificates in IBM Cloud Internet Services Ordered edge certificates in IBM Cloud Internet Services I used this page in the OpenShift docs to create the secured route for my application. Creating an edge route with a custom certificate","title":"Using your custom domain with OpenShift"},{"location":"custom-domain/manage-domain/","text":"Manage your domain with IBM Cloud Internet Services If you have your own domain you can manage it using IBM Cloud Internet Services. This service provides a number of management, reliability, performance and security features for your domain. You can learn more about IBM Cloud Internet Services here . Provision Cloud Internet Services The first steps to using Cloud Internet Services is to provision an instance of it in your IBM Cloud account. Login to IBM Cloud, navigate to the catalog and search for \"internet\". Click on the tile for Internet Services. Select the Standard plan, scroll down and provide a name for your instance and a resource group, then click Create . You will see the screen below, where you will add your domain to your CIS instance. Click on the Let's Get Started button to continue. Add your domain to Cloud Internet Services You should seen the screen below. Enter your domain in the screen above and click Connect and Continue . If you have DNS records in your existing DNS provider you can export them and import them here. In this case we don't have any existing records, so click Next Step . The next step is VERY IMPORTANT! It tells your registrar (where you registered your domain) to delegate to CIS for name servers. You need to update the regisrar with the new name servers in the New NS records table above. In the case where you used Softlayer to register your domain, here are the steps to follow. Go to Classic Infrastructure in IBM Cloud. Expand the Services menu on the left and click on \"Domain Registration\". On the page below, click the arrow to the left of your domain name to expand it. Make sure the \"Lock Domain\" dropdown is set to \"Unlocked\" and click the \"Add / Edit DNS\" link for your domain. Replace the two existing name servers with the two from your CIS domain. When you first click the link the screen will look like this: When you replace these name servers with new ones it will look something like this: Note: Your name servers will be different, but they should match what you have on the CIS screen below. If they do, click \"Associate\" on the screen above. Go back to the CIS screen. Click \"Check Name Servers\" to validate that your name servers were updated. You might get a warning saying that the name servers were not updated. This is okay; it takes a few minutes for the updates to propagate. If that happens, refresh the page and try again in a few minutes. You may also get an error saying that \"you can only perform this action once per hour\". This is also okay. If you refresh the page again you will eventually see that your domain has become active. When that happens it means that now Cloud Internet Services is managing your domain! There is one more step that you should do at this point. By default, the Web Application Firewall is not enabled for your domain. To enable it, click on the Web Application Firewall link on the overview page and move the slider to the right. Adding DNS records To add a DNS record for your application you will create either an A or CNAME record. An A record maps your application to a specific IP address, whereas a CNAME record creates a mapping from your domain to another hostname. When you use a CNAME record you don't need to know the specific address, nor do you need to know if it ever changes. When you create an kubernetes cluster in the IBM Cloud Kubernetes Service (IKS), or an IBM Cloud Foundry Enterprise Environment (which is built on IKS), an ingress subdomain is created automatically for you and is managed by IKS. When you create your own host name, say myapp.eyebeemdemos.com , and that application is running in Cloud Foundry Enterprise, you will need to create a CNAME record for it in Cloud Internet Services. To add a CNAME record for you application, go to the Overview page for your Cloud Internet Services instance. Click on the link for DNS Records . Scroll down to the DNS Records section. Change the record type to CNAME , enter your app name in the Name field and the ingress subdomain for your Cloud Foundry Enterprise cluster in the Alias Domain Name field. Click the Add Record button. You will see your new record show up in the list. To enable the security features (ie. TLS) you need to enable the proxy by clicking on the slider. Now your CNAME record has been added, but it may still take a few minutes for it to be available, as the DNS record needs to be propagated out to all the other DNS servers. When it is ready you will be able to ping it. Note The address you see when you ping your hostname is actually an IP address for your Cloud Internet Services instance. All traffic gets routed there first so that security features can be applied. This also includes the TLS-enabled connection between Cloud Internet Services and your Cloud Foundry Enterprise cluster. Ordering Origin Certificates Ordering Edge Certificates To properly encrypt traffic between your domain in CIS and the browser you need to have edge certificates. Without them the browser will not trust your application because it did not present a certificate signed by a trusted authority. Edge certificates are signed by a trusted authority. To order edge certificates for your domain in IBM Cloud Internet Services, go to the Security -> TLS pane in the left nav, then click the button on the right to order a certificate. Note You can add multiple subdomains to the certificate when you order it. This will allow you to use a single certificate for all of the subdomains you add to it. You can always order another certificate if you add subdomains later, but you cannot add them to an existing certificate. Once you have completed the order you will see them listed: You can view the details of the certificate by clicking the three dots to the right. The details include a list of all of the subdomains covered by the certificate. Here is what the TLS page should look like: Configuring your domain in Cloud Foundry Enterprise Setting up your custom domain for use in Cloud Foundry Enterprise is pretty simple. You will want to order an origin certificate for your domain in Cloud Internet Services, as shown in the previous section. This will allow you to use TLS for communication from Internet Services to your Cloud Foundry Enterprise cluster. You will also need a DNS CNAME record for your application so that Internet Services will route traffic to your Cloud Foundry Enterprise cluster. The diagram below shows the basic configuration for using your custom domain. The Edge Services box on the left is your Cloud Internet Services instance. It contains the origin certificates you ordered above, and the DNS CNAME record that maps your hostname, myapp.<your-domain>.com to the ingress subdomain that was created by your Cloud Foundry Enterprise cluster. The ingress subdomain is how all traffic gets routed to your cluster.","title":"Manage your domain"},{"location":"custom-domain/manage-domain/#manage-your-domain-with-ibm-cloud-internet-services","text":"If you have your own domain you can manage it using IBM Cloud Internet Services. This service provides a number of management, reliability, performance and security features for your domain. You can learn more about IBM Cloud Internet Services here .","title":"Manage your domain with IBM Cloud Internet Services"},{"location":"custom-domain/manage-domain/#provision-cloud-internet-services","text":"The first steps to using Cloud Internet Services is to provision an instance of it in your IBM Cloud account. Login to IBM Cloud, navigate to the catalog and search for \"internet\". Click on the tile for Internet Services. Select the Standard plan, scroll down and provide a name for your instance and a resource group, then click Create . You will see the screen below, where you will add your domain to your CIS instance. Click on the Let's Get Started button to continue.","title":"Provision Cloud Internet Services"},{"location":"custom-domain/manage-domain/#add-your-domain-to-cloud-internet-services","text":"You should seen the screen below. Enter your domain in the screen above and click Connect and Continue . If you have DNS records in your existing DNS provider you can export them and import them here. In this case we don't have any existing records, so click Next Step . The next step is VERY IMPORTANT! It tells your registrar (where you registered your domain) to delegate to CIS for name servers. You need to update the regisrar with the new name servers in the New NS records table above. In the case where you used Softlayer to register your domain, here are the steps to follow. Go to Classic Infrastructure in IBM Cloud. Expand the Services menu on the left and click on \"Domain Registration\". On the page below, click the arrow to the left of your domain name to expand it. Make sure the \"Lock Domain\" dropdown is set to \"Unlocked\" and click the \"Add / Edit DNS\" link for your domain. Replace the two existing name servers with the two from your CIS domain. When you first click the link the screen will look like this: When you replace these name servers with new ones it will look something like this: Note: Your name servers will be different, but they should match what you have on the CIS screen below. If they do, click \"Associate\" on the screen above. Go back to the CIS screen. Click \"Check Name Servers\" to validate that your name servers were updated. You might get a warning saying that the name servers were not updated. This is okay; it takes a few minutes for the updates to propagate. If that happens, refresh the page and try again in a few minutes. You may also get an error saying that \"you can only perform this action once per hour\". This is also okay. If you refresh the page again you will eventually see that your domain has become active. When that happens it means that now Cloud Internet Services is managing your domain! There is one more step that you should do at this point. By default, the Web Application Firewall is not enabled for your domain. To enable it, click on the Web Application Firewall link on the overview page and move the slider to the right.","title":"Add your domain to Cloud Internet Services"},{"location":"custom-domain/manage-domain/#adding-dns-records","text":"To add a DNS record for your application you will create either an A or CNAME record. An A record maps your application to a specific IP address, whereas a CNAME record creates a mapping from your domain to another hostname. When you use a CNAME record you don't need to know the specific address, nor do you need to know if it ever changes. When you create an kubernetes cluster in the IBM Cloud Kubernetes Service (IKS), or an IBM Cloud Foundry Enterprise Environment (which is built on IKS), an ingress subdomain is created automatically for you and is managed by IKS. When you create your own host name, say myapp.eyebeemdemos.com , and that application is running in Cloud Foundry Enterprise, you will need to create a CNAME record for it in Cloud Internet Services. To add a CNAME record for you application, go to the Overview page for your Cloud Internet Services instance. Click on the link for DNS Records . Scroll down to the DNS Records section. Change the record type to CNAME , enter your app name in the Name field and the ingress subdomain for your Cloud Foundry Enterprise cluster in the Alias Domain Name field. Click the Add Record button. You will see your new record show up in the list. To enable the security features (ie. TLS) you need to enable the proxy by clicking on the slider. Now your CNAME record has been added, but it may still take a few minutes for it to be available, as the DNS record needs to be propagated out to all the other DNS servers. When it is ready you will be able to ping it. Note The address you see when you ping your hostname is actually an IP address for your Cloud Internet Services instance. All traffic gets routed there first so that security features can be applied. This also includes the TLS-enabled connection between Cloud Internet Services and your Cloud Foundry Enterprise cluster.","title":"Adding DNS records"},{"location":"custom-domain/manage-domain/#ordering-origin-certificates","text":"","title":"Ordering Origin Certificates"},{"location":"custom-domain/manage-domain/#ordering-edge-certificates","text":"To properly encrypt traffic between your domain in CIS and the browser you need to have edge certificates. Without them the browser will not trust your application because it did not present a certificate signed by a trusted authority. Edge certificates are signed by a trusted authority. To order edge certificates for your domain in IBM Cloud Internet Services, go to the Security -> TLS pane in the left nav, then click the button on the right to order a certificate. Note You can add multiple subdomains to the certificate when you order it. This will allow you to use a single certificate for all of the subdomains you add to it. You can always order another certificate if you add subdomains later, but you cannot add them to an existing certificate. Once you have completed the order you will see them listed: You can view the details of the certificate by clicking the three dots to the right. The details include a list of all of the subdomains covered by the certificate. Here is what the TLS page should look like:","title":"Ordering Edge Certificates"},{"location":"custom-domain/manage-domain/#configuring-your-domain-in-cloud-foundry-enterprise","text":"Setting up your custom domain for use in Cloud Foundry Enterprise is pretty simple. You will want to order an origin certificate for your domain in Cloud Internet Services, as shown in the previous section. This will allow you to use TLS for communication from Internet Services to your Cloud Foundry Enterprise cluster. You will also need a DNS CNAME record for your application so that Internet Services will route traffic to your Cloud Foundry Enterprise cluster. The diagram below shows the basic configuration for using your custom domain. The Edge Services box on the left is your Cloud Internet Services instance. It contains the origin certificates you ordered above, and the DNS CNAME record that maps your hostname, myapp.<your-domain>.com to the ingress subdomain that was created by your Cloud Foundry Enterprise cluster. The ingress subdomain is how all traffic gets routed to your cluster.","title":"Configuring your domain in Cloud Foundry Enterprise"},{"location":"dev-tools/crw/","text":"CodeReady Workspaces Overview One key tool that simplifies the life of a developer is CodeReady Workspaces . This section describes the steps to install and configure CodeReady Workspaces in OpenShift in IBM Cloud . It assumes that you already have a cluster, and that it is provisioned in VPC. Steps: Access the OpenShift console, access the OAuth CRD and add an identity provider. Use Basic Authentication and use a fake URL, like https://doesnotexist.io. Install CRW using the operator. Let it use openshift-workspaces as the project. Configure the Che Cluster. Change the PVC claim size from 1Gi to 10Gi. This is because the minimum volume size in VPC is 10GB. VPC does not support Read Write Many (RWX) so the same","title":"CodeReady Workspaces"},{"location":"dev-tools/crw/#codeready-workspaces","text":"","title":"CodeReady Workspaces"},{"location":"dev-tools/crw/#overview","text":"One key tool that simplifies the life of a developer is CodeReady Workspaces . This section describes the steps to install and configure CodeReady Workspaces in OpenShift in IBM Cloud . It assumes that you already have a cluster, and that it is provisioned in VPC. Steps: Access the OpenShift console, access the OAuth CRD and add an identity provider. Use Basic Authentication and use a fake URL, like https://doesnotexist.io. Install CRW using the operator. Let it use openshift-workspaces as the project. Configure the Che Cluster. Change the PVC claim size from 1Gi to 10Gi. This is because the minimum volume size in VPC is 10GB. VPC does not support Read Write Many (RWX) so the same","title":"Overview"},{"location":"dev-tools/dev-experience/","text":"The Developer Experience Cloud native development ushers in a new set of tools, technologies and practices for developing applications more quickly and higher levels of quality and security.","title":"Overview"},{"location":"dev-tools/dev-experience/#the-developer-experience","text":"Cloud native development ushers in a new set of tools, technologies and practices for developing applications more quickly and higher levels of quality and security.","title":"The Developer Experience"},{"location":"disaster-recovery/disaster-recovery/","text":"Disaster Recovery Overview The approach to defining the disaster recover strategy needs to be systematic, and start with the \"application\", which is defined as a set of compute resources (IKS or OpenShift apps, VSIs, services, etc.) that make up a \"business application\". While a holistic approach may be desired, the reality is that each business application is independent, with its own RTO/RPO requirements, which for many customers is expressed in the form of a set of service classes or tiers. Resiliency Tier Recovery Description Recovery Time Objective Recovery Point Objective Tier 1 Continuous Availabilty <= 1 Hour <= 1 Hour Tier 2 Advanced Recovery > 1 Hrs - <= 24 Hrs <2 Hrs - <24 Hrs Tier 3 Standard Recovery > 24 Hrs - <= 72 Hrs Last Backup Tier 4 No Recovery N/A N/A Note: The IBM Cloud platform itself must support the LOWEST number in the range. Therefore, for the majority of their applications (Tier 1 and Tier 2), the IBM Cloud services (OpenShift, IKS, ICD, Cloudant, ICOS, Key Protect, Push Notifications, etc.) must have options/capabilities for support application recovery in 1 hour or less. For databases that require backup/restore, in order to qualify for Tier 2 the backups need to be every two hours? Sounds like the app owners can choose their RPO, so they could schedule their backups accordingly to fit both RTO and RPO they decide for themselves. Since each business application will have a unique set of compute, service and other resources, each one will need to be reviewed to make sure that the strategy and requirements for DR for that app are understood, documented and implemented (automation?) before going to production. To build the framework that will drive that analysis and work we will profile three business applications with comment sets of resources to create the scaffolding that application teams can use for their own applications. High Availability and Disaster Recovery When dealing with improved resilience it important to make some distinctions between High Availability (HA) and Disaster Recovery (DR). HA is mainly about keeping the service available to the end users when \"ordinary\" activities are performed on the system like deploying updates, rebooting the hosting Virtual Machines, applying security patches to the hosting OS, etc. For our purposes, High Availability within a single site can be achieved by eliminating single points of failure. The Blue Compute sample application in its current form implements high availability. HA usually doesn't deal with major unplanned (or planned) issues such as complete site loss due to major power outages, earthquakes, severe hardware failures, full-site connectivity loss, etc. In such cases, if the service must meet strict Service Level Objectives (SLO), you should make the whole application stack (infrastructure, services and application components) redundant by deploying it in at least two different Bluemix regions. This is typically defined as a DR Architecture. There are many options to implement DR solutions. For the sake of simplicity, we can group the different options in three major categories: Active/Passive - Active/Passive options are based on keeping the full application stack active in one location, while another application stack is deployed in a different location, but kept idle (or shut down). In the case of prolonged unavailability of the primary site, the application stack is activated in the backup site. Often that requires the restoring of backups taken in the primary site. This approach is not recommended when loosing data can be a problem (e.g. when the Recovery Point Objective (RPO) is less than a few hours ) or when the availability of the service is critical (e.g. when the Return to Operations (RTO) objective is less than a few hours). Active/Standby - In the Active/Standby case the full application stack is active in both primary and backup location, however users transactions are served only by the primary site. The backup site takes care of keeping a replica of the status of the main location though data replication (such as DB replication or disk replication). In case of prolonged unavailability of the primary site, all client transactions are routed to the backup site. This approach provides quite good RPO and RTO (generally measured in minutes), however it is significantly more expensive than the Active/Passive options because of the double deployment (e.g., resources are wasted because the Stand by assets can't be used to improve scalability and throughput). Active/Active - In the Active/Active case both locations are active and client transactions are distributed according to predefined policies (such as round-robin, geographical load balancing, etc. ) to both regions. In the case of failure of one site the other site must be able to serve all clients. It's possible to achieve both an RPO and RTO close to zero with this configuration. The drawback is that both regions must be sized to handle the full load, even if they are used at the half of their capabilities when both locations are available. In such cases the Bluemix Autoscaling service can help in keeping always resources allocated according to the needs (as happens with the BlueCompute sample application). *Source: Making Microservices Resilient Cloud Architecture Network Architecture Sample Application Architecture Application Profiles One approach is to build a set of architecture profiles that represent the majority of apps. These profiles would include options for various classes of service, compute requirements, and all the other stuff listed below. Tier 1 Tier 1 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured CFEE provisioned in the VPC Org, spaces, custom domains and other custom configurations needed All applications deployed with the same version as deployed in primary MZR Application: All applications deployed with the same version as deployed in primary MZR Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, etc.) Note: Databases that use backup/restore as their only cross region replication option will not support Tier 1 availability unless the backups occur hourly and a restore can be completed in less than an hour. Service credentials and service bindings Tier 1 Compute In order to meet Tier 1 RTO/RPO these components will need to be preconfigured and at their production workload capacity in US East. The size of the CF Enterprise or IKS clusters in US East may only need to be large enough to support Tier 1 and 2 workloads. All applications can always be deployed there (as they are today in Dedicated) but for Tier 3 and 4 applications they could be stopped. The idea would be to allow for Tier 1 and 2 apps to immediately become available with enough capacity to run them should a failure occur, but to save money the cluster would only be scaled up to match the primary cluster (and support the Tier 3/4 workloads) when actually needed, as Tier 3/4 apps have longer RTO in which the scaling operation would complete. The same technique could be applied to updates to the CF Enterprise or IKS clusters. The first step would be to scale up the backup cluster to full capacity (matching the primary cluster) before starting the upgrade. Then do some sort of blue/green update where workload could be switched to the backup while the primary cluster is upgraded. Once the primary cluster is upgraded and verified, workload would be switched back, the backup cluster could be scaled back down and then upgraded. Gaps/Unknowns Where will traffic routing be managed/changed in order to redirect from primary to backup site? Where is it done today? For databases, do we have any issues or constraints with scaling in terms of timing? For example, for Cloudant Enterprise, will there always be enough capacity in the backup MZR such that all databases in the instance in the primary MZR are able to be replicated or restored? If not this could be an impact on RTO for apps that use Cloudant. Tier 1 Application - Cloudant In this scenario the data will be replicated automatically by Cloudant. In the event of a disaster the only change needed will be to reconfigure DNS to point to the cluster in US East. Benefits - Fastest recovery time, dependent only on the time it takes to verify that the data was replicated and make the DNS switch Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful. Tier 1 Application - PostGreSQL In this scenario the data will be replicated automatically by IBM Cloud Databases to a read-only replica in the backup MZR. In the event of a disaster the application team will need to manually trigger a promotion of the read-only replica to become the leader. This action will take some time; as a read-only replica the service instance is NOT configured using an HA topology . When the promotion occurs several steps happen to elevate the instance to an HA configuration. The only other change needed will be to reconfigure DNS to point to the cluster in US East, once the database promotion is complete. Benefits - Faster recovery time, as the data is already replicated to the backup MZR. There is still some latency related to the time it takes to reconfigure the database to an HA configuration. Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful. - Manual intervention is required to trigger and monitor the promotion of the read-only replica to leader status. Configuring read-only replicas Basic Steps: create instance in primary MZR (i.e. Dallas) create read-only replica in DR MZR (i.e. Washington, DC) Read-only replica is a single zone instance If primary MZR is unavailable, promote read-only replica to leader. Now DR MZR becomes the leader updates the config in the DR MZR to be MZR resilient, meaning additional nodes are added takes a full backup of the database in the DR MZR DR MZR becomes the leader and ties to original leader in primary MZR are broken original instance in Primary MZR can be deleted. This deletes all backups in the primary MZR. if original instance is not deleted (i.e. backups are still available) a new instance can be created by restoring from a backup, should that become necessary. the backups taken in the original MZR are still accessible even if that MZR is completely unavailable; they are stored in cross-regional ICOS buckets. If a read-only replica is promoted to leader, the original leader is no longer viable. To move the data/workload back to the original MZR, create a read-replica in the original MZR and promote it to be the leader. This does create a new instance of the database, not a restore of the original instance. Tier 1 Application - Cloud Object Storage In this scenario the data is always available in both MZRs via cross-regional buckets. Therefore, the only change needed is to update the DNS routing to point to the backup MZR. Tier 2 Tier 2 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: - VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured - CFEE provisioned in the VPC - Org, spaces, custom domains and other custom configurations needed - All applications deployed with the same version as deployed in primary MZR Application: - All applications deployed with the same version as deployed in primary MZR - Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, backup/restore, etc.) - Service credentials and service bindings Tier 2 Application - MongoDB In this scenario, the data is not replicated to the backup MZR. When a disaster is declared the application team will need to create a new database instance in the backup MZR by restoring the data from backup. Note: The backup from the database instance in the Primary MZR is available in the backup MZR even if the primary MZR is completely unavailable. Once the data is restored, the only other change needed is to change the DNS routing to point to the backup MZR. Backup and restore procedures ElasticSearch MongoDB Redis References IBM Kubernetes Service Multi-Region Architecture Strategies for Resilient Applications Secure web applications across multiple regions Resilient and secure multi-region Kubernetes clusters with IBM Cloud Internet Services ibm-cloud-services-resilience This repository provides links to the HA / DR / Backup information for the IBM Cloud Services in Scope. Services DR Capabilities DR Ownership/Control Documentation RTO Console Core service available in multiple regions IBM N/A HA IAM Core Service available in multiple regions IBM N/A HA CFEE Service available in multiple regions Client Tgt. end of November based on Client DR Implementation Cloudant Service available in multiple regions Client Disaster recovery and backup based on Client DR Implementation IBM Cloud DB Elastic Search Service available in multiple regions Client Managing Backups based on on Client DR Implementation IBM Cloud DB Mongo DB Service available in multiple regions Client Managing Backups IBM Cloud DB Postgres Service available in multiple regions Client Managing Backups based on Client DR Implementation IBM Cloud Object Storage Service available in multiple regions Client About IBM Cloud Object Storage depends on Client DR Implementation IKS Service available in multiple regions Client High availability for IBM Cloud Kubernetes Service Planning your cluster for high availability based on Client DR Implementation Key Protect Regional: IBM is responsible for bringing the service back in a different Region IBM N/A the current documented RTO is <1 day Message Hub/Event streams Service available in multiple regions Client FAQs based on Client DR Implementation Push Notification Regional IBM TBD the current documented RTO is 1-3 days Virtual Private Cloud Service available in multiple regions Client Introduction based on Client DR Implementation Watson Natural Language Understanding (NLU) Client is responsible for restoring the service back in a different Region Client High availability and disaster recovery based on Client DR Implementation Other documentation: Build resilient applications on the cloud Hybrid integration for solutions that span environments Hybrid integration for solutions that span environments Strategies for resilient applications","title":"Overview"},{"location":"disaster-recovery/disaster-recovery/#disaster-recovery","text":"","title":"Disaster Recovery"},{"location":"disaster-recovery/disaster-recovery/#overview","text":"The approach to defining the disaster recover strategy needs to be systematic, and start with the \"application\", which is defined as a set of compute resources (IKS or OpenShift apps, VSIs, services, etc.) that make up a \"business application\". While a holistic approach may be desired, the reality is that each business application is independent, with its own RTO/RPO requirements, which for many customers is expressed in the form of a set of service classes or tiers. Resiliency Tier Recovery Description Recovery Time Objective Recovery Point Objective Tier 1 Continuous Availabilty <= 1 Hour <= 1 Hour Tier 2 Advanced Recovery > 1 Hrs - <= 24 Hrs <2 Hrs - <24 Hrs Tier 3 Standard Recovery > 24 Hrs - <= 72 Hrs Last Backup Tier 4 No Recovery N/A N/A Note: The IBM Cloud platform itself must support the LOWEST number in the range. Therefore, for the majority of their applications (Tier 1 and Tier 2), the IBM Cloud services (OpenShift, IKS, ICD, Cloudant, ICOS, Key Protect, Push Notifications, etc.) must have options/capabilities for support application recovery in 1 hour or less. For databases that require backup/restore, in order to qualify for Tier 2 the backups need to be every two hours? Sounds like the app owners can choose their RPO, so they could schedule their backups accordingly to fit both RTO and RPO they decide for themselves. Since each business application will have a unique set of compute, service and other resources, each one will need to be reviewed to make sure that the strategy and requirements for DR for that app are understood, documented and implemented (automation?) before going to production. To build the framework that will drive that analysis and work we will profile three business applications with comment sets of resources to create the scaffolding that application teams can use for their own applications. High Availability and Disaster Recovery When dealing with improved resilience it important to make some distinctions between High Availability (HA) and Disaster Recovery (DR). HA is mainly about keeping the service available to the end users when \"ordinary\" activities are performed on the system like deploying updates, rebooting the hosting Virtual Machines, applying security patches to the hosting OS, etc. For our purposes, High Availability within a single site can be achieved by eliminating single points of failure. The Blue Compute sample application in its current form implements high availability. HA usually doesn't deal with major unplanned (or planned) issues such as complete site loss due to major power outages, earthquakes, severe hardware failures, full-site connectivity loss, etc. In such cases, if the service must meet strict Service Level Objectives (SLO), you should make the whole application stack (infrastructure, services and application components) redundant by deploying it in at least two different Bluemix regions. This is typically defined as a DR Architecture. There are many options to implement DR solutions. For the sake of simplicity, we can group the different options in three major categories: Active/Passive - Active/Passive options are based on keeping the full application stack active in one location, while another application stack is deployed in a different location, but kept idle (or shut down). In the case of prolonged unavailability of the primary site, the application stack is activated in the backup site. Often that requires the restoring of backups taken in the primary site. This approach is not recommended when loosing data can be a problem (e.g. when the Recovery Point Objective (RPO) is less than a few hours ) or when the availability of the service is critical (e.g. when the Return to Operations (RTO) objective is less than a few hours). Active/Standby - In the Active/Standby case the full application stack is active in both primary and backup location, however users transactions are served only by the primary site. The backup site takes care of keeping a replica of the status of the main location though data replication (such as DB replication or disk replication). In case of prolonged unavailability of the primary site, all client transactions are routed to the backup site. This approach provides quite good RPO and RTO (generally measured in minutes), however it is significantly more expensive than the Active/Passive options because of the double deployment (e.g., resources are wasted because the Stand by assets can't be used to improve scalability and throughput). Active/Active - In the Active/Active case both locations are active and client transactions are distributed according to predefined policies (such as round-robin, geographical load balancing, etc. ) to both regions. In the case of failure of one site the other site must be able to serve all clients. It's possible to achieve both an RPO and RTO close to zero with this configuration. The drawback is that both regions must be sized to handle the full load, even if they are used at the half of their capabilities when both locations are available. In such cases the Bluemix Autoscaling service can help in keeping always resources allocated according to the needs (as happens with the BlueCompute sample application). *Source: Making Microservices Resilient","title":"Overview"},{"location":"disaster-recovery/disaster-recovery/#cloud-architecture","text":"","title":"Cloud Architecture"},{"location":"disaster-recovery/disaster-recovery/#network-architecture","text":"","title":"Network Architecture"},{"location":"disaster-recovery/disaster-recovery/#sample-application-architecture","text":"","title":"Sample Application Architecture"},{"location":"disaster-recovery/disaster-recovery/#application-profiles","text":"One approach is to build a set of architecture profiles that represent the majority of apps. These profiles would include options for various classes of service, compute requirements, and all the other stuff listed below.","title":"Application Profiles"},{"location":"disaster-recovery/disaster-recovery/#tier-1","text":"Tier 1 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured CFEE provisioned in the VPC Org, spaces, custom domains and other custom configurations needed All applications deployed with the same version as deployed in primary MZR Application: All applications deployed with the same version as deployed in primary MZR Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, etc.) Note: Databases that use backup/restore as their only cross region replication option will not support Tier 1 availability unless the backups occur hourly and a restore can be completed in less than an hour. Service credentials and service bindings","title":"Tier 1"},{"location":"disaster-recovery/disaster-recovery/#tier-1-compute","text":"In order to meet Tier 1 RTO/RPO these components will need to be preconfigured and at their production workload capacity in US East. The size of the CF Enterprise or IKS clusters in US East may only need to be large enough to support Tier 1 and 2 workloads. All applications can always be deployed there (as they are today in Dedicated) but for Tier 3 and 4 applications they could be stopped. The idea would be to allow for Tier 1 and 2 apps to immediately become available with enough capacity to run them should a failure occur, but to save money the cluster would only be scaled up to match the primary cluster (and support the Tier 3/4 workloads) when actually needed, as Tier 3/4 apps have longer RTO in which the scaling operation would complete. The same technique could be applied to updates to the CF Enterprise or IKS clusters. The first step would be to scale up the backup cluster to full capacity (matching the primary cluster) before starting the upgrade. Then do some sort of blue/green update where workload could be switched to the backup while the primary cluster is upgraded. Once the primary cluster is upgraded and verified, workload would be switched back, the backup cluster could be scaled back down and then upgraded. Gaps/Unknowns Where will traffic routing be managed/changed in order to redirect from primary to backup site? Where is it done today? For databases, do we have any issues or constraints with scaling in terms of timing? For example, for Cloudant Enterprise, will there always be enough capacity in the backup MZR such that all databases in the instance in the primary MZR are able to be replicated or restored? If not this could be an impact on RTO for apps that use Cloudant.","title":"Tier 1 Compute"},{"location":"disaster-recovery/disaster-recovery/#tier-1-application-cloudant","text":"In this scenario the data will be replicated automatically by Cloudant. In the event of a disaster the only change needed will be to reconfigure DNS to point to the cluster in US East. Benefits - Fastest recovery time, dependent only on the time it takes to verify that the data was replicated and make the DNS switch Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful.","title":"Tier 1 Application - Cloudant"},{"location":"disaster-recovery/disaster-recovery/#tier-1-application-postgresql","text":"In this scenario the data will be replicated automatically by IBM Cloud Databases to a read-only replica in the backup MZR. In the event of a disaster the application team will need to manually trigger a promotion of the read-only replica to become the leader. This action will take some time; as a read-only replica the service instance is NOT configured using an HA topology . When the promotion occurs several steps happen to elevate the instance to an HA configuration. The only other change needed will be to reconfigure DNS to point to the cluster in US East, once the database promotion is complete. Benefits - Faster recovery time, as the data is already replicated to the backup MZR. There is still some latency related to the time it takes to reconfigure the database to an HA configuration. Impacts - Cost. To support the recovery time objective the only viable option is the active/hot standby model. - Development teams need to update their deployment pipelines to also deploy their apps to the standby CFEE cluster every time they deploy to production. And validate that it was successful. - Manual intervention is required to trigger and monitor the promotion of the read-only replica to leader status. Configuring read-only replicas Basic Steps: create instance in primary MZR (i.e. Dallas) create read-only replica in DR MZR (i.e. Washington, DC) Read-only replica is a single zone instance If primary MZR is unavailable, promote read-only replica to leader. Now DR MZR becomes the leader updates the config in the DR MZR to be MZR resilient, meaning additional nodes are added takes a full backup of the database in the DR MZR DR MZR becomes the leader and ties to original leader in primary MZR are broken original instance in Primary MZR can be deleted. This deletes all backups in the primary MZR. if original instance is not deleted (i.e. backups are still available) a new instance can be created by restoring from a backup, should that become necessary. the backups taken in the original MZR are still accessible even if that MZR is completely unavailable; they are stored in cross-regional ICOS buckets. If a read-only replica is promoted to leader, the original leader is no longer viable. To move the data/workload back to the original MZR, create a read-replica in the original MZR and promote it to be the leader. This does create a new instance of the database, not a restore of the original instance.","title":"Tier 1 Application - PostGreSQL"},{"location":"disaster-recovery/disaster-recovery/#tier-1-application-cloud-object-storage","text":"In this scenario the data is always available in both MZRs via cross-regional buckets. Therefore, the only change needed is to update the DNS routing to point to the backup MZR.","title":"Tier 1 Application - Cloud Object Storage"},{"location":"disaster-recovery/disaster-recovery/#tier-2","text":"Tier 2 applications have a requirement that the platform be available in less than an hour in the event of a disaster where the primary MZR becomes unavailable. To achieve this with CFEE it will be necessary to have a fully-configured instance of CFEE up and running in hot standby mode in the backup MZR. This includes: Platform: - VPC with subnets that use non-overlapping CIDR blocks and on-premise connectivity configured - CFEE provisioned in the VPC - Org, spaces, custom domains and other custom configurations needed - All applications deployed with the same version as deployed in primary MZR Application: - All applications deployed with the same version as deployed in primary MZR - Dependent services provisioned and necessary data replication strategy in place (bi-directional, read only replica, backup/restore, etc.) - Service credentials and service bindings","title":"Tier 2"},{"location":"disaster-recovery/disaster-recovery/#tier-2-application-mongodb","text":"In this scenario, the data is not replicated to the backup MZR. When a disaster is declared the application team will need to create a new database instance in the backup MZR by restoring the data from backup. Note: The backup from the database instance in the Primary MZR is available in the backup MZR even if the primary MZR is completely unavailable. Once the data is restored, the only other change needed is to change the DNS routing to point to the backup MZR. Backup and restore procedures ElasticSearch MongoDB Redis","title":"Tier 2 Application - MongoDB"},{"location":"disaster-recovery/disaster-recovery/#references","text":"IBM Kubernetes Service Multi-Region Architecture Strategies for Resilient Applications Secure web applications across multiple regions Resilient and secure multi-region Kubernetes clusters with IBM Cloud Internet Services","title":"References"},{"location":"disaster-recovery/disaster-recovery/#ibm-cloud-services-resilience","text":"This repository provides links to the HA / DR / Backup information for the IBM Cloud Services in Scope. Services DR Capabilities DR Ownership/Control Documentation RTO Console Core service available in multiple regions IBM N/A HA IAM Core Service available in multiple regions IBM N/A HA CFEE Service available in multiple regions Client Tgt. end of November based on Client DR Implementation Cloudant Service available in multiple regions Client Disaster recovery and backup based on Client DR Implementation IBM Cloud DB Elastic Search Service available in multiple regions Client Managing Backups based on on Client DR Implementation IBM Cloud DB Mongo DB Service available in multiple regions Client Managing Backups IBM Cloud DB Postgres Service available in multiple regions Client Managing Backups based on Client DR Implementation IBM Cloud Object Storage Service available in multiple regions Client About IBM Cloud Object Storage depends on Client DR Implementation IKS Service available in multiple regions Client High availability for IBM Cloud Kubernetes Service Planning your cluster for high availability based on Client DR Implementation Key Protect Regional: IBM is responsible for bringing the service back in a different Region IBM N/A the current documented RTO is <1 day Message Hub/Event streams Service available in multiple regions Client FAQs based on Client DR Implementation Push Notification Regional IBM TBD the current documented RTO is 1-3 days Virtual Private Cloud Service available in multiple regions Client Introduction based on Client DR Implementation Watson Natural Language Understanding (NLU) Client is responsible for restoring the service back in a different Region Client High availability and disaster recovery based on Client DR Implementation Other documentation: Build resilient applications on the cloud Hybrid integration for solutions that span environments Hybrid integration for solutions that span environments Strategies for resilient applications","title":"ibm-cloud-services-resilience"},{"location":"disaster-recovery/tier1/","text":"Tier 1 Application Overview This is what makes an application a tier 1 application Architecture Recovery Requirements","title":"Tier 1 Application"},{"location":"disaster-recovery/tier1/#tier-1-application","text":"","title":"Tier 1 Application"},{"location":"disaster-recovery/tier1/#overview","text":"This is what makes an application a tier 1 application","title":"Overview"},{"location":"disaster-recovery/tier1/#architecture","text":"","title":"Architecture"},{"location":"disaster-recovery/tier1/#recovery-requirements","text":"","title":"Recovery Requirements"},{"location":"disaster-recovery/tier2/","text":"Tier 2 Application Overview Architecture","title":"Tier 2 Application"},{"location":"disaster-recovery/tier2/#tier-2-application","text":"","title":"Tier 2 Application"},{"location":"disaster-recovery/tier2/#overview","text":"","title":"Overview"},{"location":"disaster-recovery/tier2/#architecture","text":"","title":"Architecture"},{"location":"ha-app/create-env/","text":"Overview The architecture that we want to create looks like this: Regardless of whether you choose to run your applications in IBM Cloud Kubernetes Service (IKS) or IBM Cloud Foundry Enterprise Environment (CFEE) the provisioning process will be substantially the same, at least as far as the architecture goes. For the purposes of this document we wil use CFEE as the compute platform. In addition to the clusters themselves, most enterprises will use additional tools for managing and monitoring the environment. We will add additional services from IBM Cloud into the mix by provisioning them and configuring your clusters to use them. Logging - Log Analysis with LogDNA Auditing - Activity Tracker with LogDNA Monitoring - SysDig Encryption - Key Protect Setting up the VPC You can use Schematics on IBM Cloud to set up your VPCs. Schematics uses terraform to create and manage the infrastructure. Directions for using Schematics can be found here , and you can use this repo for your schematics workspaces for your VPCs. https://github.com/dwakeman/private-vpc-terraform You will need to create a separate workspace for each VPC, one in US South and one in US East. The terraform project will create your VPC in the region you specify, and it will create three subnets for you, in different availability zones. Provision the clusters Provision cluster in US South","title":"Creating the Environment"},{"location":"ha-app/create-env/#overview","text":"The architecture that we want to create looks like this: Regardless of whether you choose to run your applications in IBM Cloud Kubernetes Service (IKS) or IBM Cloud Foundry Enterprise Environment (CFEE) the provisioning process will be substantially the same, at least as far as the architecture goes. For the purposes of this document we wil use CFEE as the compute platform. In addition to the clusters themselves, most enterprises will use additional tools for managing and monitoring the environment. We will add additional services from IBM Cloud into the mix by provisioning them and configuring your clusters to use them. Logging - Log Analysis with LogDNA Auditing - Activity Tracker with LogDNA Monitoring - SysDig Encryption - Key Protect","title":"Overview"},{"location":"ha-app/create-env/#setting-up-the-vpc","text":"You can use Schematics on IBM Cloud to set up your VPCs. Schematics uses terraform to create and manage the infrastructure. Directions for using Schematics can be found here , and you can use this repo for your schematics workspaces for your VPCs. https://github.com/dwakeman/private-vpc-terraform You will need to create a separate workspace for each VPC, one in US South and one in US East. The terraform project will create your VPC in the region you specify, and it will create three subnets for you, in different availability zones.","title":"Setting up the VPC"},{"location":"ha-app/create-env/#provision-the-clusters","text":"","title":"Provision the clusters"},{"location":"ha-app/create-env/#provision-cluster-in-us-south","text":"","title":"Provision cluster in US South"},{"location":"ha-app/deploy-ha-app/","text":"Prerequisites This document is based on a topology that includes IKS on VPC in US South and US East. VPC in US South VPC in US East IKS cluster on VPC in US South IKS cluster on VPC in US East Custom domain Cloud Internet Services instance provisioned and configured to manage custom domain Setup Need to follow these steps to deploy the app, setup TLS termination and expose the app to the public.","title":"HA app deployment"},{"location":"ha-app/deploy-ha-app/#prerequisites","text":"This document is based on a topology that includes IKS on VPC in US South and US East. VPC in US South VPC in US East IKS cluster on VPC in US South IKS cluster on VPC in US East Custom domain Cloud Internet Services instance provisioned and configured to manage custom domain","title":"Prerequisites"},{"location":"ha-app/deploy-ha-app/#setup","text":"Need to follow these steps to deploy the app, setup TLS termination and expose the app to the public.","title":"Setup"},{"location":"ha-app/ha-app/","text":"Overview One of the great things about hosting your application in the cloud is the variety of options to consider when planning for a high availability application deployment pattern. Most cloud providers offer multiple regions around the world with some sort of multi-zone architecture to facilitate such deployments. What to consider Compute - Your app will need a place to run. There are many choices for compute, including bare metal and virtual servers, platforms like Kubernetes or Cloud Foundry, and serverless platforms. Each option will have its own set of characterstics, tools and features to consider; this document will focus on two of the more popular ones, cloud foundry and kubernetes. Data - If your apps will include data, as most do, you need to consider which database technology to use, and how to configure it for use in a multi-region deployment. Most cloud provider-managed databases are \"regional\" in nature, meaning they provide high availability within a region, but not across regions. It is up to you to configure them such that the data is properly replicated, backed up/restored, etc., as appropriate for your application. DevOps - How are you going to deploy your application? If your application is to highly available it has to stay up during deployments. There are many patterns for doing this, some built into the compute platforms themselves. Kubernetes, for example, does rolling deployments automatically so that your application is always available, asusming your application is configured correctly. Blue/green deployments are another common pattern that are used with many compute platforms. Routing - How are you going to route the traffic to your application? You want your application to be served up by a single URL, like myapp.eyebeemdemos.com , but you need the capability to route traffic to multiple instances of your application. If you're using kubernetes or cloud foundry they will automatically provide application load balancer capabilities to route traffic to multiple instances running in the same region (i.e kubernetes cluster). But what about a cross-region scenario? There are multiple ways to accomplish this; this document will leverage IBM Cloud Internet Services by creating a global load balancer that will perform health checking and routing. Architecture At a high level the architecture looks like this: In the diagram above you can see the that application is deployed into two regions, US South and US East . In each region the compute platform - in thise case IBM Cloud Foundry Enterprise, which runs on IBM Kubernetes Service. Within each region, the application is part of a subdomain provided by Cloud Foundry Enterprise: <cluster-name>.<region>.containers.appdomain.cloud . These subdomains can be used as sources in a global load balancer called myapp.eyebeemdemos.com in IBM Cloud Internet Services, which will route traffic across both regions. There is additional setup work that needs to be done for you to be able to use your own domain. More information can be found here . DevOps It is common for enterprises to use multiple non-production environments as they develop and test their applications. The number of these environment varies, but in general there are at least three: Dev - this is where developers do their initial testing in a server-based environment Test - Consolidated testing of multiple components is done here by the development team before handing the app over to users for testing UAT - This is where user acceptance testing is performed In addition to these non-production environments the application will have at least one production environment where the application will be used. It may also be important to have multiple production environments in different regions around the world. Typically there will also be a backup or disaster recovery environment. For this document, the non-production environments above will be implemented as different namespaces within the same IKS cluster, and the production environment will be a namespace in the production IKS clusters in two different regions. Another common practice is to use different subdomains for each environment. As the application moves through the environments on the way to production, its URL will change: Dev - myapp.dev.eyebeemdemos.com Test - myapp.test.eyebeemdemos.com UAT - myapp.uat.eyebeemdemos.com To enable this capability you will need to perform a few steps: create some DNS CNAME record to point these URLs to the ingress subdomain for your non-production IKS cluster make sure you have certificates for these wildcard domains (i.e. *.dev.eyebeemdemos.com ) create a kubernetes secret with the certificate and private key create an ingress controller to configure TLS termination and map a route to your app Documentation for these steps can be found in the IBM Kubernetes Service documentation here .","title":"HA app considerations"},{"location":"ha-app/ha-app/#overview","text":"One of the great things about hosting your application in the cloud is the variety of options to consider when planning for a high availability application deployment pattern. Most cloud providers offer multiple regions around the world with some sort of multi-zone architecture to facilitate such deployments.","title":"Overview"},{"location":"ha-app/ha-app/#what-to-consider","text":"Compute - Your app will need a place to run. There are many choices for compute, including bare metal and virtual servers, platforms like Kubernetes or Cloud Foundry, and serverless platforms. Each option will have its own set of characterstics, tools and features to consider; this document will focus on two of the more popular ones, cloud foundry and kubernetes. Data - If your apps will include data, as most do, you need to consider which database technology to use, and how to configure it for use in a multi-region deployment. Most cloud provider-managed databases are \"regional\" in nature, meaning they provide high availability within a region, but not across regions. It is up to you to configure them such that the data is properly replicated, backed up/restored, etc., as appropriate for your application. DevOps - How are you going to deploy your application? If your application is to highly available it has to stay up during deployments. There are many patterns for doing this, some built into the compute platforms themselves. Kubernetes, for example, does rolling deployments automatically so that your application is always available, asusming your application is configured correctly. Blue/green deployments are another common pattern that are used with many compute platforms. Routing - How are you going to route the traffic to your application? You want your application to be served up by a single URL, like myapp.eyebeemdemos.com , but you need the capability to route traffic to multiple instances of your application. If you're using kubernetes or cloud foundry they will automatically provide application load balancer capabilities to route traffic to multiple instances running in the same region (i.e kubernetes cluster). But what about a cross-region scenario? There are multiple ways to accomplish this; this document will leverage IBM Cloud Internet Services by creating a global load balancer that will perform health checking and routing.","title":"What to consider"},{"location":"ha-app/ha-app/#architecture","text":"At a high level the architecture looks like this: In the diagram above you can see the that application is deployed into two regions, US South and US East . In each region the compute platform - in thise case IBM Cloud Foundry Enterprise, which runs on IBM Kubernetes Service. Within each region, the application is part of a subdomain provided by Cloud Foundry Enterprise: <cluster-name>.<region>.containers.appdomain.cloud . These subdomains can be used as sources in a global load balancer called myapp.eyebeemdemos.com in IBM Cloud Internet Services, which will route traffic across both regions. There is additional setup work that needs to be done for you to be able to use your own domain. More information can be found here .","title":"Architecture"},{"location":"ha-app/ha-app/#devops","text":"It is common for enterprises to use multiple non-production environments as they develop and test their applications. The number of these environment varies, but in general there are at least three: Dev - this is where developers do their initial testing in a server-based environment Test - Consolidated testing of multiple components is done here by the development team before handing the app over to users for testing UAT - This is where user acceptance testing is performed In addition to these non-production environments the application will have at least one production environment where the application will be used. It may also be important to have multiple production environments in different regions around the world. Typically there will also be a backup or disaster recovery environment. For this document, the non-production environments above will be implemented as different namespaces within the same IKS cluster, and the production environment will be a namespace in the production IKS clusters in two different regions. Another common practice is to use different subdomains for each environment. As the application moves through the environments on the way to production, its URL will change: Dev - myapp.dev.eyebeemdemos.com Test - myapp.test.eyebeemdemos.com UAT - myapp.uat.eyebeemdemos.com To enable this capability you will need to perform a few steps: create some DNS CNAME record to point these URLs to the ingress subdomain for your non-production IKS cluster make sure you have certificates for these wildcard domains (i.e. *.dev.eyebeemdemos.com ) create a kubernetes secret with the certificate and private key create an ingress controller to configure TLS termination and map a route to your app Documentation for these steps can be found in the IBM Kubernetes Service documentation here .","title":"DevOps"},{"location":"hybrid-cloud/hybrid/","text":"Overview Application Architecture Authentication","title":"Deploying a Hybrid Cloud Application"},{"location":"hybrid-cloud/hybrid/#overview","text":"","title":"Overview"},{"location":"hybrid-cloud/hybrid/#application-architecture","text":"","title":"Application Architecture"},{"location":"hybrid-cloud/hybrid/#authentication","text":"","title":"Authentication"},{"location":"hybrid-cloud/hybrid/#_1","text":"","title":""},{"location":"infrastructure/iac/","text":"Infrastructure as Code Overview What is IaC?","title":"Infrastructure as Code (IaC)"},{"location":"infrastructure/iac/#infrastructure-as-code","text":"","title":"Infrastructure as Code"},{"location":"infrastructure/iac/#overview","text":"What is IaC?","title":"Overview"},{"location":"infrastructure/schematics/","text":"Schematics","title":"Schematics"},{"location":"infrastructure/schematics/#schematics","text":"","title":"Schematics"},{"location":"infrastructure/sds/","text":"Using Software Defined Storage There are many types of Software Defined Storage (SDS) implementations in the market, and they may apply with different types of compute technology. The purpose of this section is to document how to install and configure Portworx Enterprise with OpenShift on IBM Cloud . Portworx acts as a storage layer in kubernetes that sits on top of some underlying physical storage attached to the worker nodes in the cluster. It also provides additional storage classes for creating persistent volumes in Portworx. Additional information on using Portworx with OpenShift on IBM Cloud can be found in the documentation . Note: These instructions were tested with OpenShift 4.5.13 running in Virtual Private Cloud and assume that you already have an OpenShift cluster provisioned. Preparing for Portworx Review the documentation for Planning your Portworx setup to verify that your existing cluster meets the requirements. If not, create a new cluster. Note: It would appear that the provisioning page for Portworx in the IBM Cloud Catalog does some sort of requirements checking after you supply an API key. It did not recognize my cluster when it had bx2.8x32 nodes, but id did when I added bx2.16x64 nodes. There are two common topologies for adding SDS to your cluster. The first one is called hyper-converged , where every worker node in the cluster has attached storage for Portworx. This is the topology with the best performance, but it requires you to have storage attached to every worker node. The other topology is called storage-heavy ; in this topology only some of the worker nodes have attached storage. Portworx will still make the storage available to pods running on any worker node in the cluster, but for pods running on non-SDS nodes the storage access requests will be routed on the private network to one of the SDS nodes. Note: As far as I can tell the installation and configuration of Portworx is the same either way, but that may not be true. For purposes of this exercise I have created an OpenShift cluster with 3 worker nodes in US South , with one worker node in each zone. Creating block storage Your worker nodes will need to have block storage volumes attached to them in order for Portworx to work. See the documentation for more details. Since these instuctions are for a cluster in VPC, follow these steps to create and attach the block storage volumes to your worker nodes. Documentation can also be found here . You can create the block storage volumes using the IBM Cloud UI . Make sure to create the block storage volume in the same Availability zone as the worker node to which you want to attach it. For visibility purposes, you should consider a naming convention that will help you determine where the storage volumes are being used. In this example the name of the storage volume includes the worker ID of the worker node to which it will be attached. If you don't want to look up the worker nodes in the UI you can run this command to get a list of your worker nodes: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift Note: As you create each storage volume, take note of it's volume ID . You will need it in the next step. Attaching block storage to worker nodes In order for Portworx to work it needs the storage volumes created above to be attached to the worker nodes. The documentation was used to derive these instructions. Make sure you are logged into the IBM Cloud CLI. Run this command to fetch your oauth token and store it in an environment variable: export iamtoken=$(ibmcloud iam oauth-tokens) <--- This does not work right!! Need to figure out the right command. In this example the cluster was created in US South , with worker nodes in us-south- , us-south-2 and us-south-3 . Block storage volumes of equal size were also created in those three zones. Since worker nodes are not visible in the IBM Cloud portal we cannot attach the storage using normal methods in the UI or CLI. In order to attach the volumes to the worker nodes we have to use the IKS API. The format of the API call looks like this: curl -X POST -H \"Authorization: $iamtoken\" \"https://<region>.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=<cluster_ID>&worker=<worker_ID>&volumeID=<volume_ID>\" Use this command to get your worker IDs: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift The volume IDs should have been collected above when you created the volumes. If not, you can get them from the UI. Note: Make sure to update this section with a CLI command to get the volumes! Run the cURL command above for each worker node and volume. For example: curl -X POST -H \"Authorization: $iamtoken\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386&volumeID=r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\" The output should look like this: {\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"\"},\"name\":\"volume-attachment\",\"status\":\"attaching\",\"type\":\"data\"} To review the volume attachments for a worker node you can run this command: curl -X GET -H \"Authorization: <IAM_token>\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: <resource_group_ID>\" \"https://<region>.containers.cloud.ibm.com/v2/storage/clusters/<cluster_ID>/workers/<worker_ID>/volume_attachments\" For example: curl -X GET -H \"Authorization: $iamtoken\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: 5e6a2492dd9249659469ea51da471197\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/getAttachmentsList?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\" The output should look like this: {\"volume_attachments\":[{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e-9nh8x\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"data\"},{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31\",\"volume\":{\"name\":\"hypnotist-freezing-willow-display\",\"id\":\"r006-a354ecea-4cc3-4445-9431-12cd9ce569a8\"},\"device\":{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31-fzd6s\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"boot\"}]} Provision an instance of Databases for Etcd in the same resource group as the OpenShift cluster. For visibility purposes, consider a naming convention that includes the cluster name, such as px-sbx-ocp-11 , and add a tag with the name of the cluster as well. Review the documentation for more infomration on configuration options. Follow the steps in the documentation. When it comes to encoding the username and password, I found that the instructions did not work, as they added the -n from the command into the output. I used this command: echo \"<the username or password>\" | base64 At this point the Databases for Etcd instance is created, the username and password have been encoded and the secret has been created. Now we need to copy the image pull secrets and add them to the service account . Copy the image pull secrets To list the secrets in the default namespace: oc get secrets -n default | grep icr-io The output should look like this: all-icr-io kubernetes.io/dockerconfigjson 1 115m To copy the all-icr-io secret to the kube-system namespace: oc get secret all-icr-io -n default -o yaml | sed 's/default/kube-system/g' | oc create -n kube-system -f - The output should look like this: secret/all-icr-io created Store the pull secret in the service account Check if the image pull secret already exists for the default service account: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that the Image pull secrets sction only lists default-dockercfg-bkxjw . This means that the image pull secrets are already defined, but do not yet contain all-icr-io . Run this command to add it: oc patch -n kube-system serviceaccount/default --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\":\"all-icr-io\"}}]' The output should look like this: serviceaccount/default patched You can verify that it worked using this command: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw all-icr-io Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that now all-icr-io shows up in the Image pull secrets section.","title":"Software Defined Storage"},{"location":"infrastructure/sds/#using-software-defined-storage","text":"There are many types of Software Defined Storage (SDS) implementations in the market, and they may apply with different types of compute technology. The purpose of this section is to document how to install and configure Portworx Enterprise with OpenShift on IBM Cloud . Portworx acts as a storage layer in kubernetes that sits on top of some underlying physical storage attached to the worker nodes in the cluster. It also provides additional storage classes for creating persistent volumes in Portworx. Additional information on using Portworx with OpenShift on IBM Cloud can be found in the documentation . Note: These instructions were tested with OpenShift 4.5.13 running in Virtual Private Cloud and assume that you already have an OpenShift cluster provisioned.","title":"Using Software Defined Storage"},{"location":"infrastructure/sds/#preparing-for-portworx","text":"Review the documentation for Planning your Portworx setup to verify that your existing cluster meets the requirements. If not, create a new cluster. Note: It would appear that the provisioning page for Portworx in the IBM Cloud Catalog does some sort of requirements checking after you supply an API key. It did not recognize my cluster when it had bx2.8x32 nodes, but id did when I added bx2.16x64 nodes. There are two common topologies for adding SDS to your cluster. The first one is called hyper-converged , where every worker node in the cluster has attached storage for Portworx. This is the topology with the best performance, but it requires you to have storage attached to every worker node. The other topology is called storage-heavy ; in this topology only some of the worker nodes have attached storage. Portworx will still make the storage available to pods running on any worker node in the cluster, but for pods running on non-SDS nodes the storage access requests will be routed on the private network to one of the SDS nodes. Note: As far as I can tell the installation and configuration of Portworx is the same either way, but that may not be true. For purposes of this exercise I have created an OpenShift cluster with 3 worker nodes in US South , with one worker node in each zone.","title":"Preparing for Portworx"},{"location":"infrastructure/sds/#creating-block-storage","text":"Your worker nodes will need to have block storage volumes attached to them in order for Portworx to work. See the documentation for more details. Since these instuctions are for a cluster in VPC, follow these steps to create and attach the block storage volumes to your worker nodes. Documentation can also be found here . You can create the block storage volumes using the IBM Cloud UI . Make sure to create the block storage volume in the same Availability zone as the worker node to which you want to attach it. For visibility purposes, you should consider a naming convention that will help you determine where the storage volumes are being used. In this example the name of the storage volume includes the worker ID of the worker node to which it will be attached. If you don't want to look up the worker nodes in the UI you can run this command to get a list of your worker nodes: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift Note: As you create each storage volume, take note of it's volume ID . You will need it in the next step.","title":"Creating block storage"},{"location":"infrastructure/sds/#attaching-block-storage-to-worker-nodes","text":"In order for Portworx to work it needs the storage volumes created above to be attached to the worker nodes. The documentation was used to derive these instructions. Make sure you are logged into the IBM Cloud CLI. Run this command to fetch your oauth token and store it in an environment variable: export iamtoken=$(ibmcloud iam oauth-tokens) <--- This does not work right!! Need to figure out the right command. In this example the cluster was created in US South , with worker nodes in us-south- , us-south-2 and us-south-3 . Block storage volumes of equal size were also created in those three zones. Since worker nodes are not visible in the IBM Cloud portal we cannot attach the storage using normal methods in the UI or CLI. In order to attach the volumes to the worker nodes we have to use the IKS API. The format of the API call looks like this: curl -X POST -H \"Authorization: $iamtoken\" \"https://<region>.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=<cluster_ID>&worker=<worker_ID>&volumeID=<volume_ID>\" Use this command to get your worker IDs: ic oc worker ls -c <cluster-name> The output should look like this: OK ID Primary IP Flavor State Status Zone Version kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000162 10.1.128.12 bx2.8x32 normal Ready us-south-3 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000209 10.1.120.6 bx2.8x32 normal Ready us-south-2 4.5.13_1515_openshift kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386 10.1.112.17 bx2.8x32 normal Ready us-south-1 4.5.13_1515_openshift The volume IDs should have been collected above when you created the volumes. If not, you can get them from the UI. Note: Make sure to update this section with a CLI command to get the volumes! Run the cURL command above for each worker node and volume. For example: curl -X POST -H \"Authorization: $iamtoken\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/createAttachment?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386&volumeID=r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\" The output should look like this: {\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"\"},\"name\":\"volume-attachment\",\"status\":\"attaching\",\"type\":\"data\"} To review the volume attachments for a worker node you can run this command: curl -X GET -H \"Authorization: <IAM_token>\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: <resource_group_ID>\" \"https://<region>.containers.cloud.ibm.com/v2/storage/clusters/<cluster_ID>/workers/<worker_ID>/volume_attachments\" For example: curl -X GET -H \"Authorization: $iamtoken\" -H \"Content-Type: application/json\" -H \"X-Auth-Resource-Group-ID: 5e6a2492dd9249659469ea51da471197\" \"https://us-south.containers.cloud.ibm.com/v2/storage/vpc/getAttachmentsList?cluster=bu6rbsid0j80c74a6u2g&worker=kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\" The output should look like this: {\"volume_attachments\":[{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e\",\"volume\":{\"name\":\"px-kube-bu6rbsid0j80c74a6u2g-sbxocp11-default-00000386\",\"id\":\"r006-0be0c31c-3d1f-4931-bc4e-c554b4d3fa58\"},\"device\":{\"id\":\"0717-85669abb-a268-4fff-95ca-5c8ae953af0e-9nh8x\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"data\"},{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31\",\"volume\":{\"name\":\"hypnotist-freezing-willow-display\",\"id\":\"r006-a354ecea-4cc3-4445-9431-12cd9ce569a8\"},\"device\":{\"id\":\"0717-6acdae87-58cd-4a05-acfa-b178c3544c31-fzd6s\"},\"name\":\"volume-attachment\",\"status\":\"attached\",\"type\":\"boot\"}]} Provision an instance of Databases for Etcd in the same resource group as the OpenShift cluster. For visibility purposes, consider a naming convention that includes the cluster name, such as px-sbx-ocp-11 , and add a tag with the name of the cluster as well. Review the documentation for more infomration on configuration options. Follow the steps in the documentation. When it comes to encoding the username and password, I found that the instructions did not work, as they added the -n from the command into the output. I used this command: echo \"<the username or password>\" | base64 At this point the Databases for Etcd instance is created, the username and password have been encoded and the secret has been created. Now we need to copy the image pull secrets and add them to the service account .","title":"Attaching block storage to worker nodes"},{"location":"infrastructure/sds/#copy-the-image-pull-secrets","text":"To list the secrets in the default namespace: oc get secrets -n default | grep icr-io The output should look like this: all-icr-io kubernetes.io/dockerconfigjson 1 115m To copy the all-icr-io secret to the kube-system namespace: oc get secret all-icr-io -n default -o yaml | sed 's/default/kube-system/g' | oc create -n kube-system -f - The output should look like this: secret/all-icr-io created","title":"Copy the image pull secrets"},{"location":"infrastructure/sds/#store-the-pull-secret-in-the-service-account","text":"Check if the image pull secret already exists for the default service account: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that the Image pull secrets sction only lists default-dockercfg-bkxjw . This means that the image pull secrets are already defined, but do not yet contain all-icr-io . Run this command to add it: oc patch -n kube-system serviceaccount/default --type='json' -p='[{\"op\":\"add\",\"path\":\"/imagePullSecrets/-\",\"value\":{\"name\":\"all-icr-io\"}}]' The output should look like this: serviceaccount/default patched You can verify that it worked using this command: oc describe serviceaccount default -n kube-system The output should look like this: Name: default Namespace: kube-system Labels: <none> Annotations: <none> Image pull secrets: default-dockercfg-bkxjw all-icr-io Mountable secrets: default-token-fbhwh default-dockercfg-bkxjw Tokens: default-token-fbhwh default-token-wsnrg Events: <none> Notice that now all-icr-io shows up in the Image pull secrets section.","title":"Store the pull secret in the service account"},{"location":"infrastructure/vpc/","text":"Creating a VPC","title":"Virtual Private Cloud"},{"location":"infrastructure/vpc/#creating-a-vpc","text":"","title":"Creating a VPC"},{"location":"tools/scripts/ibmcos/","text":"#!/bin/sh #set -x #----------------------------------------------------------------------------------------------------- # Script: ibmcos # Version: 1.0.0 #----------------------------------------------------------------------------------------------------- # IBM Cloud Object Storage - Utility functions #----------------------------------------------------------------------------------------------------- # Copyright (c) 2020, International Business Machines. All Rights Reserved. #----------------------------------------------------------------------------------------------------- cos_command=\"$1\" service_name=\"$2\" bucket_name=\"$3\" location_constraint=\"$4\" key_crn=\"$5\" region=\"us\" # Note: Region should be set based on the location constraint, e.g. \"us\" for us-standard or # \"us-south\" for us-south-standard activity_tracker_crn=\"crn:v1:bluemix:public:logdnaat:us-south:a/9d5d52********************8a:55d90d9c-****-****-****-********785::\" #----------------------------------------------------------------------------------------------------- # Display usage information for the commands in this script #----------------------------------------------------------------------------------------------------- display_usage() { echo echo \"NAME:\" echo \"ibmcos.sh - Manage features of IBM Cloud Object Storage\" echo echo \"USAGE:\" echo \"ibmcos.sh command [service instance name] [options]\" echo echo \"COMMANDS:\" echo \"-------------------------------------------------------------------------------------------\" echo echo \"view-buckets View the buckets in the specified COS instance in XML format\" echo \"create-bucket Create a new bucket with predefined configuration\" echo \"delete-bucket Delete a bucket from the specified COS instance\" echo \"disable-bucket-firewall Remove the COS Firewall configuration from a bucket\" echo \"view-bucket-config Get the headers and configuration for the specified bucket\" echo \"view-objects List the objects in the specified bucket\" echo \"location-constraints Returns a list of valid location constraints for creating buckets\" echo \"view-key-protect Returns a list of Key Protect instances\" echo \"view-keys-list Returns a list of keys in the specified Key Protect instance\" echo \"help, h View help for this script\" echo echo echo \"Note: For your convenience this command executes the ibmcloud cli to look up certain\" echo \" information needed to perform these tasks. It requires you to be logged into\" echo \" the ibmcloud cli before you run this command.\" echo echo } #----------------------------------------------------------------------------------------------------- # Display the list of valid location constraints for creating buckets #----------------------------------------------------------------------------------------------------- display_location_constraints() { echo echo \"LOCATION CONSTRAINTS:\" echo \"-------------------------------------------------------------------------------------------\" echo echo \"us-south-standard Regional Bucket in US South with storage class STANDARD\" echo \"us-east-standard Regional Bucket in US East with storage class STANDARD\" echo \"us-standard Cross regional Bucket in US Geo with storage class STANDARD\" echo \"us-south-flex Regional Bucket in US South with storage class FLEX\" echo \"us-east-flex Regional Bucket in US East with storage class FLEX\" echo \"us-flex Cross regional Bucket in US Geo with storage class FLEX\" echo \"us-south-cold Regional Bucket in US South with storage class COLD\" echo \"us-east-cold Regional Bucket in US East with storage class COLD\" echo \"us-cold Cross regional Bucket in US Geo with storage class COLD\" echo \"us-south-vault Regional Bucket in US South with storage class VAULT\" echo \"us-east-vault Regional Bucket in US East with storage class VAULT\" echo \"us-vault Cross regional Bucket in US Geo with storage class VAULT\" echo echo } #----------------------------------------------------------------------------------------------------- # Display the headers and configuration of a bucket #----------------------------------------------------------------------------------------------------- view_bucket_configuration() { echo echo \"Bucket Headers:\" echo curl -s -I \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' echo echo \"Bucket Configuration:\" echo curl -s \"https://config.cloud-object-storage.cloud.ibm.com/v1/b/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' | jq --color-output echo } #----------------------------------------------------------------------------------------------------- # Validate the location constraint specified on a create-bucket request #----------------------------------------------------------------------------------------------------- validate_location_constraint() { constraint=\"$1\" #echo \"The location constraint passed to the function is ${constraint}\" locationCount=$(echo $constraints | jq '.locations | length') locationValid=false #echo \"Value of locationValid is ${locationValid}\" case $constraint in us-standard) locationValid=true;; us-south-standard) locationValid=true;; us-south-flex) locationValid=true;; us-south-cold) locationValid=true;; us-south-vault) locationValid=true;; us-east-standard) locationValid=true;; us-east-flex) locationValid=true;; us-east-cold) locationValid=true;; us-east-vault) locationValid=true;; us-standard) locationValue=true;; us-flex) locationValue=true;; us-cold) locationValue=true;; esac #echo \"Value of locationValid is ${locationValid}\" if [[ $locationValid = false ]]; then echo echo \"ERROR: ${constraint} is not a valid location constraint.\" echo display_location_constraints exit 1 fi } #----------------------------------------------------------------------------------------------------- # Check base input parameters #----------------------------------------------------------------------------------------------------- if [[ $# -lt 1 ]]; then display_usage exit 1 fi # Only need to do these commands if a service name is present if [[ $# -ge 2 ]]; then # Get the GUID for the specified service instance service_instance_guid=$(ic resource service-instance $service_name --output JSON | jq -r '.[0].guid') # Get the bearer token to use for authentication. # Note: This assumes that the user has previously logged into the IBM Cloud CLI ibm_auth_token=$(ic iam oauth-tokens | awk '{ print $4}') fi #----------------------------------------------------------------------------------------------------- # View Help #----------------------------------------------------------------------------------------------------- if [ $cos_command == \"help\" ] || [ $cos_command == \"h\" ]; then echo display_usage exit 0 fi #----------------------------------------------------------------------------------------------------- # List all the buckets in the specified COS instance in XML format #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-buckets\" ]]; then echo echo \"Listing buckets for service ${service_name}...\" echo curl -s \"https://s3.${region}.cloud-object-storage.appdomain.cloud/?extended\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' | xmllint --format - echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View all the objects in the specified COS instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-objects\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for view-objects a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-objects [service instance name] [bucket-name]\" echo echo exit 1 fi echo echo \"viewing objects for bucket ${bucket_name}...\" echo curl -s \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}?List-type=2\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' | xmllint --format - echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View all the objects in the specified COS instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"get-object\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for get-object a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-objects [service instance name] [bucket-name] [object-name]\" echo echo exit 1 fi if [[ $# -lt 4 ]]; then echo echo \"for get-object an object name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-objects [service instance name] [bucket-name] [object-name]\" echo echo exit 1 fi object_name=\"$4\" echo echo \"getting object ${object_name}...\" echo curl -s \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}/${object_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' > ${object_name}.pdf echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # Create a bucket in the specified COS instance with Key Protect, Activity Tracker # and IP Firewall enabled #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"create-bucket\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for create-bucket a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh create-bucket [service instance name] [bucket-name] [location-constraint] [key-crn]\" echo echo exit 1 fi if [[ $# -lt 4 ]]; then echo echo \"for create-bucket a location constraint is required\" echo echo \"USAGE:\" echo \"ibmcos.sh create-bucket [service instance name] [bucket-name] [location-constraint] [key-crn]\" display_location_constraints echo exit 1 fi if [[ $# -lt 5 ]]; then echo echo \"for create-bucket a Key Protect Key is required\" echo echo \"USAGE:\" echo \"ibmcos.sh create-bucket [service instance name] [bucket-name] [location-constraint] [key-crn]\" echo exit 1 fi # Validate the specified bucket constraint validate_location_constraint $location_constraint if [[ $location_constraint == \"us-standard\" ]]; then region=\"us\" fi echo \"The updated region is ${region}\" echo echo \"Creating bucket ${bucket_name} with location constraint ${location_constraint}...\" echo curl -s -X PUT \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'ibm-service-instance-id: '\"$service_instance_guid\"'' \\ -H 'ibm-sse-kp-encryption-algorithm: AES256' \\ -H 'ibm-sse-kp-customer-root-key-crn: '\"$key_crn\"'' \\ -d '<CreateBucketConfiguration><LocationConstraint>'\"${location_constraint}\"'</LocationConstraint></CreateBucketConfiguration>' # Next we need to update the config for Activity Tracker and COS Firewall # Note: A CIDR block of 0.0.0.0/0 allows all IP Addresses # Note: To remove the COS Firewall pass an empty array: {\"allowed_ip\": []} curl -s -X PATCH \"https://config.cloud-object-storage.cloud.ibm.com/v1/b/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -d '{ \"activity_tracking\": { \"read_data_events\": false, \"write_data_events\": true, \"activity_tracker_crn\": \"'\"$activity_tracker_crn\"'\" }, \"firewall\": { \"allowed_ip\": [\"0.0.0.0/0\"] } }' | jq --color-output echo echo \"Bucket Created, retrieving bucket headers and configuration...\" echo view_bucket_configuration echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # Remove COS Firewall # Note: This will probably only be used if the firewall config is invalid #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"disable-bucket-firewall\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for disable-bucket-firewall a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh disable-bucket-firewall [service instance name] [bucket-name]\" echo echo exit 1 fi echo echo \"Disabling COS Firewall for bucket ${bucket_name}...\" echo curl -s -X PATCH \"https://config.cloud-object-storage.cloud.ibm.com/v1/b/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -d '{ \"firewall\": {\"allowed_ip\": []} }' | jq --color-output echo echo \"Bucket firewall disabled...\" echo view_bucket_configuration echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # Delete a bucket #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"delete-bucket\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for delete-bucket a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh delete-bucket [service instance name] [bucket-name]\" echo echo exit 1 fi echo echo \"Deleting bucket ${bucket_name}...\" echo curl -s -X DELETE \"https://s3.${region}.cloud-object-storage.appdomain.cloud/${bucket_name}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' echo echo \"Request complete.\" echo echo exit 0 fi if [[ $cos_command == \"location-constraints\" ]]; then display_location_constraints exit 0 fi #----------------------------------------------------------------------------------------------------- # Get Bucket Config #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-bucket-config\" ]]; then if [[ $# -lt 3 ]]; then echo echo \"for view-bucket-config a bucket name is required\" echo echo \"USAGE:\" echo \"ibmcos.sh view-bucket-config [service instance name] [bucket-name]\" echo echo exit 1 fi view_bucket_configuration echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View a list of keys in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-keys\" ]]; then echo echo \"Listing keys for service ${service_name}...\" echo # Get the region for the Key Protect Instance location=$(ic resource service-instance $service_name --output json | jq -r '.[0].region_id') curl -s \"https://${location}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq --color-output echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View a list of keys in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-keys-list\" ]]; then echo echo \"Listing keys for service ${service_name}...\" echo # Get the region for the Key Protect Instance location=$(ic resource service-instance $service_name --output json | jq -r '.[0].region_id') curl -s \"https://${location}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq -r '[.resources | .[] | { name: .name, id: .id, crn: .crn} ] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" echo echo exit 0 fi #----------------------------------------------------------------------------------------------------- # View a list of keys in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $cos_command == \"view-key-protect\" ]]; then echo echo \"Listing instance of Key Protect...\" echo ic resource service-instances --output JSON | jq -r '[ .[] | select(.sub_type != null) | select(.sub_type | contains(\"kms\"))] | [ .[] | {name: .name, type: .type, sub_type: .sub_type }] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" echo echo exit 0 fi echo echo \"Error: Invalid command\" echo display_usage","title":"Ibmcos"},{"location":"tools/scripts/keyprotect/","text":"#!/bin/sh #set -x #----------------------------------------------------------------------------------------------------- # Script: keyprotect # Version: v1.0.2 #----------------------------------------------------------------------------------------------------- # IBM Key Protect - Utility functions #----------------------------------------------------------------------------------------------------- # Copyright (c) 2020, International Business Machines. All Rights Reserved. #----------------------------------------------------------------------------------------------------- display_usage() { echo echo \"NAME:\" echo \"keyprotect.sh - Manage features of Key Protect\" echo echo \"USAGE:\" echo \"keyprotect.sh <key-protect-instance-name> command [options]\" echo echo \"COMMANDS:\" echo \"-------------------------------------------------------------------------------------------\" echo echo \"view-policies List the current policies for the Key Protect Instance\" echo \"enable-dual-auth Enable the Dual Authorization policy for key deletes for all keys\" echo \"disable-dual-auth Disable the Dual Authorization policy for key deletes for all keys\" echo \"disable-public-endpoint Disable the public endpoint for the Key Protect Instance\" echo \"enable-public-endpoint Enable the public endpoint for the Key Protect Instance\" echo \"view-keys List the keys in the Key Protect Instance in JSON format\" echo \"view-keys-list List the keys in the Key Protect Instance in list format\" echo \"view-deleted-keys List the deleted keys in the Key Protect Instance in JSON format\" echo \"view-deleted-keys-list List the deleted keys in the Key Protect Instance in list format\" echo \"view-key View the details of a key\" echo \"view-key-material View the material for a standard key\" echo \"view-key-policies View the current polices for the specified key\" echo \"import-key Import a standard or root key\" echo \"restore-key Restore an imported key that has been deleted\" echo \"set-key-deletion Set the specified key for deletion (first auth)\" echo \"unset-key-deletion Unset the specified key for deletion, which removes the first auth\" echo \"help, h View help for this script\" echo echo echo \"Note: For your convenience this command executes the ibmcloud cli to look up certain\" echo \" information needed to perform these tasks. It requires you to be logged into\" echo \" the ibmcloud cli before you run this command.\" echo echo } view_policies() { echo echo \"Checking current policies for service ${service_name}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' | jq --color-output } #----------------------------------------------------------------------------------------------------- # View policies for a key # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- view_key_policies() { if [[ $# -lt 1 ]]; then echo echo \"for view-key-policies a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh <key-protect-instance-name] view-key-policies [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"Checking current policies for key ${keyId} in service ${service_name}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' | jq --color-output } #----------------------------------------------------------------------------------------------------- # View keys in a Key Protect instance in JSON format # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_keys() { echo echo \"Listing keys for service ${service_name}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View keys in a Key Protect instance in JSON format # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_deleted_keys() { echo echo \"Listing keys for service ${service_name}...\" echo echo \"Values for State field:\" echo \"-------------------------\" echo \"0 Pre-activation\" echo \"1 Active\" echo \"2 Suspended\" echo \"3 Deactivated\" echo \"5 Destroyed\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys?state=5\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View a list of keys in a Key Protect instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_deleted_keys_list() { echo echo \"Listing deleted keys for service ${service_name}...\" echo echo \"Values for State column:\" echo \"-------------------------\" echo \"0 Pre-activation\" echo \"1 Active\" echo \"2 Suspended\" echo \"3 Deactivated\" echo \"5 Destroyed\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys?state=5\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq -r '[.resources | .[] | { name: .name, id: .id, state: .state, crn: .crn} ] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View a list of keys in a Key Protect instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- view_keys_list() { echo echo \"Listing keys for service ${service_name}...\" echo echo \"Values for State column:\" echo \"-------------------------\" echo \"0 Pre-activation\" echo \"1 Active\" echo \"2 Suspended\" echo \"3 Deactivated\" echo \"5 Destroyed\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key+json' | jq -r '[.resources | .[] | { name: .name, id: .id, state: .state, crn: .crn} ] | [ .[] | with_entries( .key |= ascii_downcase ) ] | (.[0] | keys_unsorted | @tsv), (.[]|.| map(.) | @tsv) ' | column -t -s $'\\t' echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Import a key # # Arguments: key-type, key-name, key-payload # #----------------------------------------------------------------------------------------------------- import_key() { display_usage() { echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] import-key [key-type] [key-name] [key-payload]\" echo echo \"Key Type\" echo \"-------------------------------------------------------------------------------------------\" echo \"standard Creates a standard key. Material from a standard key can be exported\" echo \"root Creates a root key. material from a root key can never be exported\" echo echo \"Note: Key material must be base64 encoded. For a root key, the base64 decoded value must be\" echo \" 128, 192 or 256 bits. This is 16 (128 bit), 24 (192 bit) or 32 (256 bit) Hex value\" echo echo \" This command can create a 256 bit base64 encoded key for demo purposes:\" echo echo \" openssl rand -base64 32\" echo exit 1 } if [[ $# -lt 3 ]]; then # Key type is missing if [[ $# -lt 1 ]]; then echo echo \"FAILED: a key type is required\" # there is a problem so display usage display_usage fi # Key name is missing if [[ $# -lt 2 ]]; then echo echo \"FAILED: a key name is required\" # there is a problem so display usage display_usage fi # Key payload is missing if [[ $# -lt 3 ]]; then echo echo \"FAILED: a key payload is required\" # there is a problem so display usage display_usage fi # there is a problem so display usage display_usage fi # Get parameter values keyType=\"$1\" keyName=\"$2\" keyPayload=\"$3\" # Validate the key type if [[ $keyType == \"standard\" || $keyType == \"root\" ]]; then echo else echo echo \"FAILED: key type: $keyType is invalid\" display_usage exit 1 fi # if [[ $keyType == \"root\" ]]; then isExtractable=false else isExtractable=true fi echo \"The value of isExtractable is ${isExtractable}\" echo echo \"Importing key ...\" echo #set -x curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'content-type: application/vnd.ibm.kms.key+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"type\": \"application/vnd.ibm.kms.key+json\", \"name\": \"'\"$keyName\"'\", \"extractable\": '\"$isExtractable\"', \"payload\": \"'\"$keyPayload\"'\" } ] }' | jq --color-output echo echo \"Done.\" echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Restore a key # # Arguments: key-id, key-payload # #----------------------------------------------------------------------------------------------------- restore_key() { display_usage() { echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] restore-key [key-id] [key-payload]\" echo echo \"Note: The key-id can be found using the view-keys command. You can only restore a key that\" echo \" previously existed. Deleted keys have a State of Destroyed. You must wait 30 seconds\" echo \" after deleting a key before it can be restored.\" echo exit 1 } if [[ $# -lt 2 ]]; then # Key id is missing if [[ $# -lt 1 ]]; then echo echo \"FAILED: a key id is required\" # there is a problem so display usage display_usage fi # Key payload is missing if [[ $# -lt 2 ]]; then echo echo \"FAILED: a key payload is required\" # there is a problem so display usage display_usage fi # there is a problem so display usage display_usage fi # Get parameter values keyId=\"$1\" keyPayload=\"$2\" echo echo \"Restoring key ...\" echo set -x curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}?action=restore\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.key+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"payload\": \"'\"$keyPayload\"'\" } ] }' | jq --color-output echo echo \"Done.\" echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Enable Dual Authorization for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- enable_dual_auth() { echo echo \"Enabling Dual Authorization for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # disable Dual Authorization for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- disable_dual_auth() { echo echo \"disabling Dual Authorization for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": false } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Set Allowed Networks policy for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- disable_public_endpoint() { echo echo \"Disabling public endpoint for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"allowedNetwork\", \"policy_data\": { \"enabled\": true, \"attributes\": { \"allowed_network\": \"private-only\" } } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Set Allowed Networks policy for a Key Protect Instance # # Arguments: none # #----------------------------------------------------------------------------------------------------- enable_public_endpoint() { echo echo \"enabling public endpoint for service ${service_name}...\" echo curl -s -X PUT \"https://${region}.kms.cloud.ibm.com/api/v2/instance/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'Content-Type: application/vnd.ibm.kms.policy+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"allowedNetwork\", \"policy_data\": { \"enabled\": true, \"attributes\": { \"allowed_network\": \"public-and-private\" } } } ] }' echo echo \"Done.\" view_policies echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View the details for a key # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- view_key() { if [[ $# -lt 1 ]]; then echo echo \"for view-key a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh <key-protect-instance-name] view-key [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"viewing attributes for key ${keyId}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # View the material for a key <-- I think this is redundant with view_key # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- view_key_material() { if [[ $# -lt 1 ]]; then echo echo \"for view-key-material a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] view-key-material [key-id]\" echo echo exit 1 fi keyId=$1 curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${key_id}\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq -r ' .resources[] | .payload ' exit 0 } #----------------------------------------------------------------------------------------------------- # Set a key for deletion - the first step in Dual Deletion # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- set_key_deletion() { if [[ $# -lt 1 ]]; then echo echo \"for set-key-deletion a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] set-key-deletion [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"Setting key ${keyId} in service ${service_name} for deletion...\" echo curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}?action=setKeyForDeletion\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key_action+json' \\ -H 'content-type: application/vnd.ibm.kms.key_action+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true } } ] }' | jq --color-output echo echo \"Done.\" echo echo \"viewing policies for key ${keyId}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Unet a key for deletion - Use this function to \"undo\" a \"set key for deletion\" # # Arguments: key-id # #----------------------------------------------------------------------------------------------------- unset_key_deletion() { if [[ $# -lt 1 ]]; then echo echo \"for set-key-deletion a key id is required\" echo echo \"USAGE:\" echo \"keyprotect.sh [service instance name] set-key-deletion [key-id]\" echo echo exit 1 fi keyId=$1 echo echo \"Unsetting key ${keyId} in service ${service_name} for deletion...\" echo curl -s -X POST \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}?action=unsetKeyForDeletion\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.key_action+json' \\ -H 'content-type: application/vnd.ibm.kms.key_action+json' \\ -d '{ \"metadata\": { \"collectionType\": \"application/vnd.ibm.kms.policy+json\", \"collectionTotal\": 1 }, \"resources\": [ { \"policy_type\": \"dualAuthDelete\", \"policy_data\": { \"enabled\": true } } ] }' | jq --color-output echo echo \"Done.\" echo echo \"viewing policies for key ${keyId}...\" echo curl -s \"https://${region}.kms.cloud.ibm.com/api/v2/keys/${keyId}/policies\" \\ -H 'Authorization: Bearer '\"$ibm_auth_token\"'' \\ -H 'bluemix-instance: '\"$service_instance_guid\"'' \\ -H 'accept: application/vnd.ibm.kms.policy+json' | jq --color-output echo echo \"Request complete.\" exit 0 } #----------------------------------------------------------------------------------------------------- # Main script execution starts here #----------------------------------------------------------------------------------------------------- service_name=\"$1\" kp_command=\"$2\" key_id=\"$3\" # Note: original way was to always have key-id as 3rd arg. New way is to move command logic to functions and # pass in argument 3 and higher in raw form directly to the function and let it decide what they mean. # For now, key_id is still set to $3 so that logic outside of functions still works. # Always need to arguments: service-instance-name and a command if [[ $# -lt 2 ]]; then display_usage exit 1 fi # Only need to do these commands if a service name is present if [[ $# -ge 1 ]]; then # Get the service instance details for $service_name service_instance_details=$(ibmcloud resource service-instance $service_name --output JSON) # Get the GUID for the specified Key Protect instance service_instance_guid=$(echo $service_instance_details| jq -r '.[0].guid') # service_instance_guid=$(ibmcloud resource service-instance $service_name --output JSON | jq -r '.[0].guid') # Get the region for the Key Protect Instance # region=$(echo $service_instance_details | jq -r '.[0].region_id') # Note: the line below sets the API endpoint to the private network. # To use the public network comment the line below and uncomment the one above. region=private.$(echo $service_instance_details | jq -r '.[0].region_id') # Get the bearer token to use for authentication. # Note: This assumes that the user has previously logged into the IBM Cloud CLI ibm_auth_token=$(ibmcloud iam oauth-tokens | awk '{ print $4}') fi #----------------------------------------------------------------------------------------------------- # View the policies for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [ $kp_command == \"help\" ] || [ $kp_command == \"h\" ]; then display_usage exit 0 fi #----------------------------------------------------------------------------------------------------- # View the policies for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-policies\" ]]; then view_policies exit 0 fi #----------------------------------------------------------------------------------------------------- # View the policies for a key in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-key-policies\" ]]; then view_key_policies $3 fi #----------------------------------------------------------------------------------------------------- # View the keys for the specified Key Protect instance in JSON format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-keys\" ]]; then view_keys exit 0 fi #----------------------------------------------------------------------------------------------------- # View the keys for the specified Key Protect instance in JSON format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-deleted-keys\" ]]; then view_deleted_keys exit 0 fi #----------------------------------------------------------------------------------------------------- # View the deleted keys for the specified Key Protect instance in List format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-deleted-keys-list\" ]]; then view_deleted_keys_list exit 0 fi #----------------------------------------------------------------------------------------------------- # View the keys for the specified Key Protect instance in List format #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-keys-list\" ]]; then view_keys_list exit 0 fi #----------------------------------------------------------------------------------------------------- # View the attributes for a key in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-key\" ]]; then view_key $3 fi #----------------------------------------------------------------------------------------------------- # Import a key into the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"import-key\" ]]; then import_key $3 $4 $5 exit 0 fi #----------------------------------------------------------------------------------------------------- # Import a key into the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"restore-key\" ]]; then restore_key $3 $4 exit 0 fi #----------------------------------------------------------------------------------------------------- # Enable dual authorization delete policy for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"enable-dual-auth\" ]]; then enable_dual_auth fi #----------------------------------------------------------------------------------------------------- # Disable dual authorization delete policy for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"disable-dual-auth\" ]]; then disable_dual_auth fi #----------------------------------------------------------------------------------------------------- # Disable public network for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"disable-public-endpoint\" ]]; then disable_public_endpoint fi #----------------------------------------------------------------------------------------------------- # enable public network for the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"enable-public-endpoint\" ]]; then enable_public_endpoint fi #----------------------------------------------------------------------------------------------------- # View the attributes for a key in the specified Key Protect instance #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"view-key-material\" ]]; then view_key_material $3 fi #----------------------------------------------------------------------------------------------------- # Set the specified key for deletion (first auth) #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"set-key-deletion\" ]]; then set_key_deletion $3 fi #----------------------------------------------------------------------------------------------------- # Unset the deletion (first auth) of the specified Key #----------------------------------------------------------------------------------------------------- if [[ $kp_command == \"unset-key-deletion\" ]]; then unset_key_deletion $3 fi #----------------------------------------------------------------------------------------------------- echo echo \"Error: Invalid command\" echo display_usage","title":"Keyprotect"}]}